{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Piraeus Datastore","text":"<p>Piraeus is a cloud-native storage system that empowers Kubernetes Local Persistent Volumes with dynamic provisioning, resource management, and high-availability. It deploys and scales out automatically within Kubernetes nodes. With Piraeus, Kubernetes workloads can now consume high performance local storage using the same volume APIs that app developers have become accustomed to.</p> <ul> <li> <p> Tutorials</p> <p>Get started with Piraeus Datastore. These tutorials will guide you through the basics of setting up Piraeus Datastore.</p> <p> Tutorials</p> </li> <li> <p> How-To Guides</p> <p>How-To Guides show you how to configure a specific aspect or achieve a specific task with Piraeus Datastore.</p> <p> How-To Guides</p> </li> <li> <p> Upgrades</p> <p>Read how to upgrade a Piraeus Datastore deployment</p> <p> Upgrades</p> </li> <li> <p> Understanding Piraeus Datastore</p> <p>Read how Piraeus Datastore and it's components work, and why they work the way they do.</p> <p> Understanding Piraeus</p> </li> <li> <p> Reference</p> <p>The API Reference for the Piraeus Operator. Contains documentation of the LINSTOR related resources that the user can modify or observe.</p> <p> Reference</p> </li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#v271-2024-11-25","title":"v2.7.1 - 2024-11-25","text":""},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Updated images:<ul> <li>DRBD 9.2.12</li> <li>DRBD Reactor 1.6.0</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v270-2024-11-11","title":"v2.7.0 - 2024-11-11","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Option to select IP Family to use for LINSTOR control traffic.</li> </ul>"},{"location":"CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li>Reconciliation of a Satellites network interfaces now also deletes unneeded interfaces.</li> <li>Satellites try to detect the LVM configuration on the host, reusing locks and udev rules if available.</li> <li>Updated images:<ul> <li>LINSTOR 1.29.2</li> <li>LINSTOR CSI 1.6.4</li> <li>DRBD Reactor 1.5.0</li> <li>LINSTOR HA Controller 1.2.2</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v260-2024-09-04","title":"v2.6.0 - 2024-09-04","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Support wildcard patterns when referencing property values from node labels and annotations.</li> </ul>"},{"location":"CHANGELOG/#changed_2","title":"Changed","text":"<ul> <li>Updated images:<ul> <li>LINSTOR 1.29.0</li> <li>DRBD 9.2.11</li> <li>Latest CSI Sidecars</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v252-2024-07-17","title":"v2.5.2 - 2024-07-17","text":""},{"location":"CHANGELOG/#added_2","title":"Added","text":"<ul> <li>Option to specify custom CA certificate resources for TLS configuration.</li> <li>Ensure all Certificate resources have <code>dnsNames</code> set.</li> </ul>"},{"location":"CHANGELOG/#changed_3","title":"Changed","text":"<ul> <li>Updated images:<ul> <li>LINSTOR 1.28.0</li> <li>LINSTOR CSI 1.6.3</li> <li>DRBD 9.2.10</li> <li>DRBD Reactor 1.4.1</li> <li>kTLS-utils 0.11</li> <li>Latest CSI sidecars</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>The operator will no longer restart reconciliation on every change to <code>ServiceAccount</code> resources.</li> </ul>"},{"location":"CHANGELOG/#v251-2024-05-02","title":"v2.5.1 - 2024-05-02","text":""},{"location":"CHANGELOG/#added_3","title":"Added","text":"<ul> <li>Default PriorityClasses for all components</li> </ul>"},{"location":"CHANGELOG/#changed_4","title":"Changed","text":"<ul> <li>New DRBD loader detection for:<ul> <li>Ubuntu Noble Numbat (24.04)</li> </ul> </li> <li>Updated images:<ul> <li>LINSTOR 1.27.1</li> <li>LINSTOR CSI 1.6.0</li> <li>DRBD 9.2.9</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v250-2024-04-03","title":"v2.5.0 - 2024-04-03","text":""},{"location":"CHANGELOG/#added_4","title":"Added","text":"<ul> <li>Support for managing ZFS Storage Pools using <code>LinstorSatelliteConfiguration</code>.</li> </ul>"},{"location":"CHANGELOG/#changed_5","title":"Changed","text":"<ul> <li>Updated images:<ul> <li>LINSTOR 1.27.0</li> <li>LINSTOR CSI 1.5.0</li> <li>High Availability Controller 1.2.1</li> <li>Latest CSI sidecars</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v241-2024-03-06","title":"v2.4.1 - 2024-03-06","text":""},{"location":"CHANGELOG/#added_5","title":"Added","text":"<ul> <li>New DRBD loader detection for:<ul> <li>Debian 12 (Bookworm)</li> <li>Rocky Linux 8 &amp; 9</li> </ul> </li> <li>Report <code>seLinuxMount</code> capability for the CSI Driver, speeding up volume mounts with SELinux   relabelling enabled.</li> <li>Alerts for offline LINSTOR Controller and Satellites.</li> </ul>"},{"location":"CHANGELOG/#changed_6","title":"Changed","text":"<ul> <li>Use node label instead of pod name for Prometheus alerting descriptions.</li> <li>Updated images:<ul> <li>LINSTOR 1.26.2</li> <li>DRBD 9.2.8</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v240-2024-01-30","title":"v2.4.0 - 2024-01-30","text":""},{"location":"CHANGELOG/#added_6","title":"Added","text":"<ul> <li>Validating Webhook for Piraeus StorageClasses.</li> </ul>"},{"location":"CHANGELOG/#breaking","title":"Breaking","text":"<ul> <li>Use DaemonSet to manage Satellite resources instead of bare Pods. This enables better integration with   common administrative tasks such as node draining. This change should be transparent for users, any patches   applied on the satellite Pods are internally converted to work on the new DaemonSet instead.</li> </ul>"},{"location":"CHANGELOG/#changed_7","title":"Changed","text":"<ul> <li>Change default monitoring address for DRBD Reactor to support systems with IPv6 completely disabled.</li> <li>Updated images:<ul> <li>LINSTOR 1.26.1</li> <li>LINSTOR CSI 1.4.0</li> <li>DRBD 9.2.7</li> <li>High Availability Controller 1.2.0</li> <li>Latest CSI sidecars</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v230-2023-12-05","title":"v2.3.0 - 2023-12-05","text":""},{"location":"CHANGELOG/#added_7","title":"Added","text":"<ul> <li>Add a new option <code>.spec.internalTLS.tlsHandshakeDaemon</code> to enable deployment of <code>tlshd</code> alongside the LINSTOR   Satellite.</li> <li>Shortcuts to configure specific components. Components can be disabled by setting <code>enabled: false</code>, and the deployed   workload can be influenced using the <code>podTemplate</code> value. Available components:<ul> <li><code>LinstorCluster.spec.controller</code></li> <li><code>LinstorCluster.spec.csiController</code></li> <li><code>LinstorCluster.spec.csiNode</code></li> <li><code>LinstorCluster.spec.highAvailabilityController</code></li> </ul> </li> <li>Shortcut to modify the pod of a satellite by adding a <code>LinstorSatelliteConfiguration.spec.podTemplate</code>, which is   a shortcut for creating a <code>.spec.patches</code> patch.</li> </ul>"},{"location":"CHANGELOG/#changed_8","title":"Changed","text":"<ul> <li>Fixed service resources relying on default protocol version.</li> <li>Moved NetworkPolicy for DRBD out of default deployed resources.</li> <li>Updated images:<ul> <li>LINSTOR 1.25.1</li> <li>LINSTOR CSI 1.3.0</li> <li>DRBD Reactor 1.4.0</li> <li>Latest CSI sidecars</li> </ul> </li> <li>Add a default toleration for the HA Controller taints to the operator.</li> </ul>"},{"location":"CHANGELOG/#v220-2023-08-31","title":"v2.2.0 - 2023-08-31","text":""},{"location":"CHANGELOG/#added_8","title":"Added","text":"<ul> <li>A new <code>LinstorNodeConnection</code> resource, used to configure the LINSTOR Node Connection feature in a Kubernetes way.</li> <li>Allow image configuration to be customized by adding additional items to the config map. Items using a \"greater\" key   take precedence when referencing the same images.</li> <li>Add image configuration for CSI sidecars.</li> <li>Check kernel module parameters for DRBD on load.</li> <li>Automatically set SELinux labels when loading kernel modules.</li> <li>Allow more complex node selection by adding <code>LinstorCluster.spec.nodeAffinity</code>.</li> </ul>"},{"location":"CHANGELOG/#changed_9","title":"Changed","text":"<ul> <li>Upgrade to operator-sdk 1.29</li> <li>Upgrade to kubebuilder v4 layout</li> <li>Updated images:<ul> <li>LINSTOR 1.24.2</li> <li>LINSTOR CSI 1.2.3</li> <li>DRBD 9.2.5</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#removed","title":"Removed","text":"<ul> <li>Disable operator metrics by default. This removes a dependency on an external container.</li> <li>Dependency on cert-manager for initial deployment.</li> </ul>"},{"location":"CHANGELOG/#fixed_1","title":"Fixed","text":"<ul> <li>A crash caused by insufficient permissions on the LINSTOR Controller.</li> <li>Satellite will now restart if the Pods terminated for unexpected reasons.</li> </ul>"},{"location":"CHANGELOG/#v211-2023-05-24","title":"v2.1.1 - 2023-05-24","text":""},{"location":"CHANGELOG/#added_9","title":"Added","text":"<ul> <li>LINSTOR Controller deployment now runs DB migrations as a separate init container, creating   a backup of current DB state if needed.</li> <li>Apply global rate limit to LINSTOR API, defaulting to 100 qps.</li> </ul>"},{"location":"CHANGELOG/#changed_10","title":"Changed","text":"<ul> <li>Store LINSTOR Satellite logs on the host.</li> <li>Updated images:<ul> <li>LINSTOR 1.23.0</li> <li>LINSTOR CSI 1.1.0</li> <li>DRBD Reactor 1.2.0</li> <li>HA Controller 1.1.4</li> <li>external CSI images upgraded to latest versions</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed a bug where <code>LinstorSatellite</code> resources would be not be cleaned up when the satellite is already gone.</li> <li>Fixed a bug where the LINSTOR Controller would never report readiness when TLS is enabled.</li> <li>Fixed order in which patches are applied. Always apply user patches last.</li> </ul>"},{"location":"CHANGELOG/#v210-2023-04-24","title":"v2.1.0 - 2023-04-24","text":""},{"location":"CHANGELOG/#added_10","title":"Added","text":"<ul> <li>Ability to skip deploying the LINSTOR Controller by setting <code>LinstorCluster.spec.externalController</code>.</li> <li>Automatically reconcile changed image configuration.</li> </ul>"},{"location":"CHANGELOG/#changed_11","title":"Changed","text":"<ul> <li>Fix an issue where the CSI node driver would use the CSI socket not through the expected path in the container.</li> <li>Updated images:<ul> <li>LINSTOR 1.22.0</li> <li>LINSTOR CSI 1.0.1</li> <li>DRBD 9.2.3</li> <li>DRBD Reactor 1.1.0</li> <li>HA Controller 1.1.3</li> <li>external CSI images upgraded to latest versions</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v201-2023-03-08","title":"v2.0.1 - 2023-03-08","text":""},{"location":"CHANGELOG/#added_11","title":"Added","text":"<ul> <li><code>drbd-shutdown-guard</code> init-container, installing a systemd unit that runs on shutdown.   It's purpose is to run <code>drbdsetup secondary --force</code> during shutdown, so that systemd can unmount all volumes, even   with <code>suspend-io</code>.</li> </ul>"},{"location":"CHANGELOG/#changed_12","title":"Changed","text":"<ul> <li>Updated LINSTOR CSI to 1.0.0, mounting the <code>/run/mount</code> directory from the host to enable the <code>_netdev</code> mount option.</li> </ul>"},{"location":"CHANGELOG/#fixed_3","title":"Fixed","text":"<ul> <li>HA Controller deployment requires additional rules to run on OpenShift.</li> </ul>"},{"location":"CHANGELOG/#v200-2023-02-23","title":"v2.0.0 - 2023-02-23","text":""},{"location":"CHANGELOG/#breaking_1","title":"Breaking","text":"<ul> <li>Removed existing CRD <code>LinstorController</code>, <code>LinstorSatelliteSet</code> and <code>LinstorCSIDriver</code>.</li> <li>Helm chart deprecated in favor of new <code>kustomize</code> deployment.</li> <li>Helm chart changed to only deploy the Operator. The LinstorCluster resource to create the storage cluster needs to be   created separately.</li> </ul>"},{"location":"CHANGELOG/#added_12","title":"Added","text":"<ul> <li>New CRDs to control storage cluster: <code>LinstorCluster</code> and <code>LinstorSatelliteConfiguration</code>.</li> <li>Tutorials on how to get started.</li> <li>Automatic selection of loader images based on operating system of node.</li> <li>Customization of single nodes or groups of nodes.</li> <li>Possibility to run DRBD replication using the container network.</li> <li>Support for file system backed storage pools</li> <li>Default deployment for HA Controller. Since we switch to defaulting to <code>suspend-io</code> for lost quorum, we should include   a way for Pods to get unstuck.</li> </ul>"},{"location":"CHANGELOG/#v1100-2022-10-18","title":"v1.10.0 - 2022-10-18","text":""},{"location":"CHANGELOG/#added_13","title":"Added","text":"<ul> <li>Can set the variable <code>mountDrbdResourceDirectoriesFromHost</code> in the Helm chart to create hostPath Volumes for DRBD and   LINSTOR configuration directories for the satellite set.</li> </ul>"},{"location":"CHANGELOG/#changed_13","title":"Changed","text":"<ul> <li>Change default bind address for satellite monitoring to use IPv6 anylocal <code>[::]</code>. This will still to work on IPv4   only systems with IPv6 disabled via sysctl.</li> <li>Default images:<ul> <li>LINSTOR 1.20.0</li> <li>DRBD 9.1.11</li> <li>DRBD Reactor 0.9.0</li> <li>external CSI images upgraded to latest versions</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#fixed_4","title":"Fixed","text":"<ul> <li>Comparing IP addresses for registered components uses golang's built-in net.IP type.</li> <li>Restoring satellites after eviction only happens if the node is ready.</li> </ul>"},{"location":"CHANGELOG/#v191-2022-07-27","title":"v1.9.1 - 2022-07-27","text":""},{"location":"CHANGELOG/#fixed_5","title":"Fixed","text":"<ul> <li>ServiceMonitor resources can't be patched, instead we recreate them.</li> </ul>"},{"location":"CHANGELOG/#v190-2022-07-20","title":"v1.9.0 - 2022-07-20","text":""},{"location":"CHANGELOG/#added_14","title":"Added","text":"<ul> <li>Support for custom labels and annotations with added options to <code>values.yaml</code>.</li> <li>Instructions for deploying the affinity controller.</li> </ul>"},{"location":"CHANGELOG/#changed_14","title":"Changed","text":"<ul> <li>Satellite operator now reports basic satellite status even if controller is not reachable</li> <li>Query single satellite devices to receive errors when the satellite is offline instead of assuming   devices are already configured.</li> <li>Disabled the legacy HA Controller deployment by default. It has been replaced a   separate chart.</li> <li>Default images:<ul> <li>LINSTOR 1.19.1</li> <li>LINSTOR CSI 0.20.0</li> <li>DRBD 9.1.8</li> <li>DRBD Reactor 0.8.0</li> <li>external CSI images to latest versions</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#fixed_6","title":"Fixed","text":"<ul> <li><code>last-applied-configuration</code>annotation was never updated, so updating of some fields was not performed correctly.</li> </ul>"},{"location":"CHANGELOG/#v182-2022-05-24","title":"v1.8.2 - 2022-05-24","text":""},{"location":"CHANGELOG/#added_15","title":"Added","text":"<ul> <li>Option to disable creating monitoring resources (Services and ServiceMonitors)</li> <li>Add options <code>csi.controllerSidecars</code>, <code>csi.controllerExtraVolumes</code>, <code>csi.nodeSidecars</code>, <code>csi.nodeExtraVolumes</code>,   <code>operator.controller.sidecars</code>, <code>operator.controller.extraVolumes</code>, <code>operator.satelliteSet.sidecars</code>,   <code>operator.satelliteSet.extraVolumes</code> to allow specifying extra sidecar containers.</li> <li>Add options <code>operator.controller.httpBindAddress</code>, <code>operator.controller.httpsBindAddress</code>,   <code>operator.satelliteSet.monitoringBindAddress</code> to allow specifying bind address.</li> <li>Add example values and doc reference to run piraeus-operator with rbac-proxy.</li> </ul>"},{"location":"CHANGELOG/#changed_15","title":"Changed","text":"<ul> <li>Default images:<ul> <li>LINSTOR 1.18.2</li> <li>LINSTOR CSI 0.19.1</li> <li>DRBD Reactor 0.7.0</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#removed_1","title":"Removed","text":"<ul> <li>Get rid of operator-sdk binary, use native controller-gen instead</li> </ul>"},{"location":"CHANGELOG/#v181-2022-05-12","title":"v1.8.1 - 2022-05-12","text":""},{"location":"CHANGELOG/#changed_16","title":"Changed","text":"<ul> <li>Default images:<ul> <li>LINSTOR 1.18.1</li> <li>LINSTOR CSI 0.19.0</li> <li>DRBD 9.1.7</li> <li>DRBD Reactor 0.6.1</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#added_16","title":"Added","text":"<ul> <li>Upgrades involving k8s databases no long require manually confirming a backup exists using   <code>--set IHaveBackedUpAllMyLinstorResources=true</code>.</li> </ul>"},{"location":"CHANGELOG/#v180-2022-03-15","title":"v1.8.0 - 2022-03-15","text":""},{"location":"CHANGELOG/#added_17","title":"Added","text":"<ul> <li>Allow setting the number of parallel requests created by the CSI sidecars. This limits the load on the LINSTOR   backend, which could easily overload when creating many volumes at once.</li> <li>Unify certificates format for SSL enabled installation, no more java tooling required.</li> <li>Automatic certificates generation using Helm or Cert-manager</li> <li>HA Controller and CSI components now wait for the LINSTOR API to be initialized using InitContainers.</li> </ul>"},{"location":"CHANGELOG/#changed_17","title":"Changed","text":"<ul> <li>Create backups of LINSTOR resource if the \"k8s\" database backend is used and an image change is detected. Backups   are stored in Secret resources as a <code>tar.gz</code>. If the secret would get too big, the backup can be downloaded from   the operator pod.</li> <li>Default images:<ul> <li>LINSTOR 1.18.0</li> <li>LINSTOR CSI 0.18.0</li> <li>DRBD 9.1.6</li> <li>DRBD Reactor 0.5.3</li> <li>LINSTOR HA Controller 0.3.0</li> <li>CSI Attacher v3.4.0</li> <li>CSI Node Driver Registrar v2.4.0</li> <li>CSI Provisioner v3.1.0</li> <li>CSI Snapshotter v5.0.1</li> <li>CSI Resizer v1.4.0</li> <li>Stork v2.8.2</li> </ul> </li> <li>Stork updated to support Kubernetes v1.22+.</li> <li>Satellites no longer have a readiness probe defined. This caused issues in the satellites by repeatedly opening   unexpected connections, especially when using SSL.</li> <li>Only query node devices if a storage pool needs to be created.</li> <li>Use cached storage pool response to avoid causing excessive load on LINSTOR satellites.</li> <li>Protect LINSTOR passphrase from accidental deletion by using a finalizer.</li> </ul>"},{"location":"CHANGELOG/#breaking_2","title":"Breaking","text":"<ul> <li>If you have SSL configured, then the certificates must be regenerated in PEM format.   Learn more in the upgrade guide.</li> </ul>"},{"location":"CHANGELOG/#v171-2022-01-18","title":"v1.7.1 - 2022-01-18","text":""},{"location":"CHANGELOG/#added_18","title":"Added","text":"<ul> <li>Allow the external-provisioner and external-snapshotter access to secrets. This is required to support StorageClass   and SnapshotClass secrets.</li> <li>Instruct external-provisioner to pass PVC name+namespace to the CSI driver, enabling optional support for PVC based   names for LINSTOR volumes.</li> <li>Allow setting the log level of LINSTOR components via CRs. Other components are left using their default log level.   The new default log level is INFO (was DEBUG previously, which was often too verbose).</li> <li>Override the kernel source directory used when compiling DRBD (defaults to /usr/src). See    <code>operator.satelliteSet.kernelModuleInjectionAdditionalSourceDirectory</code></li> <li>etcd-chart: add option to set priorityClassName.</li> </ul>"},{"location":"CHANGELOG/#fixed_7","title":"Fixed","text":"<ul> <li>Use correct secret name when setting up TLS for satellites</li> <li>Correctly configure ServiceMonitor resource if TLS is enabled for LINSTOR Controller.</li> </ul>"},{"location":"CHANGELOG/#v170-2021-12-14","title":"v1.7.0 - 2021-12-14","text":""},{"location":"CHANGELOG/#added_19","title":"Added","text":"<ul> <li><code>pv-hostpath</code>: automatically determine on which nodes PVs should be created if no override is given.</li> <li>Automatically add labels on Kubernetes Nodes to LINSTOR satellites as Auxiliary Properties. This enables using   Kubernetes labels for volume scheduling, for example using <code>replicasOnSame: topology.kubernetes.io/zone</code>.</li> <li>Support LINSTORs <code>k8s</code> backend by adding the necessary RBAC resources and documentation.</li> <li>Automatically create a LINSTOR passphrase when none is configured.</li> <li>Automatic eviction and deletion of offline satellites if the Kubernetes node object was also deleted.</li> </ul>"},{"location":"CHANGELOG/#changed_18","title":"Changed","text":"<ul> <li>Default images:<ul> <li><code>quay.io/piraeusdatastore/piraeus-server:v1.17.0</code></li> <li><code>quay.io/piraeusdatastore/piraeus-csi:v0.17.0</code></li> <li><code>quay.io/piraeusdatastore/drbd9-bionic:v9.1.4</code></li> <li><code>quay.io/piraeusdatastore/drbd-reactor:v0.4.4</code></li> </ul> </li> <li>Recreates or updates to the satellite pods are now applied at once, instead of waiting for a node to complete before   moving to the next.</li> <li>Enable CSI topology by default, allowing better volume scheduling with <code>volumeBindingMode: WaitForFirstConsumer</code>.</li> <li>Disable STORK by default. Instead, we recommend using <code>volumeBindingMode: WaitForFirstConsumer</code> in storage classes.</li> </ul>"},{"location":"CHANGELOG/#v160-2021-09-02","title":"v1.6.0 - 2021-09-02","text":""},{"location":"CHANGELOG/#added_20","title":"Added","text":"<ul> <li>Allow CSI to work with distributions that use a kubelet working directory other than <code>/var/lib/kubelet</code>. See   the <code>csi.kubeletPath</code> option.</li> <li>Enable Storage Capacity Tacking. This enables Kubernetes to base Pod scheduling decisions on remaining storage   capacity. The feature is in beta and enabled by default starting with Kubernetes 1.21.</li> </ul>"},{"location":"CHANGELOG/#changed_19","title":"Changed","text":"<ul> <li> <p>Disable Stork Health Monitoring by default. Stork cannot distinguish between control plane and data plane issues,   which can lead to instances where Stork will migrate a volume that is still mounted on another node, making the   volume effectively unusable.</p> </li> <li> <p>Updated operator to kubernetes v1.21 components.</p> </li> <li> <p>Default images:</p> <ul> <li><code>quay.io/piraeusdatastore/piraeus-server:v1.14.0</code></li> <li><code>quay.io/piraeusdatastore/drbd9-bionic:v9.0.30</code></li> <li><code>quay.io/piraeusdatastore/drbd-reactor:v0.4.3</code></li> <li><code>quay.io/piraeusdatastore/piraeus-ha-controller:v0.2.0</code></li> <li>external CSI images</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#removed_2","title":"Removed","text":"<ul> <li>The cluster-wide snapshot controller is no longer deployed as a dependency of the piraeus-operator chart.   Instead, separate charts are available on artifacthub.io   that deploy the snapshot controller and extra validation for snapshot resources.</li> </ul> <p>The subchart was removed, as it unnecessarily tied updates of the snapshot controller to piraeus and vice versa. With   the tightened validation starting with snapshot CRDs <code>v1</code>, moving the snapshot controller to a proper chart seems   like a good solution.</p>"},{"location":"CHANGELOG/#v151-2021-06-21","title":"v1.5.1 - 2021-06-21","text":""},{"location":"CHANGELOG/#changed_20","title":"Changed","text":"<ul> <li>Default images:<ul> <li>Piraeus Server v1.13.0</li> <li>Piraeus CSI v0.13.1</li> <li>CSI Provisioner v2.1.2</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v150-2021-05-12","title":"v1.5.0 - 2021-05-12","text":""},{"location":"CHANGELOG/#added_21","title":"Added","text":"<ul> <li>All operator-managed workloads apply recommended labels. This requires the recreation of Deployments and DaemonSets   on upgrade. This is automatically handled by the operator, however any customizations applied to the deployments   not managed by the operator will be reverted in the process.</li> <li>Use <code>drbd-reactor</code> to expose Prometheus endpoints on each satellite.</li> <li>Configure <code>ServiceMonitor</code> resources if they are supported by the cluster (i.e. prometheus operator is configured)</li> </ul>"},{"location":"CHANGELOG/#changed_21","title":"Changed","text":"<ul> <li>CSI Nodes no longer use <code>hostNetwork: true</code>. The pods already got the correct hostname via the downwardAPI and do not   talk to DRBD's netlink interface directly.</li> <li>External: CSI snapshotter subchart now packages <code>v1</code> CRDs. Fixes deprecation warnings when installing   the snapshot controller.</li> <li>Default images:<ul> <li>Piraeus Server v1.12.3</li> <li>Piraeus CSI v0.13.0</li> <li>DRBD v9.0.29</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v140-2021-04-07","title":"v1.4.0 - 2021-04-07","text":""},{"location":"CHANGELOG/#added_22","title":"Added","text":"<ul> <li>Additional environment variables and Linstor properties can now be set in the <code>LinstorController</code> CRD.</li> <li>Set node name variable for Controller Pods, enabling k8s-await-election to correctly set up the endpoint for   hairpin mode.</li> </ul>"},{"location":"CHANGELOG/#fixed_8","title":"Fixed","text":"<ul> <li>Update the network address of controller pods if they diverged between Linstor and kubernetes. This can happen after   a node restart, where a pod is recreated with the same name but different IP address.</li> </ul>"},{"location":"CHANGELOG/#v131-2021-01-14","title":"v1.3.1 - 2021-01-14","text":""},{"location":"CHANGELOG/#added_23","title":"Added","text":"<ul> <li>New guide on host preparation here.</li> </ul>"},{"location":"CHANGELOG/#changed_22","title":"Changed","text":"<ul> <li>Default image updated:<ul> <li><code>operator.satelliteSet.kernelModuleInjectionImage</code>: <code>quay.io/piraeusdatastore/drbd9-bionic:v9.0.27</code></li> <li><code>operator.satelliteSet.satelliteImage</code>: <code>quay.io/piraeusdatastore/piraeus-server:v1.11.1</code></li> <li><code>operator.controller.controllerImage</code>: <code>quay.io/piraeusdatastore/piraeus-server:v1.11.1</code></li> <li><code>haController.image</code>: <code>quay.io/piraeusdatastore/piraeus-ha-controller:v0.1.3</code></li> <li><code>pv-hostpath</code>: <code>chownerImage</code>: <code>quay.io/centos/centos:8</code></li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v130-2020-12-21","title":"v1.3.0 - 2020-12-21","text":""},{"location":"CHANGELOG/#added_24","title":"Added","text":"<ul> <li>New component: <code>haController</code> will deploy the Piraeus High Availability Controller.   More information is available in   the optional components page</li> <li>Enable strict checking of DRBD parameter to disable usermode helper in container environments.</li> <li>Override the image used in \"chown\" jobs in the <code>pv-hostpath</code> chart by using <code>--set chownerImage=&lt;my-image&gt;</code>.</li> </ul>"},{"location":"CHANGELOG/#changed_23","title":"Changed","text":"<ul> <li>Updated <code>operator-sdk</code> to v0.19.4</li> <li>Set CSI component timeout to 1 minute to reduce the number of retries in the CSI driver</li> <li>Default images updated:<ul> <li><code>operator.controller.controllerImage</code>: <code>quay.io/piraeusdatastore/piraeus-server:v1.11.0</code></li> <li><code>operator.satelliteSet.satelliteImage</code>: <code>quay.io/piraeusdatastore/piraeus-server:v1.11.0</code></li> <li><code>operator.satelliteSet.kernelModuleInjectionImage</code>: <code>quay.io/piraeusdatastore/drbd9-bionic:v9.0.26</code></li> <li><code>csi.pluginImage</code>: <code>quay.io/piraeusdatastore/piraeus-csi:v0.11.0</code></li> </ul> </li> </ul>"},{"location":"CHANGELOG/#fixed_9","title":"Fixed","text":"<ul> <li>Fixed Helm warnings when setting \"csi.controllerAffinity\", \"operator.controller.affinity\" and   \"operator.satelliteSet.storagePools\".</li> </ul>"},{"location":"CHANGELOG/#v120-2020-11-18","title":"v1.2.0 - 2020-11-18","text":""},{"location":"CHANGELOG/#added_25","title":"Added","text":"<ul> <li><code>storagePools</code> can now also set up devices similar to <code>automaticStorageType</code>, but with more fine grained control.   See the updated storage guide</li> <li>New Helm options to disable creation of LinstorController and LinstorSatelliteSet resource   <code>operator.controller.enabled</code> and <code>operator.satelliteSet.enabled</code>.</li> <li>New Helm option to override the generated controller endpoint: <code>controllerEndpoint</code></li> <li>Allow overriding the default <code>securityContext</code> on a component basis:<ul> <li><code>etcd.podsecuritycontext</code> sets the securityContext of etcd pods</li> <li><code>stork.podsecuritycontext</code> sets the securityContext of stork plugin and scheduler pods</li> <li><code>csi-snapshotter.podsecuritycontext</code> sets the securityContext of the CSI-Snapshotter pods</li> <li><code>operator.podsecuritycontext</code> sets the securityContext of the operator pods</li> </ul> </li> <li>Example settings for openshift</li> <li>LINSTOR controller runs with additional GID 1000, to ensure write access to log directory</li> </ul>"},{"location":"CHANGELOG/#changed_24","title":"Changed","text":"<ul> <li>Fixed a bug in <code>pv-hostpath</code> where permissions on the created directory are not applied on all nodes.</li> <li>Volumes created by <code>pv-hostpath</code> are now group writable. This makes them easier to integrate with <code>fsGroup</code> settings.</li> <li>Default value for affinity on LINSTOR controller and CSI controller changed. The new default is to distribute the pods   across all available nodes.</li> <li>Default value for tolerations for etcd pods changed. They are now able to run on master nodes.</li> <li>Updates to LinstorController, LinstorSatelliteSet and LinstorCSIDriver are now propagated across all created resources</li> <li>Updated default images:<ul> <li>csi sidecar containers updated (compatible with Kubernetes v1.17+)</li> <li>LINSTOR 1.10.0</li> <li>LINSTOR CSI 0.10.0</li> </ul> </li> </ul>"},{"location":"CHANGELOG/#deprecation","title":"Deprecation","text":"<ul> <li>Using <code>automaticStorageType</code> is deprecated. Use the <code>storagePools</code> values instead.</li> </ul>"},{"location":"CHANGELOG/#v110-2020-10-13","title":"v1.1.0 - 2020-10-13","text":""},{"location":"CHANGELOG/#breaking_3","title":"Breaking","text":"<ul> <li>The LINSTOR controller image given in <code>operator.controller.controllerImage</code> has to have   its entrypoint set to <code>k8s-await-election v0.2.0</code>   or newer. Learn more in the upgrade guide.</li> </ul>"},{"location":"CHANGELOG/#added_26","title":"Added","text":"<ul> <li>LINSTOR controller can be started with multiple replicas. See  <code>operator.controller.replicas</code>.   NOTE: This requires support from the container. You need <code>piraeus-server:v1.8.0</code> or newer.</li> <li>The <code>pv-hostpath</code> helper chart automatically sets up permissions for non-root etcd containers.</li> <li>Disable securityContext enforcement by setting <code>global.setSecurityContext=false</code>.</li> <li>Add cluster roles to work with OpenShift's SCC system.</li> <li>Control volume placement and accessibility by using CSIs Topology feature. Controlled by setting   <code>csi.enableTopology</code>.</li> <li>All pods use a dedicated service account to allow for fine-grained permission control.</li> <li>The new helm section <code>psp.*</code> can automatically configure the ServiceAccount   of all components to use the appropriate PSP roles.</li> </ul>"},{"location":"CHANGELOG/#changed_25","title":"Changed","text":"<ul> <li>Default values:<ul> <li><code>operator.controller.controllerImage</code>: <code>quay.io/piraeusdatastore/piraeus-server:v1.9.0</code></li> <li><code>operator.satelliteSet.satelliteImage</code>: <code>quay.io/piraeusdatastore/piraeus-server:v1.9.0</code></li> <li><code>operator.satelliteSet.kernelModuleInjectionImage</code>: <code>quay.io/piraeusdatastore/drbd9-bionic:v9.0.25</code></li> <li><code>stork.storkImage</code>: <code>docker.io/openstorage/stork:2.5.0</code></li> </ul> </li> <li>linstor-controller no longer starts in a privileged container.</li> </ul>"},{"location":"CHANGELOG/#removed_3","title":"Removed","text":"<ul> <li>legacy CRDs (LinstorControllerSet, LinstorNodeSet) have been removed.</li> <li><code>v1alpha</code> CRD versions have been removed.</li> <li>default pull secret <code>drbdiocred</code> removed. To keep using it, use <code>--set drbdRepoCred=drbdiocred</code>.</li> </ul>"},{"location":"CHANGELOG/#v100-2020-08-06","title":"v1.0.0 - 2020-08-06","text":""},{"location":"CHANGELOG/#added_27","title":"Added","text":"<ul> <li><code>v1</code> of all CRDs</li> <li>Central value for controller image pull policy of all pods. Use <code>--set global.imagePullPolicy=&lt;value&gt;</code> on   helm deployment.</li> <li><code>charts/piraeus/values.cn.yaml</code> a set of helm values for faster image download for CN users.</li> <li>Allow specifying resource requirements for all pods. In helm you can set:<ul> <li><code>etcd.resources</code> for etcd containers</li> <li><code>stork.storkResources</code> for stork plugin resources</li> <li><code>stork.schedulerResources</code> for the kube-scheduler deployed for use with stork</li> <li><code>csi-snapshotter.resources</code> for the cluster snapshotter controller</li> <li><code>csi.resources</code> for all CSI related containers. for brevity, there is only one setting for ALL CSI containers.   They   are all stateless go process which use the same amount of resources.</li> <li><code>operator.resources</code> for operator containers</li> <li><code>operator.controller.resources</code> for LINSTOR controller containers</li> <li><code>operator.satelliteSet.resources</code> for LINSTOR satellite containers</li> <li><code>operator.satelliteSet.kernelModuleInjectionResources</code> for kernel module injector/builder containers</li> </ul> </li> <li>Components deployed by the operator can now run with multiple replicas. Components   elect a leader, that will take on the actual work as long as it is active. Should one   pod go down, another replica will take over.   Currently these components support multiple replicas:<ul> <li><code>etcd</code> =&gt; set <code>etcd.replicas</code> to the desired count</li> <li><code>stork</code> =&gt; set <code>stork.replicas</code> to the desired count for stork scheduler and controller</li> <li><code>snapshot-controller</code> =&gt; set <code>csi-snapshotter.replicas</code> to the desired count for cluster-wide CSI snapshot   controller</li> <li><code>csi-controller</code> =&gt; set <code>csi.controllerReplicas</code> to the desired count for the linstor CSI controller</li> <li><code>operator</code> =&gt; set <code>operator.replicas</code> to have multiple replicas of the operator running</li> </ul> </li> <li>Reference docs for all helm settings. Link</li> <li><code>stork.schedulerTag</code> can override the automatically chosen tag for the <code>kube-scheduler</code> image.   Previously, the tag always matched the kubernetes release.</li> </ul>"},{"location":"CHANGELOG/#changed_26","title":"Changed","text":"<ul> <li>Renamed <code>LinstorNodeSet</code> to <code>LinstorSatelliteSet</code>. This brings the operator in line with other LINSTOR resources.   Existing <code>LinstorNodeSet</code> resources will automatically be migrated to <code>LinstorSatelliteSet</code>.</li> <li>Renamed <code>LinstorControllerSet</code> to <code>LinstorController</code>. The old name implied the existence of multiple (separate)   controllers. Existing <code>LinstorControllerSet</code> resources will automatically be migrated to <code>LinstorController</code>.</li> <li>Helm values renamed to align with new CRD names:<ul> <li><code>operator.controllerSet</code> to <code>operator.controller</code></li> <li><code>operator.nodeSet</code> to <code>operator.satelliteSet</code></li> </ul> </li> <li> <p>Node scheduling no longer relies on <code>linstor.linbit.com/piraeus-node</code> labels. Instead, all CRDs support   setting pod affinity and tolerations.   In detail:</p> <ul> <li><code>linstorcsidrivers</code> gained 4 new resource keys, with no change in default behaviour:<ul> <li><code>nodeAffinity</code> affinity passed to the csi nodes</li> <li><code>nodeTolerations</code> tolerations passed to the csi nodes</li> <li><code>controllerAffinity</code> affinity passed to the csi controller</li> <li><code>controllerTolerations</code> tolerations passed to the csi controller</li> </ul> </li> <li><code>linstorcontrollerset</code> gained 2 new resource keys, with no change in default behaviour:<ul> <li><code>affinity</code> affinity passed to the linstor controller pod</li> <li><code>tolerations</code> tolerations passed to the linstor controller pod</li> </ul> </li> <li><code>linstornodeset</code> gained 2 new resource keys, with change in default behaviour:<ul> <li><code>affinity</code> affinity passed to the linstor controller pod</li> <li><code>tolerations</code> tolerations passed to the linstor controller pod</li> </ul> </li> </ul> </li> <li> <p>Controller is now a Deployment instead of StatefulSet.</p> </li> <li>Renamed <code>kernelModImage</code> to <code>kernelModuleInjectionImage</code></li> <li>Renamed <code>drbdKernelModuleInjectionMode</code> to <code>KernelModuleInjectionMode</code></li> </ul>"},{"location":"CHANGELOG/#v050-2020-06-29","title":"v0.5.0 - 2020-06-29","text":""},{"location":"CHANGELOG/#added_28","title":"Added","text":"<ul> <li>Support volume resizing with newer CSI versions.</li> <li>A new Helm chart <code>csi-snapshotter</code> that deploys extra components needed for volume snapshots.</li> <li> <p>Add new kmod injection mode <code>DepsOnly</code>. Will try load kmods for LINSTOR layers from the host. Deprecates <code>None</code>.</p> </li> <li> <p>Automatic deployment of Stork scheduler configured for LINSTOR.</p> </li> </ul>"},{"location":"CHANGELOG/#removed_4","title":"Removed","text":""},{"location":"CHANGELOG/#changed_27","title":"Changed","text":"<ul> <li>Replaced <code>bitnami/etcd</code> dependency with vendored custom version   Some important keys for the <code>etcd</code> helm chart have changed:<ul> <li><code>statefulset.replicaCount</code> -&gt; <code>replicas</code></li> <li><code>persistence.enabled</code> -&gt; <code>persistentVolume.enabled</code></li> <li><code>persistence.size</code> -&gt; <code>persistentVolume.storage</code></li> <li><code>\u00e0uth.rbac</code> was removed: use tls certificates</li> <li><code>auth.peer.useAutoTLS</code> was removed</li> <li><code>envVarsConfigMap</code> was removed</li> <li>When using etcd with TLS enabled:<ul> <li>For peer communication, peers need valid certificates for <code>*.&lt;release-name&gt;-etcd</code> (was   <code>.&lt;release-name&gt;&gt;-etcd-headless.&lt;namespace&gt;.svc.cluster.local</code>)</li> <li>For client communication, servers need valid certificates for <code>*.&lt;release-name&gt;-etcd</code>  (was   <code>.&lt;release-name&gt;&gt;-etcd.&lt;namespace&gt;.svc.cluster.local</code>)</li> </ul> </li> </ul> </li> </ul>"},{"location":"CHANGELOG/#v041-2020-06-10","title":"v0.4.1 - 2020-06-10","text":""},{"location":"CHANGELOG/#added_29","title":"Added","text":"<ul> <li>Automatic storage pool creation via <code>automaticStorageType</code> on <code>LinstorNodeSet</code>. If this option is set, LINSTOR   will create a storage pool based on all available devices on a node.</li> </ul>"},{"location":"CHANGELOG/#changed_28","title":"Changed","text":"<ul> <li>Moved storage documentation to the storage guide</li> <li>Helm: update default images</li> </ul>"},{"location":"CHANGELOG/#v040-2020-06-05","title":"v0.4.0 - 2020-06-05","text":""},{"location":"CHANGELOG/#added_30","title":"Added","text":"<ul> <li>Secured database connection for Linstor: When using the <code>etcd</code> connector, you can specify a secret containing a CA   certificate to switch from HTTP to HTTPS communication.</li> <li>Secured connection between Linstor components: You can specify TLS keys to secure the communication between   controller and satellite</li> <li>Secure storage with LUKS: You can specify the master passphrase used by Linstor when creating encrypted volumes   when installing via Helm.</li> <li>Authentication with etcd using TLS client certificates.</li> <li>Secured connection between linstor-client and controller (HTTPS). More in   the security guide</li> <li>Linstor controller endpoint can now be customized for all resources. If not specified, the old default values will be   filled in.</li> </ul>"},{"location":"CHANGELOG/#removed_5","title":"Removed","text":"<ul> <li>NodeSet service (<code>piraeus-op-ns</code>) was replaced by the ControllerSet service (<code>piraeus-op-cs</code>) everywhere</li> </ul>"},{"location":"CHANGELOG/#changed_29","title":"Changed","text":"<ul> <li>CSI storage driver setup: move setup from helm to go operator. This is mostly an internal change.   These changes may be of note if you used a non-default CSI configuration:<ul> <li>helm value <code>csi.image</code> was renamed to <code>csi.pluginImage</code></li> <li>CSI deployment can be controlled by a new resource <code>linstorcsidrivers.piraeus.linbit.com</code></li> </ul> </li> <li>PriorityClasses are not automatically created. When not specified, the priority class is:<ul> <li>\"system-node-critical\", if deployed in \"kube-system\" namespace</li> <li>default PriorityClass in other namespaces</li> </ul> </li> <li>RBAC rules for CSI: creation moved to deployment step (Helm/OLM). ServiceAccounts should be specified in CSI resource.   If no ServiceAccounts are named, the implicitly created accounts from previous deployments will be used.</li> <li>Helm: update default images</li> </ul>"},{"location":"CHANGELOG/#v030-2020-05-08","title":"v0.3.0 - 2020-05-08","text":""},{"location":"CHANGELOG/#changed_30","title":"Changed","text":"<ul> <li>Use single values for images in CRDs instead of specifying the version   separately</li> <li>Helm: Use single values for images instead of specifying repo, name and   version separately</li> <li>Helm: Replace fixed storage pool configuration with list</li> <li>Helm: Do not create any storage pools by default</li> <li>Helm: Replace <code>operator.nodeSet.spec</code> and <code>operator.controllerSet.spec</code> by   just <code>operator.nodeSet</code> and <code>operator.controllerSet</code>.</li> </ul>"},{"location":"CHANGELOG/#v022-2020-04-24","title":"v0.2.2 - 2020-04-24","text":""},{"location":"CHANGELOG/#changed_31","title":"Changed","text":"<ul> <li>Fix reporting of errors in LinstorControllerSet status</li> </ul>"},{"location":"CHANGELOG/#v021-2020-04-14","title":"v0.2.1 - 2020-04-14","text":""},{"location":"CHANGELOG/#changed_32","title":"Changed","text":"<ul> <li>Helm: Update LINSTOR server dependencies to fix startup problems</li> </ul>"},{"location":"CHANGELOG/#v020-2020-04-10","title":"v0.2.0 - 2020-04-10","text":""},{"location":"CHANGELOG/#added_31","title":"Added","text":"<ul> <li>Helm: Allow an existing database to be used instead of always setting up   a dedicated etcd instance</li> </ul>"},{"location":"CHANGELOG/#changed_33","title":"Changed","text":"<ul> <li>Rename <code>etcdURL</code> parameter of LinstorControllerSet to <code>dbConnectionURL</code> to   reflect the fact that it can be used for any database type</li> <li>Upgrade to operator-sdk v0.16.0</li> <li>Helm: Create multiple volumes with a single <code>pv-hostchart</code> installation</li> <li>Helm: Update dependencies</li> </ul>"},{"location":"CHANGELOG/#v014-2020-03-05","title":"v0.1.4 - 2020-03-05","text":""},{"location":"CHANGELOG/#added_32","title":"Added","text":"<ul> <li>Helm: Add support for <code>hostPath</code> <code>PersistentVolume</code> persistence of etcd</li> </ul>"},{"location":"CHANGELOG/#removed_6","title":"Removed","text":"<ul> <li>Helm: Remove vendored etcd chart from repository</li> </ul>"},{"location":"CHANGELOG/#changed_34","title":"Changed","text":"<ul> <li>Rename CRDs from Piraeus to Linstor</li> <li>Make priority classes configurable</li> <li>Fix LINSTOR Controller/Satellite arguments</li> <li>Helm: Make etcd persistent by default</li> <li>Helm: Fix deployment of permissions objects into a non-default namespace</li> <li>Helm: Set default etcd size to 1Gi</li> <li>Helm: Update dependent image versions</li> <li>Docker: Change base image to Debian Buster</li> </ul>"},{"location":"CHANGELOG/#v013-2020-02-24","title":"[v0.1.3] - 2020-02-24","text":""},{"location":"CHANGELOG/#added_33","title":"Added","text":"<ul> <li>Support for kernel module injection based on shipped modules - necessary for   CoreOS support.</li> </ul>"},{"location":"CHANGELOG/#v0121-2020-02-21","title":"[v0.1.2.1] - 2020-02-21","text":""},{"location":"CHANGELOG/#added_34","title":"Added","text":"<ul> <li>/charts contains Helm v3 chart for this operator</li> </ul>"},{"location":"CHANGELOG/#changed_35","title":"Changed","text":"<ul> <li>CRDs contain additional Spec parameters that allow customizing image repo and   tag/version of the image.</li> <li>Another Spec parameter 'drbdRepoCred' can specify the name of the k8s secret   used to access the container images.</li> <li>LINSTOR Controller image now contains the LINSTOR client, away from the   Satellite images as it was previously the case. Hence, the readiness probe   is changed to use <code>curl</code> instead of <code>linstor</code> client command.</li> </ul>"},{"location":"CHANGELOG/#v010-2020-01-27","title":"[v0.1.0] - 2020-01-27","text":""},{"location":"CHANGELOG/#added_35","title":"Added","text":"<ul> <li>examples/operator-intra.yaml file to bundle all the rbac, crds, etc to run the   operator</li> <li>EtcdURL field to controllersetcontroller spec. default: etcd-piraeus:2379</li> <li>Host networking on the LINSTOR Satellite pods with DNSClusterFirstWithHostNet   DNS policy</li> <li>NodeSet service for the Satellite pods that also point to the Controller   service for LINSTOR discovery</li> </ul>"},{"location":"CHANGELOG/#removed_7","title":"Removed","text":"<ul> <li><code>ControllerEndpoint</code> and <code>DefaultController</code> from the PiraeusNodeSet spec</li> </ul>"},{"location":"CHANGELOG/#changed_36","title":"Changed","text":"<ul> <li>Controller persistence is now handled by etcd. There must be a reachable and   operable etcd cluster for this operator to work.</li> <li>Networking is now handled by a kubernetes service with the same name   as the ControllerSet. The NodeSet must have the same name as the ControllerSet   for networking to work properly.</li> <li>Opt-in node label for nodes is now <code>linstor.linbit.com/piraeus-node=true</code></li> <li>Remove requirement for <code>kube-system</code> namespace</li> <li>Naming convention for NodeSet and ControllerSet Pods</li> <li>Updated ports for LINSTOR access on NodeSet and ControllerSet pods</li> <li>Updated framework to work with Operator Framework 0.13.0</li> <li>API Versions on PriorityClass, DaemonSet, StatefulSet, and CRD kinds to reflect   K8s 1.16.0 release</li> </ul>"},{"location":"CHANGELOG/#v001-2019-07-19","title":"v0.0.1 - 2019-07-19","text":""},{"location":"CHANGELOG/#added_36","title":"Added","text":"<ul> <li>Initial public version with docs</li> </ul>"},{"location":"explanation/","title":"Understanding Piraeus Datastore","text":"<p>The following documents explain how Piraeus Datastore works, and why it works the way it does.</p>"},{"location":"explanation/#understanding-piraeus-datastore-components","title":"Understanding Piraeus Datastore Components","text":"<p>A short introduction for every component of Piraeus Datastore.</p>"},{"location":"explanation/components/","title":"Understanding Piraeus Datastore Components","text":"<p>Piraeus Datastore consists of several different components. Each component runs as a separate Deployment, DaemonSet or plain Pod.</p> <p>All Pods are labelled according to their component. You can check the Pods running in your own cluster by running the following <code>kubectl</code> command:</p> <pre><code>$ kubectl get pods '-ocustom-columns=NAME:.metadata.name,COMPONENT:.metadata.labels.app\\.kubernetes\\.io/component'\nNAME                                                   COMPONENT\nha-controller-vd82w                                    ha-controller\nlinstor-controller-6c8f8dc47-cm8hr                     linstor-controller\nlinstor-csi-controller-59b9968b86-ftl76                linstor-csi-controller\nlinstor-csi-node-hcmk9                                 linstor-csi-node\nlinstor-satellite-k8s-10.test-66687                    linstor-satellite\npiraeus-operator-controller-manager-6dcfcb4568-6jntp   piraeus-operator\npiraeus-operator-gencert-59449cb449-nzg6z              piraeus-operator-gencert\n</code></pre>"},{"location":"explanation/components/#piraeus-operator","title":"<code>piraeus-operator</code>","text":"<p>The Piraeus Operator creates and maintains the other components, except for the <code>piraeus-operator-gencert</code> component.</p> <p>Along with deploying the needed Kubernetes resources, it maintains the LINSTOR\u00ae Cluster state by:</p> <ul> <li>Registering satellites</li> <li>Creating storage pools</li> <li>Maintaining node labels</li> </ul>"},{"location":"explanation/components/#piraeus-operator-gencert","title":"<code>piraeus-operator-gencert</code>","text":"<p>The \"generate certificate\" Pod creates and maintains the TLS key and certificate used by the Piraeus Operator. The TLS secret, named <code>webhook-server-cert</code>, is needed to start the Piraeus Operator Pod. In addition, this Pod keeps the <code>ValidatingWebhookConfiguration</code> for the Piraeus Operator up-to-date.</p> <p>The Piraeus Operator needs a TLS certificate to serve the validation endpoint for the custom resources it maintains.</p> <p>Historically, TLS certificates where created by <code>cert-manager</code>. This was removed to reduce the number of dependencies for deploying Piraeus Datastore.</p>"},{"location":"explanation/components/#linstor-controller","title":"<code>linstor-controller</code>","text":"<p>The LINSTOR Controller is responsible for resource placement, resource configuration, and orchestration of any operational processes that require a view of the whole cluster. It maintains a database of all the configuration information for the whole cluster, stored as Kubernetes objects.</p> <p>The LINSTOR Controller connects to the LINSTOR Satellites and sends them instructions for achieving the desired cluster state.</p> <p>It provides an external API used by LINSTOR CSI and Piraeus Operator to change the cluster state.</p>"},{"location":"explanation/components/#linstor-satellite","title":"<code>linstor-satellite</code>","text":"<p>The LINSTOR Satellite service runs on each node. It acts as the local configuration agent for LINSTOR managed storage. It is stateless and receives all the information it needs from the LINSTOR Controller.</p> <p>Satellites are started as DaemonSets, managed by the Piraeus Operator. We deploy one DaemonSet per node, this enables having per-node configuration and customization of the Satellite Pods.</p> <p>Satellites interact with the host operating system directly and are deployed as privileged containers. Integration with the host operating system also leads to two noteworthy interactions with Linux namespaces:</p> <ul> <li>Any DRBD\u00ae device will inherit the network namespace of the Satellite Pods. Unless the Satellites are using   host networking, DRBD will not be able to replicate data without a running Satellite Pod. See the   host networking guide for more information.</li> <li>The Satellite process is spawned in a separate UTS namespace: this allows us to keep control of the hostname reported   to DRBD tools, even when the Pod is using a generated name from the DaemonSet. Thus, DRBD connections will always use   the Kubernetes node name.</li> </ul> <p>The use of separate UTS namespace should be completely transparent to users: running <code>kubectl exec ...</code> on a satellite   Pod will drop you into this namespace, enabling you to run <code>drbdadm</code> commands as expected.</p>"},{"location":"explanation/components/#linstor-csi-controller","title":"<code>linstor-csi-controller</code>","text":"<p>The LINSTOR CSI Controller Pod creates, modifies and deletes volumes and snapshots. It translates the state of Kubernetes resources (<code>StorageClass</code>, <code>PersistentVolumeClaims</code>, <code>VolumeSnapshots</code>) into their equivalent in LINSTOR.</p>"},{"location":"explanation/components/#linstor-csi-node","title":"<code>linstor-csi-node</code>","text":"<p>The LINSTOR CSI Node Pods execute mount and unmount operations. Mount and Unmount are initiated by kubelet before starting a Pod with a Piraeus volume.</p> <p>They are deployed as a DaemonSet on every node in the cluster by default. There needs to be a LINSTOR Satellite running on the same node as a CSI Node Pod.</p>"},{"location":"explanation/components/#ha-controller","title":"<code>ha-controller</code>","text":"<p>The Piraeus High Availability Controller will speed up the fail-over process for stateful workloads using Piraeus for storage.</p> <p>It is deployed on every node in the cluster and listens for DRBD\u00ae events to detect storage failures on other nodes. It evicts Pods when it detects that the storage on their node is inaccessible.</p>"},{"location":"how-to/","title":"How To","text":"<p>These guides show you how to configure a specific aspect or achieve a specific task with Piraeus Datastore.</p>"},{"location":"how-to/#advanced-deployments","title":"Advanced Deployments","text":"<ul> <li> <p>Use Piraeus Datastore with an Existing LINSTOR Cluster</p> <p> Guide</p> </li> <li> <p>Configure the DRBD Module Loader</p> <p> Guide</p> </li> </ul>"},{"location":"how-to/#securing-components","title":"Securing Components","text":"<ul> <li> <p>Configure TLS Between LINSTOR Controller and LINSTOR Satellite</p> <p> Guide</p> </li> <li> <p>Configure TLS for the LINSTOR API</p> <p> Guide</p> </li> <li> <p>Configure TLS for DRBD Replication</p> <p> Guide</p> </li> <li> <p>Load DRBD with SecureBoot Enabled</p> <p> Guide</p> </li> </ul>"},{"location":"how-to/#kubernetes-distributions","title":"Kubernetes Distributions","text":"<ul> <li> <p>Deploy Piraeus Datastore on OpenShift</p> <p> Guide</p> </li> <li> <p>Deploy Piraeus Datastore on Talos Linux</p> <p> Guide</p> </li> <li> <p>Deploy Piraeus Datastore on Flatcar Container Linux</p> <p> Guide</p> </li> <li> <p>Deploy Piraeus Datastore on MicroK8s</p> <p> Guide</p> </li> <li> <p>Deploy Piraeus Datastore on k0s</p> <p> Guide</p> </li> </ul>"},{"location":"how-to/#networking","title":"Networking","text":"<ul> <li> <p>Deploy Piraeus Datastore behind an HTTP Proxy</p> <p> Guide</p> </li> <li> <p>Deploy a NetworkPolicy for Piraeus Datastore</p> <p> Guide</p> </li> <li> <p>Use the Host Network for DRBD Replication</p> <p> Guide</p> </li> </ul>"},{"location":"how-to/#maintenance-tasks","title":"Maintenance Tasks","text":"<ul> <li> <p>Monitor Piraeus Datastore with Prometheus Operator</p> <p> Guide</p> </li> <li> <p>Keep Persistent Volume Affinity Updated with LINSTOR Affinity Controller</p> <p> Guide</p> </li> <li> <p>Restore a LINSTOR Database Backup</p> <p> Guide</p> </li> </ul>"},{"location":"how-to/api-tls/","title":"How to Configure TLS for the LINSTOR API","text":"<p>This guide shows you how to set up TLS for the LINSTOR\u00ae API. The API, served by the LINSTOR Controller, is used by clients such as the CSI Driver and the Operator itself to control the LINSTOR Cluster.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorCluster</code> resources.</li> <li>using either cert-manager or <code>openssl</code> to create TLS certificates.</li> </ul>"},{"location":"how-to/api-tls/#provision-keys-and-certificates-using-cert-manager","title":"Provision Keys and Certificates Using cert-manager","text":"<p>This method requires a working cert-manager deployment in your cluster. For an alternative way to provision keys and certificates, see the <code>openssl</code> section below.</p> <p>When using TLS, the LINSTOR API uses client certificates for authentication. It is good practice to have a separate CA just for these certificates.</p> <pre><code>---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: ca-bootstrapper\n  namespace: piraeus-datastore\nspec:\n  selfSigned: { }\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: linstor-api-ca\n  namespace: piraeus-datastore\nspec:\n  commonName: linstor-api-ca\n  secretName: linstor-api-ca\n  duration: 87600h # 10 years\n  isCA: true\n  usages:\n    - signing\n    - key encipherment\n    - cert sign\n  issuerRef:\n    name: ca-bootstrapper\n    kind: Issuer\n---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: linstor-api-ca\n  namespace: piraeus-datastore\nspec:\n  ca:\n    secretName: linstor-api-ca\n</code></pre> <p>Then, configure this new issuer to let the Operator provision the needed certificates:</p> <pre><code>---\napiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  apiTLS:\n    certManager:\n      name: linstor-api-ca\n      kind: Issuer\n</code></pre> <p>This completes the necessary steps for secure the LINSTOR API with TLS using cert-manager. Skip to the last section to verify TLS is working.</p>"},{"location":"how-to/api-tls/#provision-keys-and-certificates-using-openssl","title":"Provision Keys and Certificates Using <code>openssl</code>","text":"<p>If you completed the cert-manager section above, skip directly to the verifying the configuration below.</p> <p>This method requires the <code>openssl</code> program on the command line. For an alternative way to provision keys and certificates, see the cert-manager section above.</p> <p>First, create a new Certificate Authority using a new key and a self-signed certificate, valid for 10 years:</p> <pre><code>openssl req -new -newkey rsa:4096 -days 3650 -nodes -x509 -keyout ca.key -out ca.crt -subj \"/CN=linstor-api-ca\"\n</code></pre> <p>Then, create two new keys, one for the LINSTOR API server, one for all LINSTOR API clients:</p> <pre><code>openssl genrsa -out api-server.key 4096\nopenssl genrsa -out api-client.key 4096\n</code></pre> <p>Next, we will create a certificate for the server. The clients might use different shortened service names, so we need to specify multiple subject names:</p> <pre><code>cat /etc/ssl/openssl.cnf &gt; api-csr.cnf\ncat &gt;&gt; api-csr.cnf &lt;&lt;EOF\n[ v3_req ]\nsubjectAltName = @alt_names\n[ alt_names ]\nDNS.0 = linstor-controller.piraeus-datastore.svc.cluster.local\nDNS.1 = linstor-controller.piraeus-datastore.svc\nDNS.2 = linstor-controller\nEOF\nopenssl req -new -sha256 -key api-server.key -subj \"/CN=linstor-controller\" -config api-csr.cnf -extensions v3_req -out api-server.csr\nopenssl x509 -req -in api-server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -config api-csr.cnf -extensions v3_req -out api-server.crt -days 3650 -sha256\n</code></pre> <p>For the client certificate, simply setting one subject name is enough:</p> <pre><code>openssl req -new -sha256 -key api-client.key -subj \"/CN=linstor-client\" -out api-client.csr\nopenssl x509 -req -in api-client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out api-client.crt -days 3650 -sha256\n</code></pre> <p>Now, we create Kubernetes secrets from the created keys and certificates:</p> <pre><code>kubectl create secret generic linstor-api-tls -n piraeus-datastore --type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=api-server.crt --from-file=tls.key=api-server.key\nkubectl create secret generic linstor-client-tls -n piraeus-datastore --type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=api-client.crt --from-file=tls.key=api-client.key\n</code></pre> <p>Finally, configure the Operator resources to reference the newly created secrets. We configure the same client secret for all components for simplicity.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  apiTLS:\n    apiSecretName: linstor-api-tls\n    clientSecretName: linstor-client-tls\n    csiControllerSecretName: linstor-client-tls\n    csiNodeSecretName: linstor-client-tls\n</code></pre>"},{"location":"how-to/api-tls/#verifying-tls-configuration","title":"Verifying TLS Configuration","text":"<p>You can verify that the secure API is running by manually connecting to the HTTPS endpoint using <code>curl</code></p> <pre><code>$ kubectl exec -n piraeus-datastore deploy/linstor-controller -- curl --key /etc/linstor/client/tls.key --cert /etc/linstor/client/tls.crt --cacert /etc/linstor/client/ca.crt https://linstor-controller.piraeus-datastore.svc:3371/v1/controller/version\n{\"version\":\"1.20.2\",\"git_hash\":\"58a983a5c2f49eb8d22c89b277272e6c4299457a\",\"build_time\":\"2022-12-14T14:21:28+00:00\",\"rest_api_version\":\"1.16.0\"}%\n</code></pre> <p>If the command is successful, the API is using HTTPS and clients are able to connect with their certificates.</p> <p>If you see an error, make sure that the client certificates are trusted by the API secret, and vice versa. The following script provides a quick way to verify that one TLS certificate is trusted by another:</p> <pre><code>function k8s_secret_trusted_by() {\n    kubectl get secret -n piraeus-datastore -ogo-template='{{ index .data \"tls.crt\" | base64decode }}' \"$1\" &gt; $1.tls.crt\n    kubectl get secret -n piraeus-datastore -ogo-template='{{ index .data \"ca.crt\" | base64decode }}' \"$2\" &gt; $2.ca.crt\n    openssl verify -CAfile $2.ca.crt $1.tls.crt\n}\nk8s_secret_trusted_by linstor-client-tls linstor-api-tls\n# Expected output:\n# linstor-client-tls.tls.crt: OK\n</code></pre> <p>Another issue might be the API endpoint using a Certificate not using the expected service name. A typical error message for this issue would be:</p> <pre><code>curl: (60) SSL: no alternative certificate subject name matches target host name 'linstor-controller.piraeus-datastore.svc'\n</code></pre> <p>In this case, make sure you have specified the right subject names when provisioning the certificates.</p> <p>All available options are documented in the reference for <code>LinstorCluster</code>.</p>"},{"location":"how-to/drbd-host-networking/","title":"How to Use the Host Network for DRBD Replication","text":"<p>This guide shows you how to use the host network for DRBD\u00ae replication.</p> <p>By default, DRBD will use the container network to replicate volume data. This ensures replication works on a wide range of clusters without further configuration. It also enables use of <code>NetworkPolicy</code> to block unauthorized access to DRBD traffic. Since the network interface of the Pod is tied to the lifecycle of the Pod, it also means DRBD will temporarily disrupt replication when the LINSTOR\u00ae Satellite Pod is restarted.</p> <p>In contrast, using the host network for DRBD replication will cause replication to work independent of the LINSTOR Satellite Pod. The host network might also offer better performance than the container network. As a downside, you will have to manually ensure connectivity between Nodes on the relevant ports.</p> <p>To follow the steps in this guide, you should be familiar with editing <code>LinstorSatelliteConfiguration</code> resources.</p>"},{"location":"how-to/drbd-host-networking/#switch-from-container-network-to-host-network","title":"Switch from Container Network to Host Network","text":"<p>Switching from the default container network to host network is possible at any time. Existing DRBD resources will be reconfigured to use the host network interface.</p> <p>To configure the host network for the LINSTOR Satellite, apply the following configuration:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: host-network\nspec:\n  podTemplate:\n    spec:\n      hostNetwork: true\n</code></pre> <p>After the Satellite Pods are recreated, they will use the host network. Any existing DRBD resources are reconfigured to use the new IP Address instead.</p>"},{"location":"how-to/drbd-host-networking/#switch-from-host-network-to-container-network","title":"Switch from Host Network to Container Network","text":"<p>Switching back from host network to container network involves manually resetting the configured peer addresses used by DRBD. This can either be achieved by rebooting every node, or by manually resetting the address using <code>drbdadm</code>.</p>"},{"location":"how-to/drbd-host-networking/#switch-from-host-network-to-container-network-using-reboots","title":"Switch from Host Network to Container Network Using Reboots","text":"<p>First, you need to remove the <code>LinstorSatelliteConfiguration</code> that set <code>hostNetwork: true</code>:</p> <pre><code>$ kubectl delete linstorsatelliteconfigurations.piraeus.io host-network\nlinstorsatelliteconfiguration.piraeus.io \"host-network\" deleted\n</code></pre> <p>Then, reboot each cluster node, either one by one or multiple at once. In general, replication will not work between rebooted nodes and non-rebooted nodes. The non-rebooted nodes will continue to use the host network addresses, which are generally not reachable from the container network.</p> <p>After all nodes are rebooted, all resources are configured to use the container network, and all DRBD connections should be connected again.</p>"},{"location":"how-to/drbd-host-networking/#switch-from-host-network-to-container-network-using-drbdadm","title":"Switch from Host Network to Container Network Using <code>drbdadm</code>","text":"<p>During this procedure, ensure no new volumes or snapshots are created: otherwise the migration to the container network might not be applied to all resources.</p> <p>First, you need to temporarily stop all replication and suspend all DRBD volumes using <code>drbdadm suspend-io all</code>. The command is executed once on each LINSTOR Satellite Pod.</p> <pre><code>$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm suspend-io all\n$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm suspend-io all\n$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm suspend-io all\n</code></pre> <p>Next, you will need to disconnect all DRBD connections on all nodes.</p> <pre><code>$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm disconnect --force all\n$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm disconnect --force all\n$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm disconnect --force all\n</code></pre> <p>Now, we can safely reset all DRBD connection paths, which frees the connection to be moved to the container network.</p> <pre><code>$ kubectl exec ds/linstor-satellite.node1.example.com -- drbdadm del-path all\n$ kubectl exec ds/linstor-satellite.node2.example.com -- drbdadm del-path all\n$ kubectl exec ds/linstor-satellite.node3.example.com -- drbdadm del-path all\n</code></pre> <p>Finally, removing the <code>LinstorSatelliteConfiguration</code> that set <code>hostNetwork: true</code> will trigger the creation of new LINSTOR Satellite Pods using the container network:</p> <pre><code>$ kubectl delete linstorsatelliteconfigurations.piraeus.io host-network\nlinstorsatelliteconfiguration.piraeus.io \"host-network\" deleted\n</code></pre> <p>After the Pods are recreated and the LINSTOR Satellites are <code>Online</code>, the DRBD resource will be reconfigured and resume IO.</p>"},{"location":"how-to/drbd-loader/","title":"How to Configure the DRBD Module Loader","text":"<p>This guide shows you how to configure various aspects of the DRBD\u00ae Module Loader.</p> <p>To follow the steps in this guide, you should be familiar with editing <code>LinstorSatelliteConfiguration</code> resources.</p> <p>DRBD Module Loader is the component responsible for making the DRBD kernel module available, as well as loading other useful kernel modules for Piraeus.</p> <p>The following modules are loaded if available:</p> Module Purpose <code>libcrc32c</code> dependency for DRBD <code>nvmet_rdma</code>, <code>nvme_rdma</code> LINSTOR NVME layer <code>loop</code> LINSTOR when using loop devices as backing disks <code>dm_writecache</code> LINSTOR writecache layer <code>dm_cache</code> LINSTOR cache layer <code>dm_thin_pool</code> LINSTOR thin-provisioned storage <code>dm_snapshot</code> LINSTOR Snapshots <code>dm_crypt</code> LINSTOR encrypted volumes"},{"location":"how-to/drbd-loader/#disable-the-drbd-module-loader","title":"Disable the DRBD Module Loader","text":"<p>In some circumstances it might be necessary to disable the DRBD Module Loader entirely. For example, you are using an immutable operating system, and DRBD and other modules are loaded as part of the host configuration.</p> <p>To disable the DRBD Module Loader completely, use the following configuration:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: no-loader\nspec:\n  podTemplate:\n    spec:\n      initContainers:\n        - name: drbd-module-loader\n          $patch: delete\n</code></pre>"},{"location":"how-to/drbd-loader/#select-a-different-drbd-loader-version","title":"Select a Different DRBD Loader Version","text":"<p>By default, the Operator will try to find a DRBD Module Loader matching the host operating system. The host distribution is determined by inspecting the <code>.status.nodeInfo.osImage</code> field of the Kubernetes Node resource. A user-defined image can be used if the automatic mapping does not succeed or if you have different module loading requirements.</p> <p>This configuration overrides the chosen DRBD Module Loader image with a user-defined image <code>example.com/drbd-loader:v9</code>:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: custom-drbd-module-loader-image\nspec:\n  podTemplate:\n    spec:\n      initContainers:\n        - name: drbd-module-loader\n          image: example.com/drbd-loader:v9\n</code></pre> <p>Piraeus maintains the following images:</p> Image Distribution <code>quay.io/piraeusdatastore/drbd9-almalinux9</code> RedHat Enterprise Linux 9 rebuilds <code>quay.io/piraeusdatastore/drbd9-almalinux8</code> RedHat Enterprise Linux 8 rebuilds <code>quay.io/piraeusdatastore/drbd9-centos7</code> RedHat Enterprise Linux 7 rebuilds <code>quay.io/piraeusdatastore/drbd9-jammy</code> Ubuntu 22.04 <code>quay.io/piraeusdatastore/drbd9-focal</code> Ubuntu 20.04 <code>quay.io/piraeusdatastore/drbd9-bionic</code> Ubuntu 18.04 <code>quay.io/piraeusdatastore/drbd9-bookworm</code> Debian 12 <code>quay.io/piraeusdatastore/drbd9-bullseye</code> Debian 11 <code>quay.io/piraeusdatastore/drbd9-buster</code> Debian 10 <p>You can create a loader image for your own distribution using the Piraeus sources as reference.</p>"},{"location":"how-to/drbd-loader/#change-the-way-drbd-module-loader-loads-the-drbd-module","title":"Change the Way DRBD Module Loader Loads the DRBD Module","text":"<p>By default, DRBD Module Loader will try to build the kernel module from source. The loader can also be configured to load the module from a Debian or RPM package included in the image, or skip loading DRBD entirely.</p> <p>To change the behaviour of the DRBD Loader, set the <code>LB_HOW</code> environment variable:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: no-drbd-module-loader\nspec:\n  podTemplate:\n    spec:\n      initContainers:\n        - name: drbd-module-loader\n          env:\n            - name: LB_HOW\n              value: deps_only\n</code></pre> <code>LB_HOW</code> Loader behaviour <code>compile</code> The default. Build the DRBD module from source and try to load all optional modules from the host. <code>shipped_modules</code> Searches for <code>.rpm</code> or <code>.deb</code> packages at  <code>/pkgs</code> and inserts contained the DRBD modules. Optional modules are loaded from the host if available. <code>deps_only</code> Only try loading the optional modules. No DRBD module will be loaded."},{"location":"how-to/drbd-tls/","title":"How to Use Transport Layer Security for DRBD Replication","text":"<p>This guide shows you how to use Transport Layer Security (TLS) for DRBD\u00ae replication.</p> <p>TLS is a protocol designed to provide security, including privacy, integrity and authentication for network communications. Starting with DRBD 9.2.6, replication traffic in DRBD can be encrypted using TLS, using the Linux kernel TLS Upper Layer Protocol.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorCluster</code> and <code>LinstorSatelliteConfiguration</code> resources.</li> <li>configuring TLS certificates for internal traffic.</li> </ul>"},{"location":"how-to/drbd-tls/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Piraeus Operator &gt;= 2.3.0. The following command shows the deployed version, in this case v2.3.0:   <pre><code>$ kubectl get pods -l app.kubernetes.io/component=piraeus-operator -ojsonpath='{.items[*].spec.containers[?(@.name==\"manager\")].image}{\"\\n\"}'\nquay.io/piraeusdatastore/piraeus-operator:v2.3.0\n</code></pre></li> <li>Use a host operating system with kernel TLS offload enabled. TLS offload was added in Linux 4.19. The following   distributions are known to have TLS offload enabled:</li> <li>RHEL &gt;= 8.2</li> <li>Ubuntu &gt;= 22.04</li> <li>Debian &gt;= 12</li> <li>Have DRBD 9.2.6 or newer loaded. The following script shows the currently loaded DRBD version on all nodes:   <pre><code>for SATELLITE in $(kubectl get pods -l app.kubernetes.io/component=linstor-satellite -ojsonpath='{range .items[*]}{.metadata.name}{\"\\n\"}{end}'); do\n  echo \"$SATELLITE: $(kubectl exec $SATELLITE -- head -1 /proc/drbd)\"\ndone\n</code></pre></li> </ul>"},{"location":"how-to/drbd-tls/#configure-tls-between-linstor-controller-and-linstor-satellite","title":"Configure TLS Between LINSTOR Controller and LINSTOR Satellite","text":"<p>This step is covered in a dedicated guide. Follow the steps in the guide, but set the <code>tlsHandshakeDaemon</code> field in the <code>LinstorSatelliteConfiguration</code> resource:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: internal-tls\nspec:\n  internalTLS:\n    tlsHandshakeDaemon: true\n    # ...\n</code></pre> <p>This change will cause the Piraeus Operator to deploy an additional container named <code>ktls-utils</code> on the Satellite Pod.</p> <pre><code>$ kubectl logs -l app.kubernetes.io/component=linstor-satellite -c ktls-utils\ntlshd[1]: Built from ktls-utils 0.10 on Oct  4 2023 07:26:06\ntlshd[1]: Built from ktls-utils 0.10 on Oct  4 2023 07:26:06\ntlshd[1]: Built from ktls-utils 0.10 on Oct  4 2023 07:26:06\n</code></pre>"},{"location":"how-to/drbd-tls/#configure-tls-for-drbd","title":"Configure TLS for DRBD","text":"<p>To instruct LINSTOR\u00ae to configure TLS for DRBD, the <code>DrbdOptions/Net/tls</code> property needs to be set to <code>yes</code>. This can be done directly on the <code>LinstorCluster</code> resource, so it automatically applies to all resources, or as part of the parameters in a StorageClass.</p> <p>To apply the property cluster-wide, add the following property entry to your <code>LinstorCluster</code> resource:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  properties:\n    - name: DrbdOptions/Net/tls\n      value: \"yes\"\n</code></pre> <p>To apply the property only for certain StorageClasses, add the following parameter to the selected StorageClasses:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: example\nprovisioner: linstor.csi.linbit.com\nparameters:\n  property.linstor.csi.linbit.com/DrbdOptions/Net/tls: \"yes\"\n  # ...\n</code></pre>"},{"location":"how-to/drbd-tls/#switch-existing-resources-to-tls","title":"Switch Existing Resources to TLS","text":"<p>DRBD does not support online reconfiguration to use DRBD. To switch existing resources to use TLS, for example by setting the property on the <code>LinstorCluster</code> resource, you need to perform the following steps on all nodes:</p> <p>First, you need to temporarily stop all replication and suspend all DRBD volumes using <code>drbdadm suspend-io all</code>. The command is executed once on each LINSTOR Satellite Pod.</p> <pre><code>$ kubectl exec node1.example.com -- drbdadm suspend-io all\n$ kubectl exec node2.example.com -- drbdadm suspend-io all\n$ kubectl exec node3.example.com -- drbdadm suspend-io all\n</code></pre> <p>Next, you will need to disconnect all DRBD connections on all nodes.</p> <pre><code>$ kubectl exec node1.example.com -- drbdadm disconnect --force all\n$ kubectl exec node2.example.com -- drbdadm disconnect --force all\n$ kubectl exec node3.example.com -- drbdadm disconnect --force all\n</code></pre> <p>Now, we can safely reconnect DRBD connection paths, which configures the TLS connection parameters.</p> <pre><code>$ kubectl exec node1.example.com -- drbdadm adjust all\n$ kubectl exec node2.example.com -- drbdadm adjust all\n$ kubectl exec node3.example.com -- drbdadm adjust all\n</code></pre>"},{"location":"how-to/drbd-tls/#confirm-drbd-uses-tls","title":"Confirm DRBD Uses TLS","text":"<p>To confirm that DRBD is using TLS for replication, check the following resources.</p> <p>First, confirm that the <code>ktls-utils</code> container performed the expected handshakes:</p> <pre><code>$ kubectl logs -l app.kubernetes.io/component=linstor-satellite -c ktls-utils\n...\nHandshake with node2.example.com (10.127.183.183) was successful\nHandshake with node2.example.com (10.127.183.183) was successful\n...\nHandshake with node3.example.com (10.125.97.42) was successful\nHandshake with node3.example.com (10.125.97.42) was successful\n...\n</code></pre> <p>[!NOTE] The following messages are expected when running <code>ktls-utils</code> in a container and can be safely ignored: <pre><code>File /etc/tlshd.d/tls.key: expected mode 600\nadd_key: Bad message\n</code></pre></p> <p>Next, check the statistics on TLS sessions controlled by the kernel on each node. You should see an equal, nonzero number of <code>TlsRxSw</code> and <code>TlsRxSw</code>.</p> <pre><code>$ kubectl exec node1.example.com -- cat /proc/net/tls_stat\nTlsCurrTxSw                         4\nTlsCurrRxSw                         4\nTlsCurrTxDevice                     0\nTlsCurrRxDevice                     0\nTlsTxSw                             4\nTlsRxSw                             4\nTlsTxDevice                         0\nTlsRxDevice                         0\nTlsDecryptError                     0\nTlsRxDeviceResync                   0\nTlsDecryptRetry                     0\nTlsRxNoPadViolation                 0\n</code></pre> <p>[!NOTE] If your network card supports TLS offloading, you might see <code>TlsTxDevice</code> and <code>TlsRxDevice</code> being nonzero instead.</p>"},{"location":"how-to/external-controller/","title":"How to Use an Existing LINSTOR Cluster","text":"<p>This guide shows you how to connect Piraeus with an existing LINSTOR\u00ae Cluster, managed outside Kubernetes.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorCluster</code> resources.</li> </ul>"},{"location":"how-to/external-controller/#configure-the-piraeus-resources","title":"Configure the Piraeus Resources","text":"<p>To use an externally managed LINSTOR Cluster, specify the URL of the LINSTOR Controller in the <code>LinstorCluster</code> resource. In the following example, the Controller is reachable at <code>http://linstor-controller.example.com:3370</code>:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  externalController:\n    url: http://linstor-controller.example.com:3370\n</code></pre> <p>After applying the <code>LinstorCluster</code>, Piraeus will make the following changes to the default deployment:</p> <ul> <li>The <code>linstor-controller</code> Deployment is removed</li> <li>The <code>linstor-csi-controller</code> and <code>linstor-csi-node</code> resources will reference the external controller.</li> </ul>"},{"location":"how-to/external-controller/#configuring-host-networking-for-linstor-satellites","title":"Configuring Host Networking for LINSTOR Satellites","text":"<p>Normally the Pod network is not reachable from outside the Kubernetes Cluster. In this case the existing LINSTOR Controller won't be able to communicate with the Satellites in the Kubernetes cluster. So you should configure your Satellites to use host networking.</p> <p>To use host networking, apply the following configuration resource:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: host-network\nspec:\n  podTemplate:\n    spec:\n      hostNetwork: true\n</code></pre>"},{"location":"how-to/external-controller/#verifying-the-configuration","title":"Verifying the Configuration","text":"<p>To verify that the external LINSTOR Controller is configured, check:</p> <ul> <li>The <code>Available</code> condition on the <code>LinstorCluster</code> resource reports the expected Controller URL:   <pre><code>$ kubectl get LinstorCluster -ojsonpath='{.items[].status.conditions[?(@.type==\"Available\")].message}{\"\\n\"}'\nController 1.20.3 (API: 1.16.0, Git: 8d19a891df018f6e3d40538d809904f024bfe361) reachable at 'http://linstor-controller.example.com:3370'\n</code></pre></li> <li>The <code>linstor-csi-controller</code> Deployment uses the expected URL:   <pre><code>$ kubectl get -n piraeus-datastore deployment linstor-csi-controller -ojsonpath='{.spec.template.spec.containers[?(@.name==\"linstor-csi\")].env[?(@.name==\"LS_CONTROLLERS\")].value}{\"\\n\"}'\nhttp://linstor-controller.example.com:3370\n</code></pre></li> <li>The <code>linstor-csi-node</code> Deployment uses the expected URL:   <pre><code>$ kubectl get -n piraeus-datastore daemonset linstor-csi-node -ojsonpath='{.spec.template.spec.containers[?(@.name==\"linstor-csi\")].env[?(@.name==\"LS_CONTROLLERS\")].value}{\"\\n\"}'\nhttp://linstor-controller.example.com:3370\n</code></pre></li> <li>The Kubernetes nodes are registered as Satellite nodes on the LINSTOR Controller:   <pre><code>$ kubectl get nodes -owide\nNAME               STATUS   ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                    KERNEL-VERSION                 CONTAINER-RUNTIME\nk8s-1-26-10.test   Ready    control-plane   22m   v1.26.3   192.168.122.10   &lt;none&gt;        AlmaLinux 9.1 (Lime Lynx)   5.14.0-162.22.2.el9_1.x86_64   containerd://1.6.20\n...\n$ linstor node list\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u250a Node             \u250a NodeType  \u250a Addresses                   \u250a State  \u250a\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u250a k8s-1-26-10.test \u250a SATELLITE \u250a 192.168.122.10:3366 (PLAIN) \u250a Online \u250a\n...\n</code></pre></li> </ul>"},{"location":"how-to/flatcar/","title":"How to Load DRBD on Flatcar Container Linux","text":"<p>This guide shows you how to set up the DRBD\u00ae Module Loader when using Flatcar Container Linux.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorSatelliteConfiguration</code> resources.</li> </ul>"},{"location":"how-to/flatcar/#configure-the-drbd-module-loader","title":"Configure the DRBD Module Loader","text":"<p>Flatcar Container Linux uses a read-only <code>/usr</code> file system. For building DRBD from source on Flatcar Container Linux, the default bind mount for the not existing <code>/usr/src</code> directory needs to be disabled for the <code>drbd-module-loader</code> init container.</p> <p>To change the configuration for the <code>drbd-module-loader</code> container, apply the following <code>LinstorSatelliteConfiguration</code>:</p> <pre><code>---\napiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: no-usr-src-mount\nspec:\n  podTemplate:\n    spec:\n      volumes:\n        - name: usr-src\n          $patch: delete\n      initContainers:\n        - name: drbd-module-loader\n          volumeMounts:\n            - mountPath: /usr/src\n              name: usr-src\n              $patch: delete\n</code></pre>"},{"location":"how-to/http-proxy/","title":"How to Use Piraeus Datastore with an HTTP Proxy","text":"<p>This guide shows you how to configure the DRBD\u00ae Module Loader when using a HTTP Proxy.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorSatelliteConfiguration</code> resources.</li> <li>using the <code>kubectl</code> command line tool to access the Kubernetes cluster.</li> </ul>"},{"location":"how-to/http-proxy/#configuration","title":"Configuration","text":"<p>We will use environment variables to configure the proxy, this tells the drbd-module-loader component to use the proxy for outgoing communication.</p> <p>Configure the sample below according to your environment and apply the configuration using <code>kubectl apply -f filename.yml</code>.</p> <p>This sample configuration assumes that a HTTP proxy is reacheable at <code>http://10.0.0.1:3128</code>.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: http-proxy\nspec:\n  podTemplate:\n    spec:\n      initContainers:\n        - name: drbd-module-loader\n          env:\n            - name: HTTP_PROXY\n              value: http://10.0.0.1:3128 # Add your proxy connection here\n            - name: HTTPS_PROXY\n              value: http://10.0.0.1:3128 # Add your proxy connection here\n            - name: NO_PROXY\n              value: localhost,127.0.0.1,10.0.0.0/8,172.16.0.0/12 # Add internal IP ranges and domains here\n</code></pre>"},{"location":"how-to/install-kernel-headers/","title":"Installing Linux Kernel Headers","text":"<p>Piraeus Datastore uses DRBD\u00ae to mirror your volume data to one or more nodes. This document aims to describe the way to install the necessary files for common Linux distributions.</p> <p>To check if your nodes already have the necessary files installed, try running:</p> <pre><code>$ test -d /lib/modules/$(uname -r)/build/ &amp;&amp; echo found headers\n</code></pre> <p>If the command prints <code>found headers</code>, your nodes are good to go.</p>"},{"location":"how-to/install-kernel-headers/#installing-on-ubuntu","title":"Installing on Ubuntu","text":"<p>Installation of Linux Kernel headers on Ubuntu can be done through the <code>apt</code> package manager:</p> <pre><code>$ sudo apt-get update\n$ sudo apt-get install -y linux-headers-$(uname -r)\n</code></pre> <p>In addition, you can install the <code>linux-headers-virtual</code> package. This causes <code>apt upgrade</code> to install the headers matching any newly installed kernel versions.</p> <pre><code>$ sudo apt-get update\n$ sudo apt-get install -y linux-headers-virtual\n</code></pre>"},{"location":"how-to/install-kernel-headers/#installing-on-debian","title":"Installing on Debian","text":"<p>Installation of Linux Kernel headers on Debian can be done through the <code>apt</code> package manager:</p> <pre><code>$ sudo apt-get update\n$ sudo apt-get install -y linux-headers-$(uname -r)\n</code></pre> <p>In addition, you can install an additional package, that causes <code>apt</code> to also install Kernel headers on upgrade:</p> <pre><code>$ sudo apt-get update\n$ sudo apt-get install -y linux-headers-$(dpkg --print-architecture)\n</code></pre>"},{"location":"how-to/install-kernel-headers/#installing-on-redhat-enterprise-linux","title":"Installing on RedHat Enterprise Linux","text":"<p>Installing on RedHat Enterprise Linux or compatible distributions, such as AlmaLinux or Rocky Linux can be done through the <code>dnf</code> package manager:</p> <pre><code>$ sudo dnf install -y kernel-devel-$(uname -r)\n</code></pre> <p>In addition, you can install an additional package, that causes <code>yum</code> to also install Kernel headers on upgrade:</p> <pre><code>$ sudo dnf install -y kernel-devel\n</code></pre>"},{"location":"how-to/internal-tls/","title":"How to Configure TLS Between LINSTOR Controller and LINSTOR Satellite","text":"<p>This guide shows you how to set up TLS between LINSTOR\u00ae Controller and LINSTOR Satellite.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorCluster</code> and <code>LinstorSatelliteConfiguration</code> resources.</li> <li>using the LINSTOR client to verify success.</li> <li>using either cert-manager or <code>openssl</code> to create TLS certificates.</li> </ul>"},{"location":"how-to/internal-tls/#provision-keys-and-certificates-using-cert-manager","title":"Provision Keys and Certificates Using cert-manager","text":"<p>This method requires a working cert-manager deployment in your cluster. For an alternative way to provision keys and certificates, see the <code>openssl</code> section below.</p> <p>LINSTOR Controller and Satellite only need to trust each other, so it is good practice to have a CA only for those components. Apply the following YAML to create a new Issuer:</p> <pre><code>---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: ca-bootstrapper\n  namespace: piraeus-datastore\nspec:\n  selfSigned: { }\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: linstor-internal-ca\n  namespace: piraeus-datastore\nspec:\n  commonName: linstor-internal-ca\n  secretName: linstor-internal-ca\n  duration: 87600h # 10 years\n  isCA: true\n  usages:\n    - signing\n    - key encipherment\n    - cert sign\n  issuerRef:\n    name: ca-bootstrapper\n    kind: Issuer\n---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: linstor-internal-ca\n  namespace: piraeus-datastore\nspec:\n  ca:\n    secretName: linstor-internal-ca\n</code></pre> <p>Then, configure this new issuer to let the Operator provision the needed certificates:</p> <pre><code>---\napiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  internalTLS:\n    certManager:\n      name: linstor-internal-ca\n      kind: Issuer\n---\napiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: internal-tls\nspec:\n  internalTLS:\n    certManager:\n      name: linstor-internal-ca\n      kind: Issuer\n</code></pre> <p>This completes the necessary steps for secure TLS connections between LINSTOR Controller and Satellite using cert-manager. Skip to the last section to verify TLS is working.</p>"},{"location":"how-to/internal-tls/#provision-keys-and-certificates-using-openssl","title":"Provision Keys and Certificates Using <code>openssl</code>","text":"<p>If you completed the cert-manager section above, skip directly to the verifying the configuration below.</p> <p>This method requires the <code>openssl</code> program on the command line. For an alternative way to provision keys and certificates, see the cert-manager section above.</p> <p>First, create a new Certificate Authority using a new key and a self-signed certificate, valid for 10 years:</p> <pre><code>openssl req -new -newkey rsa:4096 -days 3650 -nodes -x509 -keyout ca.key -out ca.crt -subj \"/CN=linstor-internal-ca\"\n</code></pre> <p>Then, create two new keys, one for the LINSTOR Controller, one for all Satellites:</p> <pre><code>openssl genrsa -out controller.key 4096\nopenssl genrsa -out satellite.key 4096\n</code></pre> <p>Next, we will create a certificate for each key, valid for 10 years, signed by the Certificate Authority:</p> <pre><code>openssl req -new -sha256 -key controller.key -subj \"/CN=linstor-controller\" -out controller.csr\nopenssl req -new -sha256 -key satellite.key -subj \"/CN=linstor-satellite\" -out satellite.csr\nopenssl x509 -req -in controller.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out controller.crt -days 3650 -sha256\nopenssl x509 -req -in satellite.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out satellite.crt -days 3650 -sha256\n</code></pre> <p>Now, we create Kubernetes secrets from the created keys and certificates:</p> <pre><code>kubectl create secret generic linstor-controller-internal-tls -n piraeus-datastore --type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=controller.crt --from-file=tls.key=controller.key\nkubectl create secret generic linstor-satellite-internal-tls -n piraeus-datastore --type=kubernetes.io/tls --from-file=ca.crt=ca.crt --from-file=tls.crt=satellite.crt --from-file=tls.key=satellite.key\n</code></pre> <p>Finally, configure the Operator resources to reference the newly created secrets:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  internalTLS:\n    secretName: linstor-controller-internal-tls\n---\napiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: internal-tls\nspec:\n  internalTLS:\n    secretName: linstor-satellite-internal-tls\n</code></pre>"},{"location":"how-to/internal-tls/#verifying-tls-configuration","title":"Verifying TLS Configuration","text":"<p>You can verify the secure TLS connection between LINSTOR Controller and Satellite by checking the output of the <code>linstor node list</code> command. If TLS is enabled, you will see <code>(SSL)</code> next to the active satellite address:</p> <pre><code>$ kubectl exec -n piraeus-datastore deploy/linstor-controller -- linstor node list\n+---------------------------------------------------------------------+\n| Node               | NodeType  | Addresses                 | State  |\n|=====================================================================|\n| node01.example.com | SATELLITE | 10.116.72.142:3367 (SSL)  | Online |\n| node02.example.com | SATELLITE | 10.127.183.140:3367 (SSL) | Online |\n| node03.example.com | SATELLITE | 10.125.97.50:3367 (SSL)   | Online |\n+---------------------------------------------------------------------+\n</code></pre> <p>If you see <code>(PLAIN)</code> instead, this indicates that the TLS configuration was not applied successfully. Check the status of the <code>LinstorCluster</code> and <code>LinstorSatellite</code> resource.</p> <p>If you see <code>(SSL)</code>, but the node remains offline, this usually indicates that a certificate is not trusted by the other party. Verify that the Controller's <code>tls.crt</code> is trusted by the Satellite's <code>ca.crt</code> and vice versa. The following script provides a quick way to verify that one TLS certificate is trusted by another:</p> <pre><code>function k8s_secret_trusted_by() {\n    kubectl get secret -n piraeus-datastore -ogo-template='{{ index .data \"tls.crt\" | base64decode }}' \"$1\" &gt; $1.tls.crt\n    kubectl get secret -n piraeus-datastore -ogo-template='{{ index .data \"ca.crt\" | base64decode }}' \"$2\" &gt; $2.ca.crt\n    openssl verify -CAfile $2.ca.crt $1.tls.crt\n}\nk8s_secret_trusted_by satellite-tls controller-tls\n# Expected output:\n# satellite-tls.tls.crt: OK\n</code></pre> <p>All available options are documented in the reference for <code>LinstorCluster</code> and <code>LinstorSatelliteConfiguration</code>.</p>"},{"location":"how-to/k0s/","title":"How to Configure Piraeus Datastore on k0s","text":"<p>This guide shows you how to configure Piraeus Datastore when using k0s.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorCluster</code> resources.</li> </ul>"},{"location":"how-to/k0s/#configure-the-csi-driver","title":"Configure the CSI Driver","text":"<p>Because k0s store their state in a separate directory (<code>/var/lib/k0s</code>) the LINSTOR CSI Driver needs to be updated to use a new path for mounting volumes.</p> <p>To change the LINSTOR CSI Driver, so that it uses the k0s state paths, apply the following <code>LinstorCluster</code>:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  csiNode:\n    enabled: true\n    podTemplate:\n      spec:\n        containers:\n          - name: linstor-csi\n            volumeMounts:\n            - mountPath: /var/lib/k0s/kubelet\n              name: publish-dir\n              mountPropagation: Bidirectional\n          - name: csi-node-driver-registrar\n            args:\n            - --v=5\n            - --csi-address=/csi/csi.sock\n            - --kubelet-registration-path=/var/lib/k0s/kubelet/plugins/linstor.csi.linbit.com/csi.sock\n            - --health-port=9809\n        volumes:\n         - name: publish-dir\n           hostPath:\n             path: /var/lib/k0s/kubelet\n         - name: registration-dir\n           hostPath:\n             path: /var/lib/k0s/kubelet/plugins_registry\n         - name: plugin-dir\n           hostPath:\n              path: /var/lib/k0s/kubelet/plugins/linstor.csi.linbit.com\n</code></pre>"},{"location":"how-to/linstor-affinity-controller/","title":"How to Deploy the LINSTOR Affinity Controller","text":"<p>This guide shows you how to deploy the LINSTOR Affinity Controller for Piraeus Datastore.</p> <p>The LINSTOR Affinity Controller keeps the affinity of your volumes in sync between Kubernetes and LINSTOR.</p> <p>When using a strict volume affinity setting, such as <code>allowRemoteVolumeAccess: false</code>, the Persistent Volume (PV) resource created by Piraeus Datastore will have a fixed affinity. When the volume is moved to a different node, for example because one of the existing replicas is being evacuated, the PV is not updated automatically.</p> <p>The LINSTOR Affinity controller watches PVs and LINSTOR resource and keeps the affinity up-to-date.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>Deploying workloads in Kubernetes using <code>helm</code></li> </ul>"},{"location":"how-to/linstor-affinity-controller/#add-the-piraeus-datastore-chart-repository","title":"Add the Piraeus Datastore Chart Repository","text":"<p>Piraeus Datastore maintains a helm chart repository for commonly deployed components, including the LINSTOR Affinity Controller. To add the repository to your helm configuration, run:</p> <pre><code>$ helm repo add piraeus-charts https://piraeus.io/helm-charts/\n</code></pre>"},{"location":"how-to/linstor-affinity-controller/#deploy-the-linstor-affinity-controller","title":"Deploy the LINSTOR Affinity Controller","text":"<p>After adding the repository, deploy the LINSTOR Affinity Controller:</p> <pre><code>$ helm install linstor-affinity-controller piraeus-charts/linstor-affinity-controller\nNAME: linstor-affinity-controller\nLAST DEPLOYED: Mon Dec  4 09:14:07 2023\nNAMESPACE: piraeus-datastore\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nLINSTOR Affinity Controller deployed.\n\nUsed LINSTOR URL: http://linstor-controller.piraeus-datastore.svc:3370\n</code></pre> <p>If you deploy the LINSTOR Affinity Controller to the same namespace as the Piraeus Operator, the deployment will automatically determine the necessary parameters for connecting to LINSTOR.</p> <p>In some cases, helm may not be able to determine the connection parameters. In this case, you need to manually provide the following values:</p> <pre><code>linstor:\n  # The URL of the LINSTOR Controller API. This example contains the default value.\n  endpoint: http://linstor-controller.piraeus-datastore.svc:3370\n  # This is the default URL when using TLS for securing the API\n  #endpoint: https://linstor-controller.piraeus-datastore.svc:3371\n  # This is the name of the secret containing TLS key and certificates for connecting to the LINSTOR API with TLS.\n  clientSecret: \"\"\noptions:\n  # This is the namespace used by Piraeus Operator to sync Kubernetes Node labels with LINSTOR node properties.\n  # For Piraeus Operator, this needs to be set to the following value:\n  propertyNamespace: Aux/topology\n</code></pre>"},{"location":"how-to/microk8s/","title":"How to Configure Piraeus Datastore on MicroK8s","text":"<p>This guide shows you how to configure Piraeus Datastore when using MicroK8s.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorCluster</code> resources.</li> </ul>"},{"location":"how-to/microk8s/#configure-the-csi-driver","title":"Configure the CSI Driver","text":"<p>MicroK8s is distributed as a Snap. Because Snaps store their state in a separate directory (<code>/var/snap</code>) the LINSTOR CSI Driver needs to be updated to use a new path for mounting volumes.</p> <p>To change the LINSTOR CSI Driver, so that it uses the MicroK8s state paths, apply the following <code>LinstorCluster</code>:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  patches:\n    - target:\n        name: linstor-csi-node\n        kind: DaemonSet\n      patch: |\n        apiVersion: apps/v1\n        kind: DaemonSet\n        metadata:\n          name: linstor-csi-node\n        spec:\n          template:\n            spec:\n              containers:\n              - name: linstor-csi\n                volumeMounts:\n                - mountPath: /var/lib/kubelet\n                  name: publish-dir\n                  $patch: delete\n                - mountPath: /var/snap/microk8s/common/var/lib/kubelet\n                  name: publish-dir\n                  mountPropagation: Bidirectional\n</code></pre>"},{"location":"how-to/monitoring/","title":"How to Deploy Monitoring with Prometheus Operator","text":"<p>Piraeus Datastore offers integration with the Prometheus monitoring stack.</p> <p>The integration configures:</p> <ul> <li>Metrics scraping for the LINSTOR and DRBD state.</li> <li>Alerts based on the cluster state</li> <li>A Grafana dashboard</li> </ul> <p>To complete this guide, you should be familiar with:</p> <ul> <li>Deploying workloads in Kubernetes using <code>helm</code></li> <li>Deploying resources using <code>kubectl</code></li> </ul>"},{"location":"how-to/monitoring/#deploying-prometheus-operator","title":"Deploying Prometheus Operator","text":"<p>\ud83d\uddd2\ufe0f NOTE: If you already have a working Prometheus Operator deployment, skip this step.</p> <p>First deploy the Prometheus Operator. A simple way to deploy it, is to use the helm chart provided by the Prometheus Community.</p> <p>First, add the helm chart repository to your local helm configuration:</p> <pre><code>$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n</code></pre> <p>Then, deploy the <code>kube-prometheus-stack</code> chart. This chart will set up Prometheus, AlertManager and Grafana for your cluster. Configure it to search for monitoring and alerting rules in all namespaces:</p> <pre><code>$ helm install --create-namespace -n monitoring prometheus prometheus-community/kube-prometheus-stack \\\n  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\\n  --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \\\n  --set prometheus.prometheusSpec.ruleSelectorNilUsesHelmValues=false\n</code></pre> <p>\ud83d\uddd2\ufe0f NOTE: By default, the deployment will only monitor resources in the <code>kube-system</code> and its own namespace. Piraeus Datastore is usually deployed in a different namespace, so Prometheus needs to be configured to watch this namespace. In the example above, this is achieved by setting the various <code>*NilUsesHelmValues</code> parameters to <code>false</code>.</p>"},{"location":"how-to/monitoring/#deploying-monitoring-and-alerting-rules-for-piraeus-datastore","title":"Deploying Monitoring and Alerting Rules for Piraeus Datastore","text":"<p>After creating a Prometheus Operator deployment and configuring it to watch all namespaces, apply the monitoring and alerting resources for Piraeus Datastore:</p> <pre><code>$ kubectl apply --server-side -n piraeus-datastore -k \"https://github.com/piraeusdatastore/piraeus-operator//config/extras/monitoring?ref=v2\"\n</code></pre>"},{"location":"how-to/monitoring/#verify-monitoring","title":"Verify Monitoring","text":"<p>Verify that the monitoring configuration is working by checking the prometheus console. First, get access to the prometheus console from your local browser by forwarding it to local port 9090:</p> <pre><code>$ kubectl port-forward -n monitoring services/prometheus-kube-prometheus-prometheus 9090:9090\n</code></pre> <p>Now, open http://localhost:9090/graph and display the <code>linstor_info</code> and <code>drbd_version</code> metrics:</p> <p> </p> <p>To view the dashboard, forward the grafana service to local port 3000:</p> <pre><code>$ kubectl port-forward -n monitoring services/prometheus-grafana 3000:http-web\n</code></pre> <p>Now, open http://localhost:3000 and log in. If using the example deployment from above, use username <code>admin</code> and password <code>prom-operator</code> to gain access. Then, select \"Piraeus Datastore\" from the available dashboards:</p> <p></p>"},{"location":"how-to/network-policy/","title":"How to Deploy a NetworkPolicy for Piraeus Datastore","text":"<p>Network Policies offer a way to restrict network access of specific pods. They can be used to block undesired network connections to specific Pods.</p> <p>Piraeus Datastore provides basic Network Policies that configure restricted access to the LINSTOR Satellites. The provided policy restricts incoming network connections to LINSTOR Satellite to only the following sources:</p> <ul> <li>Other LINSTOR Satellite Pods to allow for DRBD replication</li> <li>The LINSTOR Controller Pod, to facilitate LINSTOR Cluster operations</li> <li>Metrics collection from DRBD Reactor.</li> </ul> <p>To deploy the Network Policy, apply the following remote resource:</p> <pre><code>$ kubectl apply --server-side -k \"https://github.com/piraeusdatastore/piraeus-operator//config/extras/network-policy?ref=v2\"\nnetworkpolicy.networking.k8s.io/satellite serverside-applied\n</code></pre>"},{"location":"how-to/openshift/","title":"How to Load DRBD on OpenShift","text":"<p>This guide shows you how to set the DRBD\u00ae Module Loader when using OpenShift.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorSatelliteConfiguration</code> resources.</li> <li>using the <code>oc</code> command line tool to access the OpenShift cluster.</li> </ul>"},{"location":"how-to/openshift/#find-the-driver-toolkit-image","title":"Find the Driver Toolkit Image","text":"<p>The Driver Toolkit image contains the necessary files for building DRBD. Every OpenShift release includes a version of the Driver Toolkit image matching the operating system version of the release. You can use <code>oc</code> to print the correct image version:</p> <pre><code>$ oc adm release info --image-for=driver-toolkit\nquay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1328c4e7944b6d8eda40a8f789471a1aec63abda75ac1199ce098b965ec16709\n</code></pre>"},{"location":"how-to/openshift/#configure-the-drbd-module-loader","title":"Configure the DRBD Module Loader","text":"<p>By default, the DRBD Module Loader will try to find the necessary header files to build DRBD from source on the host system. In OpenShift, these header files are not included in the host system. Instead, they are included in the Driver Toolkit image.</p> <p>To change the DRBD Module Loader, so that it uses the header files included in the Driver Toolkit image, apply the following <code>LinstorSatelliteConfiguration</code>:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: openshift-loader-override\nspec:\n  podTemplate:\n    spec:\n      volumes:\n        - name: usr-src\n          emptyDir: { }\n          hostPath:\n            $patch: delete\n      initContainers:\n        - name: kernel-header-copy\n          image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1328c4e7944b6d8eda40a8f789471a1aec63abda75ac1199ce098b965ec16709\n          args:\n            - cp\n            - -avt\n            - /container/usr/src\n            - /usr/src/kernels\n          volumeMounts:\n            - name: usr-src\n              mountPath: /container/usr/src\n          securityContext:\n            privileged: true\n        - name: drbd-module-loader\n          securityContext:\n            privileged: true\n</code></pre> <p>NOTE: Replace the <code>image</code> of the <code>kernel-header-copy</code> container with the image returned by <code>oc adm release info</code>.</p> <p>After the automatic restart of the LINSTOR\u00ae Satellite Pods, DRBD will be built from source, using the correct header files.</p>"},{"location":"how-to/openshift/#openshift-update-considerations","title":"OpenShift Update Considerations","text":"<p>OpenShift updates also update the host operating system. Since every node will be rebooted during the update, the DRBD Module loader needs to rebuild DRBD once after the restart for the new host operating system.</p> <p>To ensure that the DRBD rebuild is successful, you need to update the image in the <code>openshift-loader-override</code> configuration before starting the upgrade process. You can extract the Driver Toolkit image for the new OpenShift version before starting the upgrade like this:</p> <pre><code>$ oc adm release info 4.12.2 --image-for=driver-toolkit\nquay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:1328c4e7944b6d8eda40a8f789471a1aec63abda75ac1199ce098b965ec16709\n</code></pre>"},{"location":"how-to/restore-linstor-db/","title":"How to Restore a LINSTOR Database Backup","text":"<p>This guide shows you how to restore a LINSTOR\u00ae Controller from a database backup. A backup is created automatically on every database migration of the default <code>k8s</code> database.</p> <p>[!CAUTION] Restoring from a backup means all changes made to the Cluster state after the backup was taken are lost. This includes information about Persistent Volumes and Volume Snapshots that where created after the backup.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>using the <code>kubectl</code> command line tool to access the Kubernetes cluster.</li> </ul>"},{"location":"how-to/restore-linstor-db/#find-the-latest-backup","title":"Find the Latest Backup","text":"<p>The backup is stored in Kubernetes Secrets. List all backups by using the following command:</p> <pre><code>$ kubectl get secrets --field-selector type=piraeus.io/linstor-backup --sort-by .metadata.creationTimestamp -ocustom-columns=\"NAME:.metadata.name,CREATED-AT:metadata.creationTimestamp,VERSION:.metadata.annotations.piraeus\\.io/linstor-version\"\nNAME                                                     CREATED-AT             VERSION\nlinstor-backup-for-linstor-controller-db7fbfd95-zsfmm    2024-11-04T07:54:29Z   LINSTOR Controller 1.27.0\nlinstor-backup-for-linstor-controller-745d54bf99-544hf   2024-11-04T08:03:59Z   LINSTOR Controller 1.29.1\n</code></pre> <p>Select the backup you want to restore, making note of the name. For example, to restore the LINSTOR 1.27.0 version, we set:</p> <pre><code>$ BACKUP_NAME=linstor-backup-for-linstor-controller-db7fbfd95-zsfmm\n</code></pre>"},{"location":"how-to/restore-linstor-db/#temporarily-stop-the-linstor-controller-deployment","title":"Temporarily stop the LINSTOR Controller Deployment","text":"<p>To safely restore the database, ensure that the LINSTOR Controller is shut down by scaling the Piraeus Operator and LINSTOR Controller deployment to 0 replicas:</p> <pre><code>$ kubectl scale deployment piraeus-operator-controller-manager --replicas 0\ndeployment.apps/piraeus-operator-controller-manager scaled\n$ kubectl scale deployment linstor-controller --replicas 0\ndeployment.apps/linstor-controller scaled\n</code></pre>"},{"location":"how-to/restore-linstor-db/#create-a-new-backup-of-the-current-cluster-state","title":"Create a New Backup of the Current Cluster State","text":"<p>Since the restore process is destructive, first create a backup of the current database:</p> <pre><code>$ mkdir backup\n$ cd backup\n$ kubectl api-resources --api-group=internal.linstor.linbit.com -oname | xargs --no-run-if-empty kubectl get crds -oyaml &gt; crds.yaml\n$ kubectl api-resources --api-group=internal.linstor.linbit.com -oname | xargs --no-run-if-empty -I {} sh -c 'kubectl get {} -oyaml &gt; {}.yaml'\n</code></pre>"},{"location":"how-to/restore-linstor-db/#restore-the-database","title":"Restore the Database","text":"<p>Copy and unpack the selected backup to a local directory by using the following commands:</p> <pre><code>$ mkdir restore\n$ cd restore\n# Replace $BACKUP_NAME with your selected backup name\n$ kubectl get secrets --sort-by=.metadata.name -l piraeus.io/backup=$BACKUP_NAME -ogo-template='{{range .items}}{{index .data \"backup.tar.gz\" | base64decode}}{{end}}' &gt; backup.tar.gz\n$ tar -xvf backup.tar.gz\ncrds.yaml\n[...]\n</code></pre> <p>Then, replace the current database with the database from the backup:</p> <pre><code>$ kubectl api-resources --api-group=internal.linstor.linbit.com -oname | xargs --no-run-if-empty kubectl delete crds\n$ kubectl create -f .\n</code></pre>"},{"location":"how-to/restore-linstor-db/#restart-the-linstor-controller-deployment","title":"Restart the LINSTOR Controller Deployment","text":"<p>Now we can safely restart the Piraeus Operator and LINSTOR Controller Deployment.</p> <pre><code>$ kubectl scale deployment piraeus-operator-controller-manager --replicas 1\ndeployment.apps/piraeus-operator-controller-manager scaled\n$ kubectl scale deployment linstor-controller --replicas 1\ndeployment.apps/linstor-controller scaled\n$ kubectl rollout status deployment piraeus-operator-controller-manager\nWaiting for deployment \"piraeus-operator-controller-manager\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"piraeus-operator-controller-manager\" successfully rolled out\n$ kubectl rollout status deployment linstor-controller\nWaiting for deployment \"linstor-controller\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"linstor-controller\" successfully rolled out\n</code></pre>"},{"location":"how-to/secure-boot/","title":"How to Load DRBD with SecureBoot Enabled","text":"<p>This guide shows you how to use the DRBD\u00ae Module Loader on machines with SecureBoot enabled.</p> <p>When SecureBoot is enabled the Linux Kernel refuses to load unsigned kernel modules. Since the DRBD modules are built from source, they do not have a valid signature by default. To load them, they need to be signed, and the signing key needs to be inserted into the machine's trust store.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorSatelliteConfiguration</code> resources.</li> <li>generating key material with <code>openssl</code>.</li> <li>managing machine owner keys with <code>mokutil</code>.</li> <li>creating and protecting Kubernetes Secret resources.</li> </ul>"},{"location":"how-to/secure-boot/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes node running with SecureBoot enabled. This can be checked by running the following command on each node:   <pre><code># mokutil --sb-state\nSecureBoot enabled\n</code></pre></li> <li>Console access to the machine during early boot, or permissions to update the EFI platform keys another way.</li> </ul>"},{"location":"how-to/secure-boot/#generate-key-material","title":"Generate Key Material","text":"<p>Create a private key and self-signed certificate using the <code>openssl</code> command line utility. The following command will create a new private key named <code>signing_key.pem</code> and a certificate named <code>signing_key.x509</code>, valid for 10 years, in the appropriate format:</p> <pre><code>$ openssl req -x509 -new -nodes -utf8 -sha256 -days 36500 -batch -outform DER \\\n    -out signing_key.x509 -keyout signing_key.pem -config - &lt;&lt;EOF\n[ req ]\ndefault_bits = 4096\ndistinguished_name = req_distinguished_name\nprompt = no\nstring_mask = utf8only\nx509_extensions = myexts\n\n[ req_distinguished_name ]\nO = Piraeus Datastore\nCN = Piraeus Datastore kernel module signing key\nemailAddress = piraeus-signing-key@example.com\n\n[ myexts ]\nbasicConstraints=critical,CA:FALSE\nkeyUsage=digitalSignature\nsubjectKeyIdentifier=hash\nauthorityKeyIdentifier=keyid\nEOF\n</code></pre>"},{"location":"how-to/secure-boot/#enroll-the-certificate-in-the-machine-trust-store","title":"Enroll the Certificate in the Machine Trust Store","text":"<p>The generated certificate needs to be added to the machine's trust store. This step depends on the machine platform you are using. The instructions here apply to any system where you have console access during early boot, such as bare-metal or most virtualization platforms.</p> <p>First, distribute the generated certificate <code>signing_key.x509</code> to all nodes. Then, use the following command to add the certificate to the machine owner keys (MOK) using a password of your choice:</p> <pre><code># mokutil --import signing_key.x509\ninput password:\ninput password again:\n</code></pre> <p>To enable the keys, start a console session on the machine, either by directly attching a keyboard and monitor, attaching a virtual console using the machines BMC, or using tools of your virtualization platform such as virt-viewer or VNC.</p> <p>Then, reboot the machine, chosing to \"Perform MOK management\" when promted. Now: * Select \"Enroll MOK\":</p> <p> * Continue enrollment of the key:</p> <p> * Enter the password chosen when running <code>mokutil --import</code>:</p> <p> * Reboot the machine:</p> <p></p>"},{"location":"how-to/secure-boot/#sign-the-drbd-module-using-the-created-key-material","title":"Sign the DRBD Module Using the Created Key Material","text":"<p>Create a Kubernetes Secret resource containing the generated key material. The following command creates a secret named <code>drbd-signing-keys</code>:</p> <pre><code>$ kubectl create secret generic drbd-signing-keys --type piraeus.io/signing-key \\\n    --from-file=signing_key.pem --from-file=signing_key.x509\nsecret/drbd-signing-keys created\n</code></pre> <p>Now, configure the <code>drbd-module-loader</code> to use the key material to sign the kernel modules. The following <code>LinstorSatelliteConfiguration</code> resource makes the key available in the <code>drbd-module-loader</code> container available and sets the <code>LB_SIGN</code> environment variable to start signing the modules:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: drbd-module-signing\nspec:\n  podTemplate:\n    spec:\n      initContainers:\n      - name: drbd-module-loader\n        env:\n        - name: LB_SIGN\n          value: /signing-key\n        volumeMounts:\n        - name: signing-key\n          mountPath: /signing-key\n          readOnly: true\n      volumes:\n      - name: signing-key\n        secret:\n          secretName: drbd-signing-keys\n</code></pre>"},{"location":"how-to/secure-boot/#verifying-success","title":"Verifying Success","text":"<p>When everything is configured correctly, the <code>drbd-module-loader</code> init container will be able to insert the DRBD modules.</p> <pre><code>$ kubectl logs ds/linstor-satellite.node1.example.com drbd-module-loader\n...\nDRBD version loaded:\nversion: 9.2.8 (api:2/proto:86-122)\nGIT-hash: e163b05a76254c0f51f999970e861d72bb16409a build by @linstor-satellite.k8s-21.test-zskzl, 2024-04-03 09:04:15\nTransports (api:20): tcp (9.2.8) lb-tcp (9.2.8) rdma (9.2.8)\n</code></pre> <p>If the modules are not signed, or the signature is not trusted, the <code>drbd-module-loader</code> container will crash with one of the following error messages:</p> <pre><code>insmod: ERROR: could not insert module ./drbd.ko: Operation not permitted\ninsmod: ERROR: could not insert module ./drbd_transport_tcp.ko: Operation not permitted\ninsmod: ERROR: could not insert module ./drbd_transport_lb-tcp.ko: Operation not permitted\n\nCould not load DRBD kernel modules\n</code></pre> <pre><code>insmod: ERROR: could not insert module ./drbd.ko: Key was rejected by service\ninsmod: ERROR: could not insert module ./drbd_transport_tcp.ko: Key was rejected by service\ninsmod: ERROR: could not insert module ./drbd_transport_lb-tcp.ko: Key was rejected by service\n\nCould not load DRBD kernel modules\n</code></pre> <pre><code>insmod: ERROR: could not insert module ./drbd.ko: Required key not available\ninsmod: ERROR: could not insert module ./drbd_transport_tcp.ko: Required key not available\ninsmod: ERROR: could not insert module ./drbd_transport_lb-tcp.ko: Required key not available\n\nCould not load DRBD kernel modules\n</code></pre> <p>Additional information is available in the kernel logs:</p> <pre><code>$ dmesg\n...\n# The module is not signed. Check that the LB_SIGN environment variable is set in the container.\nLockdown: insmod: unsigned module loading is restricted; see man kernel_lockdown.7\n# The module is signed, but the signing key is not in the machine's trust store.\n# Check the enrolled keys using \"mokutil --list-enrolled\".\nPKCS#7 signature not signed with a trusted key\n</code></pre>"},{"location":"how-to/talos/","title":"How to Load DRBD on Talos Linux","text":"<p>This guide shows you how to set the DRBD\u00ae Module Loader when using Talos Linux.</p> <p>To complete this guide, you should be familiar with:</p> <ul> <li>editing <code>LinstorSatelliteConfiguration</code> resources.</li> <li>using the <code>talosctl</code> command line to to access Talos Linux nodes.</li> <li>using the <code>kubectl</code> command line tool to access the Kubernetes cluster.</li> </ul>"},{"location":"how-to/talos/#configure-talos-system-extension-for-drbd","title":"Configure Talos system extension for DRBD","text":"<p>By default, the DRBD Module Loader will try to find the necessary header files to build DRBD from source on the host system. In Talos Linux these header files are not included in the host system. Instead, the Kernel modules is packed into a system extension.</p> <p>Ensure Talos has the correct <code>drbd</code> system extension loaded for the running Kernel.</p> <p>This is done by building a install image from the talos factory with drbd included</p> <p>You will also need to update the machine config to set the following kernel module parameters:</p> <pre><code>machine:\n  kernel:\n    modules:\n      - name: drbd\n        parameters:\n          - usermode_helper=disabled\n      - name: drbd_transport_tcp\n</code></pre> <p>Validate <code>drbd</code> module is loaded: <pre><code>$ talosctl -n &lt;NODE_IP&gt; read /proc/modules\ndrbd_transport_tcp 28672 - - Live 0xffffffffc046c000 (O)\ndrbd 643072 - - Live 0xffffffffc03b9000 (O)\n</code></pre></p> <p>Validate <code>drbd</code> module parameter <code>usermode_helper</code> is set to <code>disabled</code>: <pre><code>$ talosctl -n &lt;NODE_IP&gt; read /sys/module/drbd/parameters/usermode_helper\ndisabled\n</code></pre></p>"},{"location":"how-to/talos/#configure-the-drbd-module-loader","title":"Configure the DRBD Module Loader","text":"<p>To change the DRBD Module Loader, so that it uses the modules provided by system extension, apply the following <code>LinstorSatelliteConfiguration</code>:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: talos-loader-override\nspec:\n  podTemplate:\n    spec:\n      initContainers:\n        - name: drbd-shutdown-guard\n          $patch: delete\n        - name: drbd-module-loader\n          $patch: delete\n      volumes:\n        - name: run-systemd-system\n          $patch: delete\n        - name: run-drbd-shutdown-guard\n          $patch: delete\n        - name: systemd-bus-socket\n          $patch: delete\n        - name: lib-modules\n          $patch: delete\n        - name: usr-src\n          $patch: delete\n        - name: etc-lvm-backup\n          hostPath:\n            path: /var/etc/lvm/backup\n            type: DirectoryOrCreate\n        - name: etc-lvm-archive\n          hostPath:\n            path: /var/etc/lvm/archive\n            type: DirectoryOrCreate\n</code></pre> <p>Explanation:</p> <ul> <li><code>/etc/lvm/*</code> is read-only in Talos and therefore can't be used. Let's use <code>/var/etc/lvm/*</code> instead.</li> <li>Talos does not ship with Systemd, so everything Systemd related needs to be removed</li> <li><code>/usr/lib/modules</code> and <code>/usr/src</code> are not needed as the Kernel module is already compiled and needs just to be used.</li> </ul>"},{"location":"how-to/upgrade/","title":"Upgrading Piraeus Operator from Version 1 to Version 2","text":"<p>The following document guides you through the upgrade process for Piraeus Operator from version 1 (\"v1\") to version 2 (\"v2\").</p> <p>Piraeus Operator v2 offers improved convenience and customization. This however made it necessary to make significant changes to the way Piraeus Operator manages Piraeus Datastore. As such, upgrading from v1 to v2 is a procedure requiring manual oversight.</p> <p>Upgrading Piraeus Operator is done in four steps:</p> <ul> <li>Step 1: (Optional) Migrate the LINSTOR database to use the <code>k8s</code> backend.</li> <li>Step 2: Collect information about the current deployment.</li> <li>Step 3: Remove the Piraeus Operator v1 deployment, keeping existing volumes untouched.</li> <li>Step 4: Deploy Piraeus Operator v2 using the information gathered in step 2.</li> </ul>"},{"location":"how-to/upgrade/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes:</p> <ul> <li>You used Helm to create the original deployment and are familiar with upgrading Helm deployments.</li> <li>Your Piraeus Datastore deployment is up-to-date with the latest v1 release. Check the releases here.</li> <li>You have the following command line tools available:</li> <li><code>kubectl</code></li> <li><code>helm</code></li> <li><code>jq</code></li> <li>You are familiar with the <code>linstor</code> command line utility, specifically to verify the cluster state.</li> </ul>"},{"location":"how-to/upgrade/#key-changes-between-operator-v1-and-v2","title":"Key Changes Between Operator V1 and V2","text":""},{"location":"how-to/upgrade/#administrative-changes","title":"Administrative Changes","text":"<ul> <li>The resources <code>LinstorController</code>, <code>LinstorSatelliteSet</code> and <code>LinstorCSIDriver</code> have been replaced by   <code>LinstorCluster</code> and   <code>LinstorSatelliteConfgiuration</code>.</li> <li>The default deployment runs the LINSTOR Satellite in the container network.   The migration script will propose changing to the host network.</li> </ul>"},{"location":"how-to/upgrade/#operational-changes","title":"Operational Changes","text":"<ul> <li>In Operator v1, all labels on the Kubernetes node resource were replicated on the satellite, making them usable in the   <code>replicasOnSame</code> and <code>replicasOnDifferent</code> parameters on the storage class. In Operator v2, only the following   labels are automatically synchronized.</li> <li><code>kubernetes.io/hostname</code></li> <li><code>topology.kubernetes.io/region</code></li> <li><code>topology.kubernetes.io/zone</code>   Use <code>LinstorSatelliteConfiguration.spec.properties</code>   to synchronize additional labels.</li> <li>The following settings are applied by Operator v2 cluster-wide:</li> <li>DrbdOptions/Net/rr-conflict: retry-connect</li> <li>DrbdOptions/Resource/on-suspended-primary-outdated: force-secondary</li> <li>DrbdOptions/Resource/on-no-data-accessible: suspend-io</li> <li>DrbdOptions/auto-quorum: suspend-io</li> <li>Operator v2 also includes a High-Availability Controller   deployment to prevent stuck nodes caused by suspended DRBD devices.</li> </ul>"},{"location":"how-to/upgrade/1-migrate-database/","title":"Migrating the LINSTOR Controller Database to Use the <code>k8s</code> Backend","text":"<p>This document guides you through the process of migrating an existing LINSTOR\u00ae Cluster to use the <code>k8s</code> database backend.</p> <p>This is the optional first step when migrating Piraeus Operator from version 1 (v1) to version 2 (v2). Click here to get back to the overview.</p>"},{"location":"how-to/upgrade/1-migrate-database/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install <code>kubectl</code></li> <li>Install <code>helm</code></li> </ul>"},{"location":"how-to/upgrade/1-migrate-database/#when-to-migrate-the-linstor-controller-database-to-the-k8s-backend","title":"When to Migrate the LINSTOR Controller Database to the <code>k8s</code> Backend","text":"<p>LINSTOR can use different database backends to store important state, such as the available volumes and where they are deployed. The available backends are:</p> <ul> <li>An H2 database stored in a local file on the controller.</li> <li>A compatible database server, such as MySQL or PostgreSQL.</li> <li>An etcd cluster.</li> <li>An <code>k8s</code> backend, using Custom Resources to store the cluster state directly in the Kubernetes API.</li> </ul> <p>Of these backends, only the <code>k8s</code> backend works without any additional components when running in the Kubernetes cluster.</p> <p>Because the Piraeus Operator v1 is older than the LINSTOR <code>k8s</code> backend, the default for a long time was the use of a separate etcd cluster, deployed by default when using Piraeus Operator v1. Since then, etcd proved to be difficult to maintain, often requiring manual intervention.</p> <p>Because of this, consider migrating away from separate etcd cluster to using the <code>k8s</code> backend.</p> <p>If you already use one of the other backends, no migration is necessary. You can check the currently used backend of your LINSTOR cluster by running the following command:</p> <pre><code>$ kubectl exec deploy/piraeus-op-cs-controller -- cat /etc/linstor/linstor.toml\n[db]\n  connection_url = \"etcd://piraeus-op-etcd:2379\"\n</code></pre> <p>If the <code>connection_url</code> value starts with <code>etcd://</code> you are using the etcd backend and should consider migrating.</p>"},{"location":"how-to/upgrade/1-migrate-database/#migrating-the-linstor-controller-database","title":"Migrating the LINSTOR Controller Database","text":"<p>During the migration, the LINSTOR Controller needs to be stopped. This means that provisioning and deletion of volumes will not work for the duration of the migration. Existing volumes will continue working normally.</p>"},{"location":"how-to/upgrade/1-migrate-database/#1-stop-the-linstor-controller","title":"1. Stop the LINSTOR Controller","text":"<p>To prevent unwanted modifications to the database during the migration, you need to stop the LINSTOR Controller. To stop the LINSTOR Controller, set the expected replicas for the controller to zero.</p> <p>First, find the currently deployed Piraeus Datastore release. For this guide, it is 1.10.8:</p> <pre><code>$ helm list\nNAME        NAMESPACE          REVISION  UPDATED                                 STATUS    CHART           APP VERSION\npiraeus-op  piraeus-datastore  1         2023-11-07 10:30:44.184389038 +0100 CET deployed  piraeus-1.10.8  1.10.8\n</code></pre> <p>Then change the deployment to deploy no replicas for the LINSTOR Controller. Ensure to check out the chart version currently deployed, so that no accidental upgrade happens:</p> <pre><code>$ git checkout v1.10.8\n$ helm upgrade piraeus-op ./charts/piraeus --reuse-values --set operator.controller.replicas=0\n$ kubectl rollout status deploy/piraeus-op-cs-controller -w\ndeployment \"piraeus-op-cs-controller\" successfully rolled out\n</code></pre>"},{"location":"how-to/upgrade/1-migrate-database/#2-prepare-a-pod-for-running-the-migration","title":"2. Prepare a Pod for Running the Migration","text":"<p>To run the migration, you need access to the <code>linstor-database</code> tool, which comes preinstalled with the LINSTOR Controller image. The following command displays the LINSTOR Controller image, the name of the ConfigMap storing the LINSTOR Configuration, and the service account for LINSTOR Controller. Take note of them, as they will be used for the migration Pod.</p> <pre><code>$ kubectl get deploy/piraeus-op-cs-controller --output=jsonpath='IMAGE={$.spec.template.spec.containers[?(@.name==\"linstor-controller\")].image}{\"\\n\"}CONFIG_MAP={$.spec.template.spec.volumes[?(@.name==\"linstor-conf\")].configMap.name}{\"\\n\"}SERVICE_ACCOUNT={$.spec.template.spec.serviceAccountName}{\"\\n\"}'\nIMAGE=quay.io/piraeusdatastore/piraeus-server:v1.24.2\nCONFIG_MAP=piraeus-op-cs-controller-config\nSERVICE_ACCOUNT=linstor-controller\n</code></pre> <p>Using the information from the LINSTOR Controller deployment, create a Pod used during the migration process. Replace <code>$IMAGE</code>, <code>$CONFIG_MAP</code> and <code>$SERVICE_ACCOUNT</code> with the appropriate values.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: linstor-database-migration\nspec:\n  serviceAccountName: $SERVICE_ACCOUNT\n  containers:\n    - name: backup\n      image: $IMAGE\n      command:\n        - /bin/bash\n        - -c\n        - \"sleep infinity\"\n      volumeMounts:\n        - name: linstor-conf\n          mountPath: /etc/linstor\n          readOnly: true\n        - name: backup\n          mountPath: /backup\n        - name: logs\n          mountPath: /logs\n  volumes:\n    - name: backup\n      emptyDir: {}\n    - name: logs\n      emptyDir: {}\n    - name: linstor-conf\n      configMap:\n        name: $CONFIG_MAP\n</code></pre>"},{"location":"how-to/upgrade/1-migrate-database/#3-create-a-database-backup","title":"3. Create a Database Backup","text":"<p>Using the newly created migration Pod, create a database backup using the <code>linstor-database</code> tool.</p> <pre><code>$ kubectl exec linstor-database-migration -- /usr/share/linstor-server/bin/linstor-database export-db -c /etc/linstor /backup/backup-before-migration.json\nLoading configuration file \"/etc/linstor/linstor.toml\"\n15:22:55.684 [main] INFO LINSTOR/linstor-db -- SYSTEM - ErrorReporter DB first time init.\n15:22:55.685 [main] INFO LINSTOR/linstor-db -- SYSTEM - Log directory set to: './logs'\n15:22:56.417 [main] INFO LINSTOR/linstor-db -- SYSTEM - Attempting dynamic load of extension module \"com.linbit.linstor.modularcrypto.FipsCryptoModule\"\n15:22:56.418 [main] INFO LINSTOR/linstor-db -- SYSTEM - Extension module \"com.linbit.linstor.modularcrypto.FipsCryptoModule\" is not installed\n15:22:56.418 [main] INFO LINSTOR/linstor-db -- SYSTEM - Attempting dynamic load of extension module \"com.linbit.linstor.modularcrypto.JclCryptoModule\"\n15:22:56.426 [main] INFO LINSTOR/linstor-db -- SYSTEM - Dynamic load of extension module \"com.linbit.linstor.modularcrypto.JclCryptoModule\" was successful\n15:22:56.427 [main] INFO LINSTOR/linstor-db -- SYSTEM - Attempting dynamic load of extension module \"com.linbit.linstor.spacetracking.ControllerSpaceTrackingModule\"\n15:22:56.429 [main] INFO LINSTOR/linstor-db -- SYSTEM - Dynamic load of extension module \"com.linbit.linstor.spacetracking.ControllerSpaceTrackingModule\" was successful\n15:22:57.666 [main] INFO LINSTOR/linstor-db -- SYSTEM - Initializing the etcd database\n15:22:57.667 [main] INFO LINSTOR/linstor-db -- SYSTEM - etcd connection URL is \"etcd://piraeus-op-etcd:2379\"\n15:22:59.480 [main] INFO LINSTOR/linstor-db -- SYSTEM - Export finished\n</code></pre> <p>Then, copy the just created backup to permanent storage, so that it is not lost should the migration pod restart:</p> <pre><code>$ kubectl cp linstor-database-migration:/backup/backup-before-migration.json backup-before-migration.json\n</code></pre>"},{"location":"how-to/upgrade/1-migrate-database/#4-update-the-linstor-configuration-to-point-to-the-new-database-backend","title":"4. Update the LINSTOR Configuration to Point to the New Database Backend","text":"<p>After creating and storing the backup it is safe to change the LINSTOR configuration. This is done by running another <code>helm upgrade</code>, changing the database configuration for the LINSTOR Controller. Once again, ensure that you apply the chart version currently deployed, so that no accidental upgrade happens:</p> <pre><code>$ helm upgrade piraeus-op ./charts/piraeus --reuse-values --set operator.controller.dbConnectionURL=k8s\n</code></pre> <p>This will also change the configuration in the <code>linstor-database-migration</code> Pod, but it might need some time to propagate. Eventually, you will see the <code>k8s</code> database connection:</p> <pre><code>$ kubectl exec linstor-database-migration -- cat /etc/linstor/linstor.toml\n[db]\n  connection_url = \"k8s\"\n</code></pre>"},{"location":"how-to/upgrade/1-migrate-database/#5-import-the-database-from-the-backup","title":"5. Import the Database from the Backup","text":"<p>After changing the configuration, you can again use the <code>linstor-database</code> tool to restore the database backup to the new backend.</p> <pre><code>$ kubectl exec linstor-database-migration -- /usr/share/linstor-server/bin/linstor-database import-db -c /etc/linstor /backup/backup-before-migration.json\nLoading configuration file \"/etc/linstor/linstor.toml\"\n15:39:41.270 [main] INFO LINSTOR/linstor-db -- SYSTEM - ErrorReporter DB version 1 found.\n15:39:41.272 [main] INFO LINSTOR/linstor-db -- SYSTEM - Log directory set to: './logs'\n15:39:41.848 [main] INFO LINSTOR/linstor-db -- SYSTEM - Attempting dynamic load of extension module \"com.linbit.linstor.modularcrypto.FipsCryptoModule\"\n15:39:41.849 [main] INFO LINSTOR/linstor-db -- SYSTEM - Extension module \"com.linbit.linstor.modularcrypto.FipsCryptoModule\" is not installed\n15:39:41.850 [main] INFO LINSTOR/linstor-db -- SYSTEM - Attempting dynamic load of extension module \"com.linbit.linstor.modularcrypto.JclCryptoModule\"\n15:39:41.858 [main] INFO LINSTOR/linstor-db -- SYSTEM - Dynamic load of extension module \"com.linbit.linstor.modularcrypto.JclCryptoModule\" was successful\n15:39:41.859 [main] INFO LINSTOR/linstor-db -- SYSTEM - Attempting dynamic load of extension module \"com.linbit.linstor.spacetracking.ControllerSpaceTrackingModule\"\n15:39:41.860 [main] INFO LINSTOR/linstor-db -- SYSTEM - Dynamic load of extension module \"com.linbit.linstor.spacetracking.ControllerSpaceTrackingModule\" was successful\n15:39:43.307 [main] INFO LINSTOR/linstor-db -- SYSTEM - Initializing the k8s crd database connector\n15:39:43.307 [main] INFO LINSTOR/linstor-db -- SYSTEM - Kubernetes-CRD connection URL is \"k8s\"\n15:40:22.418 [main] INFO LINSTOR/linstor-db -- SYSTEM - Import finished\n</code></pre>"},{"location":"how-to/upgrade/1-migrate-database/#6-start-the-linstor-controller","title":"6. Start the LINSTOR Controller","text":"<p>Now that the new database backend is initialized, it is safe to start the LINSTOR Controller again. Simply change the number of replicas for the LINSTOR Controller back to the original value before starting the migration. Once again, ensure that you apply the chart version currently deployed, so that no accidental upgrade happens:</p> <pre><code>$ helm upgrade piraeus-op ./charts/piraeus --reuse-values --set operator.controller.replicas=1\ndeployment \"piraeus-op-cs-controller\" successfully rolled out\n</code></pre> <p>Once the LINSTOR Controller is up and running, verify the state of your cluster: check all nodes and resource are in the expected state:</p> <pre><code>$ kubectl exec deploy/piraeus-op-cs-controller -- linstor node list\n$ kubectl exec deploy/piraeus-op-cs-controller -- linstor storage-pool list\n$ kubectl exec deploy/piraeus-op-cs-controller -- linstor resource list-volumes\n$ kubectl exec deploy/piraeus-op-cs-controller -- linstor error-reports list\n</code></pre>"},{"location":"how-to/upgrade/1-migrate-database/#7-clean-up-migration-resources","title":"7. Clean up Migration Resources","text":"<p>After successful migration, you can clean up left over resources.</p> <p>First, you can remove the <code>linstor-database-migration</code> Pod:</p> <pre><code>$ kubectl delete pod linstor-database-migration\n</code></pre> <p>Then, you can also remove the etcd deployment by upgrading the helm deployment, setting <code>etcd.enabled=false</code>:</p> <pre><code>$ helm upgrade piraeus-op ./charts/piraeus --reuse-values --set etcd.enabled=false\n</code></pre>"},{"location":"how-to/upgrade/2-collect-information/","title":"Collect Information before Performing the Upgrade","text":"<p>Before performing the upgrade and removing the old deployment, collect information on the state of your cluster. This ensures the upgraded deployment will have a compatible configuration.</p> <p>This is the second step when migrating Piraeus Operator from version 1 (v1) to version 2 (v2). Click here to get back to the overview.</p>"},{"location":"how-to/upgrade/2-collect-information/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install <code>kubectl</code></li> <li>Install <code>helm</code></li> <li>Install <code>jq</code></li> </ul>"},{"location":"how-to/upgrade/2-collect-information/#run-the-data-collection-script","title":"Run the Data Collection Script","text":"<p>To collect the necessary information for the migration, run the script provided at <code>docs/how-to/upgrade/collect-operator-v1-information.sh</code>. You can run the script either from a checked out repository, or directly from GitHub:</p> <pre><code>$ curl --proto '=https' --tlsv1.2 -sSf \\\n  https://raw.githubusercontent.com/piraeusdatastore/piraeus-operator/v2/docs/how-to/upgrade/collect-operator-v1-information.sh \\\n  | bash -s\n</code></pre> <p>The script will ask you if you want to keep modifications made with the deployed Piraeus Operator. For each modification, the script will show you the proposed change to the deployed resources and ask for confirmation.</p> <p>In addition, it will also inform you about values that cannot automatically be migrated, along with recommended actions should you need to keep the modification. The output looks like this:</p> <pre><code>Using kubectl context: kubernetes-admin@kubernetes\nFound LinstorControllers: [\"piraeus-cs\"]\nFound LinstorSatelliteSets: [\"piraeus-ns\"]\nFound LinstorCSIDrivers: [\"piraeus\"]\nFound additional environment variables passed to LINSTOR Controller\n--- Default resource\n+++ Updated resource\n@@ -143,6 +143,12 @@ kind: Deployment\n       - args:\n         - startController\n         env:\n+        - name: FOO\n+          value: bar\n+        - name: EXAMPLE\n+          valueFrom:\n+            fieldRef:\n+              fieldPath: status.podIP\n         - name: JAVA_OPTS\n           value: -Djdk.tls.acknowledgeCloseNotify=true\n         - name: K8S_AWAIT_ELECTION_ENABLED\nApply the patch (Y/n)? y\n...\n---------------------------------------------------------------------------------------------\n| After upgrading, apply the following resources. They have been saved to v2-resources.yaml |\n---------------------------------------------------------------------------------------------\n\napiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  linstorPassphraseSecret: piraeus-passphrase\n  patches: []\n...\n</code></pre> <p>A copy of the identified resources is saved at <code>v2-resources.yaml</code>.</p>"},{"location":"how-to/upgrade/2-collect-information/#collect-helm-generated-secrets","title":"Collect Helm-Generated Secrets","text":"<p>Helm automatically generates a passphrase for LINSTOR. Once LINSTOR is initialized with the passphrase, the LINSTOR Controller will not be able fully start without supplying the passphrase.</p> <p>To migrate the generated passphrase to Piraeus Operator v2, create a backup of the secret:</p> <pre><code>$ kubectl get secrets piraeus-op-passphrase -ogo-template='{{.data.MASTER_PASSPHRASE}}{{\"\\n\"}}'\nZVRxanl1cVBscXV1ZTk1cmEwVTk5b0ptd0F4OGFmWlVPUVA0NWNnUw==\n</code></pre> <p>Save the generated passphrase for later use.</p>"},{"location":"how-to/upgrade/3-remove-operator-v1/","title":"Remove the Piraeus Operator v1 Deployment","text":"<p>Migrating to the new Piraeus Operator deployment requires temporarily removing the existing deployment.</p> <p>After this step no new volumes can be created and no existing volume can be attached or detached, until the new deployment is rolled out.</p> <p>This is the third step when migrating Piraeus Operator from version 1 (v1) to version 2 (v2). Click here to get back to the overview.</p>"},{"location":"how-to/upgrade/3-remove-operator-v1/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install <code>kubectl</code></li> <li>Install <code>helm</code></li> </ul>"},{"location":"how-to/upgrade/3-remove-operator-v1/#scale-down-the-operator-deployment","title":"Scale Down the Operator Deployment","text":"<p>To prevent modification of the existing cluster, scale down the existing Piraeus Operator deployment.</p> <pre><code>$ helm upgrade piraeus-op ./charts/piraeus --set operator.replicas=0\n$ kubectl rollout status -w deploy/piraeus-op-operator\ndeployment \"piraeus-op-operator\" successfully rolled out\n</code></pre>"},{"location":"how-to/upgrade/3-remove-operator-v1/#remove-the-finalizers-from-the-piraeus-resources","title":"Remove the Finalizers from the Piraeus Resources","text":"<p>The Operator sets Finalizers on the resource it controls. This prevents the deletion of these resources when the Operator is not running. Remove the Finalizers by applying a patch:</p> <pre><code>$ kubectl patch linstorsatellitesets piraeus-op-ns --type merge --patch '{\"metadata\": {\"finalizers\": []}}'\nlinstorsatelliteset.piraeus.linbit.com/piraeus-op-ns patched\n$ kubectl patch linstorcontrollers piraeus-op-cs --type merge --patch '{\"metadata\": {\"finalizers\": []}}'\nlinstorcontroller.piraeus.linbit.com/piraeus-op-cs patched\n</code></pre>"},{"location":"how-to/upgrade/3-remove-operator-v1/#remove-the-piraeus-resources","title":"Remove the Piraeus Resources","text":"<p>Having removed the Finalizers, you can delete the Piraeus Resources. This will stop the LINSTOR Cluster, and no new volumes can be created, and existing volumes will not attach or detach. Volumes already attached to a Pod will continue to replicate.</p> <pre><code>$ kubectl delete linstorcsidrivers/piraeus-op\n$ kubectl delete linstorsatellitesets/piraeus-op-ns\n$ kubectl delete linstorcontrollers/piraeus-op-cs\n</code></pre>"},{"location":"how-to/upgrade/3-remove-operator-v1/#remove-the-piraeus-deployment","title":"Remove the Piraeus Deployment","text":"<p>As a last step, you can completely remove the helm deployment, deleting additional resources such as service accounts and RBAC resources. In addition, also clean up the Custom Resource Definitions:</p> <pre><code>$ helm uninstall piraeus-op\n$ kubectl delete crds linstorcsidrivers.piraeus.linbit.com linstorsatellitesets.piraeus.linbit.com linstorcontrollers.piraeus.linbit.com\n</code></pre>"},{"location":"how-to/upgrade/3-remove-operator-v1/#optional-remove-additional-piraeus-components","title":"Optional: Remove additional Piraeus Components","text":"<p>If you have deployed additional components from Piraeus, such as the HA Controller or LINSTOR Affinity Controller, you will need to remove them, too. After completing the migration, you can install them again, if needed.</p> <pre><code>$ helm uninstall piraeus-ha-controller\n$ helm uninstall linstor-affinity-controller\n</code></pre>"},{"location":"how-to/upgrade/4-install-operator-v2/","title":"Install Piraeus Operator v2","text":"<p>After removing the Piraeus Operator v1 deployment, you can install Piraeus Operator v2.</p> <p>You can either use the <code>kubectl</code> method to deploy Piraeus Operator, or alternatively deploy the Operator using Helm.</p> <p>This is the last step when migrating Piraeus Operator from version 1 (v1) to version 2 (v2). Click here to get back to the overview.</p>"},{"location":"how-to/upgrade/4-install-operator-v2/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install <code>kubectl</code></li> <li>Install <code>helm</code></li> </ul>"},{"location":"how-to/upgrade/4-install-operator-v2/#deploy-piraeus-operator-v2-using-kubectl","title":"Deploy Piraeus Operator v2 Using <code>kubectl</code>","text":"<p>You can use <code>kubectl</code> and the built-in <code>kustomize</code> feature to deploy Piraeus Operator v2.</p> <p>First, create a file named <code>kustomization.yaml</code>, referencing the latest Piraeus Operator release, and the namespace you which to deploy the Piraeus Operator in. The latest releases are available  on the release page.</p> <p>In the example below, the version is <code>v2.3.0</code>, and the namespace is <code>piraeus-datastore</code>.</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - https://github.com/piraeusdatastore/piraeus-operator//config/default?ref=v2.3.0 # Change the version here\nnamespace: piraeus-datastore # Change the namespace here\n</code></pre> <p>Then, deploy the Operator using <code>kubectl</code> and wait for the deployment to complete:</p> <pre><code>$ kubectl apply -k . --server-side\n$ kubectl rollout status -n piraeus-datastore -w deploy/piraeus-operator-controller-manager\n</code></pre>"},{"location":"how-to/upgrade/4-install-operator-v2/#deploy-piraeus-operator-v2-using-helm","title":"Deploy Piraeus Operator v2 Using Helm","text":"<p>To deploy Piraeus Operator v2 using Helm, clone the latest v2 release of Piraeus. The latest releases are available on the release page.</p> <p>To also install the necessary Custom Resource Definitions for Piraeus, set <code>installCRDs</code> to <code>true</code>.</p> <pre><code>$ git clone -b v2.3.0 https://github.com/piraeusdatastore/piraeus-operator.git\n$ cd piraeus-operator\n$ helm install piraeus-operator charts/piraeus --set installCRDs=true\n</code></pre>"},{"location":"how-to/upgrade/4-install-operator-v2/#deploy-the-piraeus-datastore-cluster","title":"Deploy the Piraeus Datastore Cluster","text":"<p>Once the Operator is deployed, deploy the Cluster using the generated resources from step 2.</p> <p>First, create the secret used for the LINSTOR passphrase collected at the end of step 2.</p> <pre><code>$ cat &lt;&lt;-EOF | kubectl apply -f - --server-side\napiVersion: v1\nkind: Secret\nmetadata:\n  name: piraeus-op-passphrase\n  namespace: piraeus-datastore # Change the namespace here\ndata:\n  MASTER_PASSPHRASE: ZVRxanl1cVBscXV1ZTk1cmEwVTk5b0ptd0F4OGFmWlVPUVA0NWNnUw==\nEOF\n</code></pre> <p>Then, apply the resource generated by the script from step 2:</p> <pre><code>$ kubectl apply -f v2-resources.yaml --server-side\nlinstorcluster.piraeus.io/linstorcluster created\nlinstorsatelliteconfiguration.piraeus.io/host-networking created\nlinstorsatelliteconfiguration.piraeus.io/piraeus-op-ns created\n</code></pre> <p>Now the cluster will come up, using the existing data to restore the cluster state.</p>"},{"location":"how-to/upgrade/4-install-operator-v2/#verifying-cluster-state","title":"Verifying Cluster State","text":"<p>You can check the Cluster state by using the <code>linstor</code> command line client:</p> <pre><code>$ kubectl exec deploy/linstor-controller -- linstor node list\n$ kubectl exec deploy/linstor-controller -- linstor storage-pool list\n$ kubectl exec deploy/linstor-controller -- linstor resource list-volumes\n$ kubectl exec deploy/linstor-controller -- linstor error-reports list\n</code></pre> <p>You can also provision new volumes, to verify that the cluster is operational.</p>"},{"location":"how-to/upgrade/UPGRADE/","title":"General notes","text":"<p>During the upgrade process, provisioning of volumes and attach/detach operations might not work. Existing volumes and volumes already in use by a pod will continue to work without interruption.</p> <p>To upgrade, apply the resource of the latest release. Use the same method that was used to create the initial deployment (<code>kubectl</code> vs <code>helm</code>). There is no need to change existing <code>LinstorCluster</code>, <code>LinstorSatelliteConfiguration</code> or <code>LinstorNodeConnection</code> resources.</p> <p>To upgrade to the latest release using <code>kubectl</code>, run the following commands:</p> <pre><code>$ kubectl apply --server-side -k \"https://github.com/piraeusdatastore/piraeus-operator//config/default?ref=v2.7.1\"\n$ kubectl wait pod --for=condition=Ready -n piraeus-datastore --all\n</code></pre>"},{"location":"how-to/upgrade/UPGRADE/#upgrades-from-v26-to-v27","title":"Upgrades from v2.6 to v2.7","text":"<p>Generally, no special steps required.</p> <p>LINSTOR Satellites now try to automatically detect the LVM configuration on the host. Any patch targeting the Satellites /etc/lvm/lvm.conf file may need to be adapted.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrades-from-v25-to-v26","title":"Upgrades from v2.5 to v2.6","text":"<p>No special steps required.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrades-from-v24-to-v25","title":"Upgrades from v2.4 to v2.5","text":"<p>No special steps required.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrades-from-v23-to-v24","title":"Upgrades from v2.3 to v2.4","text":"<p>Generally, no special steps required.</p> <p>LINSTOR Satellites are now managed via DaemonSet resources. Any patch targeting a <code>satellite</code> Pod resources is automatically converted to the equivalent DaemonSet resource patch. In the Pod list, you will see these Pods using a new <code>linstor-satellite</code> prefix.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrades-from-v22-to-v23","title":"Upgrades from v2.2 to v2.3","text":"<p>Removed the <code>NetworkPolicy</code> resource from default deployment. It can be reapplied as a separate step.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrades-from-v21-to-v22","title":"Upgrades from v2.1 to v2.2","text":"<p>Removed the dependency on cert-manager for the initial deployment. To clean up an existing <code>Certificate</code> resource, run the following commands:</p> <pre><code>$ kubectl delete certificate -n piraeus-datastore piraeus-operator-serving-cert\n</code></pre>"},{"location":"how-to/upgrade/UPGRADE/#upgrades-from-v20-to-v21","title":"Upgrades from v2.0 to v2.1","text":"<p>No special steps required.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrades-from-v1-to-v2","title":"Upgrades from v1 to v2","text":"<p>Please follow the specialized upgrade guides.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-from-v19-to-v110","title":"Upgrade from v1.9 to v1.10","text":"<p>If you want to share DRBD configuration directories with the host, please update the CRDs before upgrading using helm:</p> <pre><code>$ kubectl replace -f ./charts/piraeus/crds\n</code></pre>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-from-v18-to-v19","title":"Upgrade from v1.8 to v1.9","text":"<p>If you want to protect metrics endpoints, take a look on guide on enabling rbac-proxy sidecars.</p> <p>We've also disabled the <code>haController</code> component in our chart. The replacement is available from artifacthub.io, containing much needed improvements in robustness and fail-over speed. If you want to still use the old version, set <code>haController.enabled=true</code>.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-from-v17-to-v18","title":"Upgrade from v1.7 to v1.8","text":"<p>If you need to set the number of worker threads, or you need to set the log level of LINSTOR components, please update the CRDs before upgrading using helm:</p> <pre><code>$ kubectl replace -f ./charts/piraeus/crds\n</code></pre> <p>In case if you have SSL enabled installation, you need to regenerate your certificates in PEM format. Read the guide on securing the deployment and repeat the described steps.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-from-v16-to-v17","title":"Upgrade from v1.6 to v1.7","text":"<p>Node labels are now automatically applied to LINSTOR satellites as \"Auxiliary Properties\". That means you can reuse your existing Kubernetes Topology information (for example <code>topology.kubernetes.io/zone</code> labels) when scheduling volumes using the <code>replicasOnSame</code> and <code>replicasOnDifferent</code> settings. However, this also means that the Operator will delete any existing auxiliary properties that were already applied. To apply auxiliary properties to satellites, you have to apply a label to the Kubernetes node object.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-from-v15-to-v16","title":"Upgrade from v1.5 to v1.6","text":"<p>The <code>csi-snapshotter</code> subchart was removed from this repository. Users who relied on it for their snapshot support should switch to the seperate charts provided by the Piraeus team on artifacthub.io The new charts also include notes on how to upgrade to newer CRD versions.</p> <p>To support the new <code>csi.kubeletPath</code> option an update to the LinstorCSIDriver CRD is required:</p> <pre><code>$ kubectl replace -f ./charts/piraeus/crds\n</code></pre>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-from-v14-to-v15","title":"Upgrade from v1.4 to v1.5","text":"<p>To make use of the new <code>monitoringImage</code> value in the LinstorSatelliteSet CRD, you need to replace the existing CRDs before running the upgrade. If the CRDs are not upgraded, the operator will not deploy the monitoring container alongside the satellites.</p> <pre><code>$ kubectl replace -f ./charts/piraeus/crds\n</code></pre> <p>Then run the upgrade:</p> <pre><code>helm upgrade piraeus-op ./charts/piraeus -f &lt;overrides&gt;\n</code></pre>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-from-v13-to-v14","title":"Upgrade from v1.3 to v1.4","text":"<p>No special steps required, unless using the newly introduced <code>additionalEnv</code> and <code>additionalProperties</code> overrides.</p> <p>If you plan to use the new overrides, replace the CustomResourceDefinitions before running the upgrade</p> <pre><code>$ kubectl replace -f ./charts/piraeus/crds\n</code></pre> <p>Then run the upgrade:</p> <pre><code>helm upgrade piraeus-op ./charts/piraeus -f &lt;overrides&gt;\n</code></pre>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-from-v12-to-v13","title":"Upgrade from v1.2 to v1.3","text":"<p>No special steps required. After checking out the new repo, run a simple helm upgrade:</p> <pre><code>helm upgrade piraeus-op ./charts/piraeus -f &lt;overrides&gt;\n</code></pre>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-from-v11-to-v12","title":"Upgrade from v1.1 to v1.2","text":"<p>Piraeus v1.2 is supported on Kubernetes 1.17+. If you are using an older Kubernetes distribution, you may need to change the default settings (for example the CSI provisioner)</p> <p>To start the upgrade process, ensure you have a backup of the LINSTOR Controller database. If you are using the etcd deployment included in Piraeus, you can create a backup using:</p> <pre><code>kubectl exec piraeus-op-etcd-0 -- etcdctl snapshot save /tmp/save.db\nkubectl cp piraeus-op-etcd-0:/tmp/save.db save.db\n</code></pre> <p>Now you can start the upgrade process. Simply run <code>helm upgrade piraeus-op ./charts/piraeus</code>. If you installed Piraeus with customization, pass the same options you used for <code>helm install</code> to <code>helm upgrade</code>. This will cause the operator pod to be re-created and shortly after all other Piraeus pods.</p> <p>There is a known issue when updating the CSI components: the pods will not be updated to the newest image and the <code>errors</code> section of the LinstorCSIDrivers resource shows an error updating the DaemonSet. In this case, manually delete <code>deployment/piraeus-op-csi-controller</code> and <code>daemonset/piraeus-op-csi-node</code>. They will be re-created by the operator.</p> <p>After a short wait, all pods should be running and ready. Check that no errors are listed in the status section of LinstorControllers, LinstorSatelliteSets and LinstorCSIDrivers.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-from-v10-to-v11","title":"Upgrade from v1.0 to v1.1","text":"<ul> <li>The LINSTOR controller image given in <code>operator.controller.controllerImage</code> has to have   its entrypoint set to <code>k8s-await-election v0.2.0</code>   or newer. All images starting with <code>piraeus-server:v1.8.0</code> meet this requirement.</li> </ul> <p>Older images will not work, as the <code>Service</code> will not automatically pick up on the active pod.</p> <p>To upgrade, first update the deployed LINSTOR image to a compatible version, then upgrade the   operator.</p>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-to-v10","title":"Upgrade to v1.0","text":"<p>Upgrades from v0.* versions to v1.0 are best-effort only. The following guide assumes an upgrade from v0.5.0.</p> <ul> <li> <p>CRDs have been updated and set to version <code>v1</code>. You need to replace any existing CRDs with these new ones:   <pre><code>kubectl replace -f charts/piraeus/crds/piraeus.linbit.com_linstorcontrollers_crd.yaml\nkubectl replace -f charts/piraeus/crds/piraeus.linbit.com_linstorcsidrivers_crd.yaml\nkubectl replace -f charts/piraeus/crds/piraeus.linbit.com_linstorsatellitesets_crd.yaml\n</code></pre></p> </li> <li> <p>Renamed <code>LinstorNodeSet</code> to <code>LinstorSatelliteSet</code>. This brings the operator in line with other LINSTOR resources.   Existing <code>LinstorNodeSet</code> resources will automatically be migrated to <code>LinstorSatelliteSet</code>. The old resources will   not be deleted. You can verify that migration was successful by running the following command:   <pre><code>$ kubectl get linstornodesets.piraeus.linbit.com -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.ResourceMigrated}{\"\\t\"}{.status.DependantsMigrated}{\"\\n\"}{end}'\npiraeus-op-ns true    true\n</code></pre>   If both values are <code>true</code>, migration was successful. The old resource can be removed after migration.</p> </li> <li> <p>Renamed <code>LinstorControllerSet</code> to <code>LinstorController</code>. The old name implied the existence of multiple (separate)   controllers. Existing <code>LinstorControllerSet</code> resources will automatically be migrated to <code>LinstorController</code>. The old   resources will not be deleted. You can verify that migration was successful by running the following command:   <pre><code>$ kubectl get linstorcontrollersets.piraeus.linbit.com -o=jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.ResourceMigrated}{\"\\t\"}{.status.DependantsMigrated}{\"\\n\"}{end}'\npiraeus-op-cs true    true\n</code></pre>   If both values are <code>true</code>, migration was successful. The old resource can be removed after migration.</p> </li> <li> <p>Along with the CRDs, Helm settings changed too:</p> <ul> <li><code>operator.controllerSet</code> to <code>operator.controller</code></li> <li><code>operator.nodeSet</code> to <code>operator.satelliteSet</code>   If old settings are used, Helm will return an error.</li> </ul> </li> <li> <p>Node scheduling no longer relies on <code>linstor.linbit.com/piraeus-node</code> labels. Instead, all CRDs support   setting pod affinity and tolerations.   In detail:</p> <ul> <li><code>linstorcsidrivers</code> gained 4 new resource keys, with no change in default behaviour:<ul> <li><code>nodeAffinity</code> affinity passed to the csi nodes</li> <li><code>nodeTolerations</code> tolerations passed to the csi nodes</li> <li><code>controllerAffinity</code> affinity passed to the csi controller</li> <li><code>controllerTolerations</code> tolerations passed to the csi controller</li> </ul> </li> <li><code>linstorcontrollerset</code> gained 2 new resource keys, with no change in default behaviour:<ul> <li><code>affinity</code> affinity passed to the linstor controller pod</li> <li><code>tolerations</code> tolerations passed to the linstor controller pod</li> </ul> </li> <li><code>linstornodeset</code> gained 2 new resource keys, with change in default behaviour:<ul> <li><code>affinity</code> affinity passed to the linstor controller pod</li> <li><code>tolerations</code> tolerations passed to the linstor controller pod</li> </ul> </li> </ul> <p>Previously, linstor satellite pods required nodes marked with <code>linstor.linbit.com/piraeus-node=true</code>. The new   default value does not place this restriction on nodes. To restore the old behaviour, set the following affinity:   <pre><code>affinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: linstor.linbit.com/piraeus-node\n          operator: In\n          values:\n          - \"true\"\n</code></pre></p> </li> <li> <p>Other changed helm settings:   <code>drbdKernelModuleInjectionMode</code>: renamed to <code>kernelModuleInjectionMode</code> <code>kernelModImage</code>: renamed to <code>kernelModuleInjectionImage</code>   If old settings are used, Helm will return an error.</p> </li> </ul>"},{"location":"how-to/upgrade/UPGRADE/#upgrade-between-v0-versions","title":"Upgrade between v0.* versions","text":"<p>While the API version was <code>v1alpha1</code>, this project did not maintain stability of or provide conversion of the Custom Resource Definitions.</p> <p>If you are using the Helm deployment, you may find that upgrades fail with errors similar to the following:</p> <pre><code>UPGRADE FAILED: cannot patch \"piraeus-op-cs\" with kind LinstorController: LinstorController.piraeus.linbit.com \"piraeus-op-cs\" is invalid: spec.etcdURL: Required value\n</code></pre> <p>The simplest solution in this case is to manually replace the CRD:</p> <pre><code>kubectl replace -f charts/piraeus/crds/piraeus.linbit.com_linstorcsidrivers_crd.yaml\nkubectl replace -f charts/piraeus/crds/piraeus.linbit.com_linstorsatellitesets_crd.yaml\nkubectl replace -f charts/piraeus/crds/piraeus.linbit.com_linstorcontrollers_crd.yaml\n</code></pre> <p>Then continue with the Helm upgrade. Values that are lost during the replacement will be set again by Helm.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This is the API Reference for Piraeus Operator. A user may make modifications to these resources to change the cluster state (<code>LinstorCluster</code> or <code>LinstorSatelliteConfiguration</code>) or check the status of a resource (<code>LinstorSatellite</code>).</p> <ul> <li> <p>LinstorCluster</p> <p>This resource controls the state of the LINSTOR\u00ae cluster and integration with Kubernetes.</p> <p> Reference</p> </li> <li> <p>LinstorSatelliteConfiguration</p> <p>This resource controls the state of the LINSTOR Satellites, optionally applying it to only a subset of nodes.</p> <p> Reference</p> </li> <li> <p>LinstorNodeConnection</p> <p>This resource controls the state of the LINSTOR\u00ae node connections.</p> <p> Reference</p> </li> <li> <p>LinstorSatellite</p> <p>This resource controls the state of a single LINSTOR Satellite. This resource is not intended to be changed directly, instead it is created by the Piraeus Operator by merging all matching <code>LinstorSatelliteConfiguration</code> resources.</p> <p> Reference</p> </li> </ul>"},{"location":"reference/linstorcluster/","title":"<code>LinstorCluster</code>","text":"<p>This resource controls the state of the LINSTOR\u00ae cluster and integration with Kubernetes. In particular, it controls:</p> <ul> <li>LINSTOR Controller</li> <li>LINSTOR CSI Driver</li> <li><code>LinstorSatellite</code>, configured through <code>LinstorSatelliteConfiguration</code> resources.</li> </ul>"},{"location":"reference/linstorcluster/#spec","title":"<code>.spec</code>","text":"<p>Configures the desired state of the cluster.</p>"},{"location":"reference/linstorcluster/#specnodeselector","title":"<code>.spec.nodeSelector</code>","text":"<p>Selects on which nodes Piraeus Datastore should be deployed. Nodes that are excluded by the selector will not be able to run any workload using a Piraeus volume.</p> <p>If empty (the default), Piraeus will be deployed on all nodes in the cluster.</p> <p>When this is used together with <code>.spec.nodeAffinity</code>, both need to match in order for a node to run Piraeus.</p>"},{"location":"reference/linstorcluster/#example","title":"Example","text":"<p>This example restricts Piraeus Datastore to nodes matching <code>example.com/storage: \"yes\"</code>:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  nodeSelector:\n    example.com/storage: \"yes\"\n</code></pre>"},{"location":"reference/linstorcluster/#specnodeaffinity","title":"<code>.spec.nodeAffinity</code>","text":"<p>Selects on which nodes Piraeus Datastore should be deployed. Nodes that are excluded by the affinity will not be able to run any workload using a Piraeus volume.</p> <p>If empty (the default), Piraeus will be deployed on all nodes in the cluster.</p> <p>When this is used together with <code>.spec.nodeSelector</code>, both need to match in order for a node to run Piraeus.</p>"},{"location":"reference/linstorcluster/#example_1","title":"Example","text":"<p>This example restricts Piraeus Datastore to nodes in zones <code>a</code> and <code>b</code>, but not on <code>control-plane</code> nodes:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  nodeAffinity:\n    nodeSelectorTerms:\n      - matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n              - a\n              - b\n          - key: node-role.kubernetes.io/control-plane\n            operator: DoesNotExist\n</code></pre>"},{"location":"reference/linstorcluster/#specrepository","title":"<code>.spec.repository</code>","text":"<p>Sets the default image registry to use for all Piraeus images. The full image name is created by appending an image identifier and tag.</p> <p>If empty (the default), Piraeus will use <code>quay.io/piraeusdatastore</code>.</p> <p>The current list of default images is available here.</p>"},{"location":"reference/linstorcluster/#example_2","title":"Example","text":"<p>This example pulls all Piraeus images from <code>registry.example.com/piraeus-mirror</code> rather than <code>quay.io/piraeusdatastore</code>.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  repository: registry.example.com/piraeus-mirror\n</code></pre>"},{"location":"reference/linstorcluster/#specproperties","title":"<code>.spec.properties</code>","text":"<p>Sets the given properties on the LINSTOR Controller level, applying them to the whole Cluster.</p>"},{"location":"reference/linstorcluster/#example_3","title":"Example","text":"<p>This example sets the port range used for DRBD\u00ae volumes to <code>10000-20000</code>.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  properties:\n    - name: TcpPortAutoRange\n      value: \"10000-20000\"\n</code></pre>"},{"location":"reference/linstorcluster/#speclinstorpassphrasesecret","title":"<code>.spec.linstorPassphraseSecret</code>","text":"<p>Configures the LINSTOR passphrase, used by LINSTOR when creating encrypted volumes and storing access credentials for backups.</p> <p>The referenced secret must exist in the same namespace as the operator (by default <code>piraeus-datastore</code>), and have a <code>MASTER_PASSPHRASE</code> entry.</p>"},{"location":"reference/linstorcluster/#example_4","title":"Example","text":"<p>This example configures a passphrase <code>example-passphrase</code>. Please choose a different passphrase for your deployment.</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: linstor-passphrase\n  namespace: piraeus-datastore\ndata:\n  # CHANGE THIS TO USE YOUR OWN PASSPHRASE!\n  # Created by: echo -n \"example-passphrase\" | base64\n  MASTER_PASSPHRASE: ZXhhbXBsZS1wYXNzcGhyYXNl\n---\napiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  linstorPassphraseSecret: linstor-passphrase\n</code></pre>"},{"location":"reference/linstorcluster/#specpatches","title":"<code>.spec.patches</code>","text":"<p>The given patches will be applied to all resources controlled by the operator. The patches are forwarded to <code>kustomize</code> internally, and take the same format.</p> <p>The unpatched resources are available in the subdirectories of the <code>pkg/resources</code> directory.</p>"},{"location":"reference/linstorcluster/#warning","title":"Warning","text":"<p>No checks are run on the result of user-supplied patches: the resources are applied as-is. Patching some fundamental aspect, such as removing a specific volume from a container may lead to a degraded cluster.</p>"},{"location":"reference/linstorcluster/#example_5","title":"Example","text":"<p>This example sets a CPU limit of <code>10m</code> on the CSI Node init container and changes the LINSTOR Controller service to run in <code>SingleStack</code> mode.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  patches:\n    - target:\n        kind: Daemonset\n        name: csi-node\n      patch: |-\n        - op: add\n          path: /spec/template/spec/initContainers/0/resources\n          value:\n            limits:\n               cpu: 10m\n    - target:\n        kind: Service\n        name: linstor-controller\n      patch: |-\n        apiVersion: v1\n        kind: service\n        metadata:\n          name: linstor-controller\n        spec:\n          ipFamilyPolicy: SingleStack\n</code></pre>"},{"location":"reference/linstorcluster/#specexternalcontroller","title":"<code>.spec.externalController</code>","text":"<p>Configures the Operator to use an external controller instead of deploying one in the Cluster.</p>"},{"location":"reference/linstorcluster/#example_6","title":"Example","text":"<p>This examples instructs the Operator to use the external LINSTOR Controller reachable at <code>http://linstor.example.com:3370</code>.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  externalController:\n    url: http://linstor.example.com:3370\n</code></pre>"},{"location":"reference/linstorcluster/#speccontroller","title":"<code>.spec.controller</code>","text":"<p>Controls the LINSTOR Controller Deployment: * Setting <code>enabled: false</code> disables the controller deployment entirely. See also <code>.spec.externalController</code>. * Setting a <code>podTemplate:</code> allows for simple modification of the LINSTOR Controller Deployment.</p>"},{"location":"reference/linstorcluster/#example_7","title":"Example","text":"<p>This example configures a resource request of <code>memory: 1Gi</code> for the LINSTOR Controller Deployment:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  controller:\n    enabled: true\n    podTemplate:\n      spec:\n        containers:\n          - name: linstor-controller\n            resources:\n              requests:\n                memory: 1Gi\n</code></pre>"},{"location":"reference/linstorcluster/#speccsicontroller","title":"<code>.spec.csiController</code>","text":"<p>Controls the CSI Controller Deployment: * Setting <code>enabled: false</code> disables the deployment entirely. * Setting a <code>podTemplate:</code> allows for simple modification of the CSI Controller Deployment.</p>"},{"location":"reference/linstorcluster/#example_8","title":"Example","text":"<p>This example configures a resource request of <code>cpu: 10m</code> for the CSI Controller Deployment:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  csiController:\n    enabled: true\n    podTemplate:\n      spec:\n        containers:\n          - name: linstor-csi\n            resources:\n              requests:\n                memory: 1Gi\n</code></pre>"},{"location":"reference/linstorcluster/#speccsinode","title":"<code>.spec.csiNode</code>","text":"<p>Controls the CSI Node DaemonSet: * Setting <code>enabled: false</code> disables the deployment entirely. * Setting a <code>podTemplate:</code> allows for simple modification of the CSI Node DaemonSet.</p>"},{"location":"reference/linstorcluster/#example_9","title":"Example","text":"<p>This example configures a resource request of <code>cpu: 10m</code> for the CSI Node DaemonSet:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  csiNode:\n    enabled: true\n    podTemplate:\n      spec:\n        containers:\n          - name: linstor-csi\n            resources:\n              requests:\n                memory: 1Gi\n</code></pre>"},{"location":"reference/linstorcluster/#spechighavailabilitycontroller","title":"<code>.spec.highAvailabilityController</code>","text":"<p>Controls the High Availability Controller DaemonSet: * Setting <code>enabled: false</code> disables the deployment entirely. * Setting a <code>podTemplate:</code> allows for simple modification of the CSI Node Deployment.</p>"},{"location":"reference/linstorcluster/#example_10","title":"Example","text":"<p>This example configures a resource request of <code>cpu: 10m</code> for the CSI Node Deployment:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  highAvailabilityController:\n    enabled: true\n    podTemplate:\n      spec:\n        containers:\n          - name: ha-controller\n            resources:\n              requests:\n                memory: 1Gi\n</code></pre>"},{"location":"reference/linstorcluster/#specinternaltls","title":"<code>.spec.internalTLS</code>","text":"<p>Configures a TLS secret used by the LINSTOR Controller to: * Validate the certificate of the LINSTOR Satellites, that is the Satellites must have certificates signed by <code>ca.crt</code>. * Provide a client certificate for authentication with LINSTOR Satellites, that is <code>tls.key</code> and <code>tls.crt</code> must be accepted by the Satellites.</p> <p>To configure TLS communication between Satellite and Controller, <code>LinstorSatelliteConfiguration.spec.internalTLS</code> must be set accordingly.</p> <p>Setting a <code>secretName</code> is optional, it will default to <code>linstor-controller-internal-tls</code>.</p> <p>Optional, a reference to a cert-manager <code>Issuer</code> can be provided to let the operator create the required secret.</p> <p>If the referenced secret does not contain a <code>ca.crt</code> certificate of a certificate authority, a <code>caReference</code> pointing to a secondary <code>Secret</code> or <code>ConfigMap</code> resource can be configured.</p>"},{"location":"reference/linstorcluster/#example_11","title":"Example","text":"<p>This example creates a manually provisioned TLS secret and references it in the LinstorCluster configuration.</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-linstor-controller-tls\n  namespace: piraeus-datastore\ndata:\n  ca.crt: LS0tLS1CRUdJT...\n  tls.crt: LS0tLS1CRUdJT...\n  tls.key: LS0tLS1CRUdJT...\n---\napiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  internalTLS:\n    secretName: my-linstor-controller-tls\n</code></pre>"},{"location":"reference/linstorcluster/#example_12","title":"Example","text":"<p>This example sets up automatic creation of the LINSTOR Controller TLS secret using a cert-manager issuer named <code>piraeus-root</code>. The certificates will be validated against the certificate in the <code>trust-root</code> ConfigMap.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  internalTLS:\n    certManager:\n      kind: Issuer\n      name: piraeus-root\n    caReference:\n      name: trust-root\n      kind: ConfigMap\n      key: root.crt\n</code></pre>"},{"location":"reference/linstorcluster/#specapitls","title":"<code>.spec.apiTLS</code>","text":"<p>Configures the TLS secrets used to secure the LINSTOR API. There are four different secrets to configure:</p> <ul> <li><code>apiSecretName</code>: sets the name of the secret used by the LINSTOR Controller to enable HTTPS. Defaults to   <code>linstor-api-tls</code>. All clients of the API must have certificates signed by the <code>ca.crt</code> of this secret.</li> <li><code>clientSecretName</code>: sets the name of the secret used by the Operator to connect to the LINSTOR API. Defaults to   <code>linstor-client-tls</code>. Must be trusted by <code>ca.crt</code> in the API Secret. Also used by the LINSTOR Controller to configure   the included LINSTOR CLI.</li> <li><code>csiControllerSecretName</code> sets the name of the secret used by the CSI Controller. Defaults to   <code>linstor-csi-controller-tls</code>. Must be trusted by <code>ca.crt</code> in the API Secret.</li> <li><code>csiNodeSecretName</code> sets the name of the secret used by the CSI Controller. Defaults to <code>linstor-csi-node-tls</code>.   Must be trusted by <code>ca.crt</code> in the API Secret.</li> </ul> <p>Optional, a reference to a cert-manager <code>Issuer</code> can be provided to let the operator create the required secrets.</p> <p>If the referenced secret does not contain a <code>ca.crt</code> certificate of a certificate authority, a <code>caReference</code> pointing to a secondary <code>Secret</code> or <code>ConfigMap</code> resource can be configured.</p>"},{"location":"reference/linstorcluster/#example_13","title":"Example","text":"<p>This example creates a manually provisioned TLS secret and references it in the LinstorCluster configuration. It uses the same secret for all clients of the LINSTOR API.</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-linstor-api-tls\n  namespace: piraeus-datastore\ndata:\n  ca.crt: LS0tLS1CRUdJT...\n  tls.crt: LS0tLS1CRUdJT...\n  tls.key: LS0tLS1CRUdJT...\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-linstor-client-tls\n  namespace: piraeus-datastore\ndata:\n  ca.crt: LS0tLS1CRUdJT...\n  tls.crt: LS0tLS1CRUdJT...\n  tls.key: LS0tLS1CRUdJT...\n---\napiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  apiTLS:\n    apiSecretName: my-linstor-api-tls\n    clientSecretName: my-linstor-client-tls\n    csiControllerSecretName: my-linstor-client-tls\n    csiNodeSecretName: my-linstor-client-tls\n</code></pre>"},{"location":"reference/linstorcluster/#example_14","title":"Example","text":"<p>This example sets up automatic creation of the LINSTOR API and LINSTOR Client TLS secret using a cert-manager issuer named <code>piraeus-root</code>. The certificates will be validated against the certificate in the <code>trust-root</code> Secret.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec:\n  apiTLS:\n    certManager:\n      kind: Issuer\n      name: piraeus-root\n    caReference:\n      name: trust-root\n      kind: Secret\n      key: root.crt\n</code></pre>"},{"location":"reference/linstorcluster/#status","title":"<code>.status</code>","text":"<p>Reports the actual state of the cluster.</p>"},{"location":"reference/linstorcluster/#statusconditions","title":"<code>.status.conditions</code>","text":"<p>The Operator reports the current state of the Cluster through a set of conditions. Conditions are identified by their <code>type</code>.</p> <code>type</code> Explanation <code>Applied</code> All Kubernetes resources controlled by the Operator are applied and up to date. <code>Available</code> The LINSTOR Controller is deployed and reponding to requests. <code>Configured</code> The LINSTOR Controller is configured with the properties from <code>.spec.properties</code>"},{"location":"reference/linstornodeconnection/","title":"<code>LinstorNodeConnection</code>","text":"<p>This resource controls the state of the LINSTOR\u00ae node connections.</p> <p>Node connections control the DRBD options set on a particular path, such as which protocol and network interface to use.</p>"},{"location":"reference/linstornodeconnection/#spec","title":"<code>.spec</code>","text":"<p>Configures the desired state of the node connections.</p>"},{"location":"reference/linstornodeconnection/#specselector","title":"<code>.spec.selector</code>","text":"<p>Selects which connections the resource should apply to. By default, a <code>LinstorNodeConnection</code> resource applies to every possible connection in the cluster.</p> <p>A resource applies to a connection if one of the provided selectors match. A selector itself can contain multiple expressions. If multiple expressions are specified in a selector, the connection must match all of them.</p> <p>Every expression requires a label name (<code>key</code>) on which it operates and an operator <code>op</code>. Depending on the operator, you can also specify specific values for the node label.</p> <p>The following operators are available:</p> <ul> <li><code>Exists</code>, the label specified in <code>key</code> is present on both nodes of the connection, with any value. This is the   default.</li> <li><code>DoesNotExist</code>, the label specified in <code>key</code> is not present on the nodes in the connections.</li> <li><code>In</code>, the label specified in <code>key</code> matches any of the provided <code>values</code> for the nodes in the connection.</li> <li><code>NotIn</code> the label specified in <code>key</code> does not match any of the provided <code>values</code> for the nodes in the connection.</li> <li><code>Same</code> the label specified in <code>key</code> has the same value for the nodes in the connection.</li> <li><code>NotSame</code> the label specified in <code>key</code> has different values for the nodes in the connection.</li> </ul>"},{"location":"reference/linstornodeconnection/#example","title":"Example","text":"<p>This example restricts the resource to connections between nodes matching <code>example.com/storage: \"yes\"</code>:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorNodeConnection\nmetadata:\n  name: selector\nspec:\n  selector:\n    - matchLabels:\n        - key: example.com/storage\n          op: In\n          values:\n            - yes\n</code></pre>"},{"location":"reference/linstornodeconnection/#example_1","title":"Example","text":"<p>This example restricts the resource to connections between nodes in the same region, but different zones.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorNodeConnection\nmetadata:\n  name: selector\nspec:\n  selector:\n    - matchLabels:\n        - key: topology.kubernetes.io/region\n          op: Same\n        - key: topology.kubernetes.io/zone\n          op: NotSame\n</code></pre>"},{"location":"reference/linstornodeconnection/#specpaths","title":"<code>.spec.paths</code>","text":"<p>Paths configure one or more network connections between nodes. If a path is configured, LINSTOR will use the given network interface to configure DRBD replication. The network interfaces have to be registered with LINSTOR first, using <code>linstor node interface create ...</code>.</p>"},{"location":"reference/linstornodeconnection/#example_2","title":"Example","text":"<p>This example configures all nodes to use the \"data-nic\" network interface instead of the default interface.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorNodeConnection\nmetadata:\n  name: network-paths\nspec:\n  paths:\n    - name: path1\n      interface: data-nic\n</code></pre>"},{"location":"reference/linstornodeconnection/#specproperties","title":"<code>.spec.properties</code>","text":"<p>Sets the given properties on the LINSTOR Node Connection level.</p>"},{"location":"reference/linstornodeconnection/#example_3","title":"Example","text":"<p>This example sets the DRBD\u00ae protocol to <code>C</code>.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorNodeConnection\nmetadata:\n  name: drbd-options\nspec:\n  properties:\n    - name: DrbdOptions/Net/protocol\n      value: C\n</code></pre>"},{"location":"reference/linstornodeconnection/#status","title":"<code>.status</code>","text":"<p>Reports the actual state of the connections.</p>"},{"location":"reference/linstornodeconnection/#statusconditions","title":"<code>.status.conditions</code>","text":"<p>The Operator reports the current state of the LINSTOR Node Connection through a set of conditions. Conditions are identified by their <code>type</code>.</p> <code>type</code> Explanation <code>Configured</code> The LINSTOR Node Connection is applied to all matching pairs of nodes."},{"location":"reference/linstorsatellite/","title":"<code>LinstorSatellite</code>","text":"<p>This resource controls the state of a LINSTOR\u00ae satellite.</p> <p>NOTE: This resource is not intended to be changed directly, instead it is created by the Piraeus Operator by merging all matching <code>LinstorSatelliteConfiguration</code> resources.</p>"},{"location":"reference/linstorsatellite/#spec","title":"<code>.spec</code>","text":"<p>Holds the desired state the satellite.</p>"},{"location":"reference/linstorsatellite/#specrepository","title":"<code>.spec.repository</code>","text":"<p>Holds the default image registry to use for all Piraeus images. Inherited from <code>LinstorCluster</code>.</p> <p>If empty (the default), the operator will use <code>quay.io/piraeusdatastore</code>.</p>"},{"location":"reference/linstorsatellite/#specclusterref","title":"<code>.spec.clusterRef</code>","text":"<p>Holds a reference to the <code>LinstorCluster</code> that controls this satellite.</p>"},{"location":"reference/linstorsatellite/#specstoragepools","title":"<code>.spec.storagePools</code>","text":"<p>Holds the storage pools to configure on the node. Inherited from matching <code>LinstorSatelliteConfiguration</code> resources.</p>"},{"location":"reference/linstorsatellite/#specproperties","title":"<code>.spec.properties</code>","text":"<p>Holds the properties which should be set on the node level. Inherited from matching <code>LinstorSatelliteConfiguration</code> resources.</p>"},{"location":"reference/linstorsatellite/#specinternaltls","title":"<code>.spec.internalTLS</code>","text":"<p>Configures a TLS secret used by the LINSTOR Satellite. Inherited from matching <code>LinstorSatelliteConfiguration</code> resources.</p>"},{"location":"reference/linstorsatellite/#specpatches","title":"<code>.spec.patches</code>","text":"<p>Holds patches to apply to the Kubernetes resources. Inherited from matching <code>LinstorSatelliteConfiguration</code> resources.</p>"},{"location":"reference/linstorsatellite/#status","title":"<code>.status</code>","text":"<p>Reports the actual state of the satellite.</p>"},{"location":"reference/linstorsatellite/#statusconditions","title":"<code>.status.conditions</code>","text":"<p>The Operator reports the current state of the LINSTOR Satellite through a set of conditions. Conditions are identified by their <code>type</code>.</p> <code>type</code> Explanation <code>Applied</code> All Kubernetes resources were applied. <code>Available</code> The LINSTOR Satellite is connected to the LINSTOR Controller <code>Configured</code> Storage Pools and Properties are configured on the Satellite <code>EvacuationCompleted</code> Only available when the Satellite is being deleted: Indicates progress of the eviction of resources."},{"location":"reference/linstorsatelliteconfiguration/","title":"<code>LinstorSatelliteConfiguration</code>","text":"<p>This resource controls the state of one or more LINSTOR\u00ae satellites.</p>"},{"location":"reference/linstorsatelliteconfiguration/#spec","title":"<code>.spec</code>","text":"<p>Configures the desired state of satellites.</p>"},{"location":"reference/linstorsatelliteconfiguration/#specnodeselector","title":"<code>.spec.nodeSelector</code>","text":"<p>Selects which nodes the LinstorSatelliteConfiguration should apply to. If empty, the configuration applies to all nodes.</p>"},{"location":"reference/linstorsatelliteconfiguration/#example","title":"Example","text":"<p>This example sets the <code>AutoplaceTarget</code> property to <code>no</code> on all nodes labelled <code>piraeus.io/autoplace: \"no\"</code>.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: disabled-nodes\nspec:\n  nodeSelector:\n    piraeus.io/autplace: \"no\"\n  properties:\n    - name: AutoplaceTarget\n      value: \"no\"\n</code></pre>"},{"location":"reference/linstorsatelliteconfiguration/#specnodeaffinity","title":"<code>.spec.nodeAffinity</code>","text":"<p>Selects which nodes the LinstorSatelliteConfiguration should apply to. If empty, the configuration applies to all nodes.</p> <p>When this is used together with <code>.spec.nodeSelector</code>, both need to match in order for the configuration to apply to a node.</p>"},{"location":"reference/linstorsatelliteconfiguration/#example_1","title":"Example","text":"<p>This example sets the <code>AutoplaceTarget</code> property to <code>no</code> on all non-worker nodes:</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: disabled-nodes\nspec:\n  nodeAffinity:\n    nodeSelectorTerms:\n      - matchExpressions:\n          - key: node-role.kubernetes.io/control-plane\n            operator: Exists\n  properties:\n    - name: AutoplaceTarget\n      value: \"no\"\n</code></pre>"},{"location":"reference/linstorsatelliteconfiguration/#specproperties","title":"<code>.spec.properties</code>","text":"<p>Sets the given properties on the LINSTOR Satellite level.</p> <p>The property value can either be set directly using <code>value</code>, inherited from the Kubernetes Node's metadata using <code>valueFrom</code>, or expanded from Kubernetes Node's metadata using <code>expandFrom</code>. Metadata fields are specified using the same syntax as the Downward API for Pods.</p> <p>Using <code>expandFrom</code> allows for using field references matching more than one field. Either specify a field that is already a map (<code>metadata.labels</code> or <code>metadata.annotations</code>), or select a subset by using <code>*</code> at the end of a key. Using <code>*</code> will select the keys and values matching the prefix up to the <code>*</code> character. There are two ways to use <code>expandFrom</code>:</p> <ul> <li>Setting a <code>nameTemplate</code> will create one property per matched field. The property name is generated by taking   the <code>name</code> field and appending the expanded <code>nameTemplate</code>. <code>nameTemplate</code> supports the following expansions:</li> <li><code>$1</code> is replaced with the field key, i.e. the part matched by the <code>+</code> character.</li> <li><code>$2</code> is replaced with the value of the matched field.</li> </ul> <p>The <code>valueTemplate</code> is expanded using the same replacements, and sets the property value. * Setting no <code>nameTemplate</code> will create one property using <code>name</code>. The value of the property is the   joined expansion of the <code>valueTemplate</code> field for every matched field. See above for supported expansions. The result   is joined using the optional <code>delimiter</code> value.</p> <p>In addition, setting <code>optional</code> to true means the property is only applied if the value is not empty. This is useful in case the property value should be inherited from the node's metadata</p>"},{"location":"reference/linstorsatelliteconfiguration/#example_2","title":"Example","text":"<p>This examples sets the following Properties on every satellite: * <code>PrefNic</code> (the preferred network interface) is always set to <code>default-ipv6</code> * <code>Aux/example-property</code> (an auxiliary property, unused by LINSTOR itself) takes the value from the <code>piraeus.io/example</code>   label of the Kubernetes Node. If a node has no <code>piraeus.io/example</code> label, the property value will be <code>\"\"</code>. * <code>AutoplaceTarget</code> (if set to <code>no</code>, will exclude the node from LINSTOR's Autoplacer) takes the value from the   <code>piraeus.io/autoplace</code> annotation of the Kubernetes Node. If a node has no <code>piraeus.io/autoplace</code> annotation, the   property will not be set. * <code>Aux/role/</code> copies all <code>node-role.kubernetes.io/*</code> label keys and values. For example, a worker node with the   <code>node-role.kubernetes.io/worker: \"true\"</code> label will have <code>Aux/role/worker</code> set to <code>\"true\"</code>. * <code>Aux/features</code> copies the names of all <code>feature.example.com/*</code> label keys to the value, joined by <code>,</code>. For example,   a node with <code>feature.example.com/gpu</code> and <code>feature.example.com/storage</code> will have <code>Aux/features</code> set to   <code>\"gpu,storage\"</code>.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: ipv6-nodes\nspec:\n  properties:\n    - name: PrefNic\n      value: \"default-ipv6\"\n    - name: Aux/example-property\n      valueFrom:\n        nodeFieldRef: metadata.labels['piraeus.io/example']\n    - name: AutoplaceTarget\n      valueFrom:\n        nodeFieldRef: metadata.annotations['piraeus.io/autoplace']\n      optional: yes\n    - name: Aux/role/\n      expandFrom:\n        nodeFieldRef: metadata.labels['node-role.kubernetes.io/*']\n        nameTemplate: \"$1\"\n        valueTemplate: \"$2\"\n    - name: Aux/features\n      expandFrom:\n        nodeFieldRef: metadata.labels['feature.example.com/*']\n        valueTemplate: \"$1\"\n        delimiter: \",\"\n</code></pre>"},{"location":"reference/linstorsatelliteconfiguration/#specstoragepools","title":"<code>.spec.storagePools</code>","text":"<p>Configures LINSTOR Storage Pools.</p> <p>Every Storage Pool needs at least a <code>name</code>, and a type. Types are specified by setting a (potentially empty) value on the matching key. Available types are:</p> <ul> <li><code>lvmPool</code>: Configures a LVM Volume Group as storage   pool. Defaults to using the storage pool name as the VG name. Can be overridden by setting <code>volumeGroup</code>.</li> <li><code>lvmThinPool</code>: Configures a LVM Thin Pool as storage pool.   Defaults to using the storage pool name as name for the thin pool volume and the storage pool name prefixed by   <code>linstor_</code> as the VG name. Can be overridden by setting <code>thinPool</code> and <code>volumeGroup</code>.</li> <li><code>filePool</code>: Configures a file system based storage pool. Configures a host directory as location for the volume files.   Defaults to using the <code>/var/lib/linstor-pools/&lt;storage pool name&gt;</code> directory.</li> <li><code>fileThinPool</code>: Configures a file system based storage pool. Behaves the same as <code>filePool</code>, except the files will   be thinly allocated on file systems that support sparse files.</li> <li><code>zfsPool</code>: Configure a ZFS ZPool as storage pool. Defaults to using the storage   pool name as name for the zpool. Can be overriden by setting <code>zPool</code>.</li> <li><code>zfsThinPool</code>: Configure a ZFS ZPool as storage pool. Behaves the same as   <code>zfsPool</code>, except the contained zVol will be created using sparse reservation.</li> </ul> <p>Optionally, you can configure LINSTOR to automatically create the backing pools. <code>source.hostDevices</code> takes a list of raw block devices, which LINSTOR will prepare as the chosen backing pool.</p> <p>All storage pools also can also be configured with <code>properties</code>. Properties are set on the Storage Pool level. The configuration values have the same form as Satellite Properties.</p>"},{"location":"reference/linstorsatelliteconfiguration/#example_3","title":"Example","text":"<p>This example configures these LINSTOR Storage Pools on all satellites: * A LVM Pool named <code>vg1</code>. It will use the VG <code>vg1</code>, which needs to exist on the nodes already. * A LVM Thin Pool named <code>vg1-thin</code>. It will use the thin pool <code>vg1/thin</code>, which also needs to exist on the nodes. * A LVM Pool named <code>vg2-from-raw-devices</code>. It will use the VG <code>vg2</code>, which will be created on demand from the raw   devices <code>/dev/sdb</code> and <code>/dev/sdc</code> if it does not exist already. In addition, it sets the <code>StorDriver/LvcreateOptions</code>   property to <code>-i 2</code>, which causes every created LV to be striped across 2 PVs. * A File System Pool named <code>fs1</code>. It will use the <code>/var/lib/linstor-pools/fs1</code> directory on the host, creating the   directory if necessary. * A File System Pool named <code>fs2</code>, using sparse files. It will use the custom <code>/mnt/data</code> directory on the host. * A ZFS Pool named <code>zfs1</code>. It will use ZPool <code>zfs1</code>, which needs to exist on the nodes already. * A ZFS Thin Pool named <code>zfs2</code>. It will use ZPool <code>zfs-thin2</code>, which will be created on demand from the raw device   <code>/dev/sdd</code>.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: storage-satellites\nspec:\n  storagePools:\n    - name: vg1\n      lvmPool: {}\n    - name: vg1-thin\n      lvmThinPool:\n        volumeGroup: vg1\n        thinPool: thin\n    - name: vg2-from-raw-devices\n      lvmPool:\n        volumeGroup: vg2\n      source:\n        hostDevices:\n          - /dev/sdb\n          - /dev/sdc\n      properties:\n        - name: StorDriver/LvcreateOptions\n          value: '-i 2'\n    - name: fs1\n      filePool: {}\n    - name: fs2\n      fileThinPool:\n        directory: /mnt/data\n    - name: zfs1\n      zfsPool: {}\n    - name: zfs2\n      zfsThinPool:\n        zPool: zfs-thin2\n      source:\n        hostDevices:\n        - /dev/sdd\n</code></pre>"},{"location":"reference/linstorsatelliteconfiguration/#specinternaltls","title":"<code>.spec.internalTLS</code>","text":"<p>Configures a TLS secret used by the LINSTOR Satellites to: * Validate the certificate of the LINSTOR Controller, that is the Controller must have certificates signed by <code>ca.crt</code>. * Provide a server certificate for authentication by the LINSTOR Controller, that is <code>tls.key</code> and <code>tls.crt</code> must be accepted by the Controller.</p> <p>To configure TLS communication between Satellite and Controller, <code>LinstorCluster.spec.internalTLS</code> must be set accordingly.</p> <p>Setting a <code>secretName</code> is optional, it will default to <code>&lt;node-name&gt;-tls</code>, where <code>&lt;node-name&gt;</code> is replaced with the name of the Kubernetes Node.</p> <p>Optional, a reference to a cert-manager <code>Issuer</code> can be provided to let the operator create the required secret.</p>"},{"location":"reference/linstorsatelliteconfiguration/#example_4","title":"Example","text":"<p>This example creates a manually provisioned TLS secret and references it in the LinstorSatelliteConfiguration, setting it for all nodes.</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: my-node-tls\n  namespace: piraeus-datastore\ndata:\n  ca.crt: LS0tLS1CRUdJT...\n  tls.crt: LS0tLS1CRUdJT...\n  tls.key: LS0tLS1CRUdJT...\n---\napiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: satellite-tls\nspec:\n  internalTLS:\n    secretName: my-node-tls\n</code></pre>"},{"location":"reference/linstorsatelliteconfiguration/#example_5","title":"Example","text":"<p>This example sets up automatic creation of the LINSTOR Satellite TLS secrets using a cert-manager issuer named <code>piraeus-root</code>.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: satellite-tls\nspec:\n  internalTLS:\n    certManager:\n      kind: Issuer\n      name: piraeus-root\n</code></pre>"},{"location":"reference/linstorsatelliteconfiguration/#specipfamilies","title":"<code>.spec.ipFamilies</code>","text":"<p>Configures the IP Family (IPv4 or IPv6) to use to connect to the LINSTOR Satellite.</p> <p>If unset, the LINSTOR Controller will attempt to reach the LINSTOR Satellite via all recognized addresses in the Satellite Pods' Status. If set, the LINSTOR Controller will only attempt to reach the LINSTOR Satellite via all addresses matching the listed IP Families.</p> <p>Valid values are <code>IPv4</code> and <code>IPv6</code>.</p>"},{"location":"reference/linstorsatelliteconfiguration/#example_6","title":"Example","text":"<p>This example configures the LINSTOR Controller to only use IPv4, even in a dual stack cluster.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: ipv4-only\nspec:\n  ipFamilies:\n  - IPv4\n</code></pre>"},{"location":"reference/linstorsatelliteconfiguration/#specpodtemplate","title":"<code>.spec.podTemplate</code>","text":"<p>Configures the Pod used to run the LINSTOR Satellite.</p> <p>The template is applied as a patch (see <code>.spec.patches</code>) to the default resources, so it can be \"sparse\".</p>"},{"location":"reference/linstorsatelliteconfiguration/#example_7","title":"Example","text":"<p>This example configures a resource request of <code>cpu: 100m</code> on the satellite, and also enables host networking.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: resource-and-host-network\nspec:\n  podTemplate:\n    spec:\n      hostNetwork: true\n      containers:\n        - name: linstor-satellite\n          resources:\n            requests:\n              cpu: 100m\n</code></pre>"},{"location":"reference/linstorsatelliteconfiguration/#specpatches","title":"<code>.spec.patches</code>","text":"<p>The given patches will be applied to all resources controlled by the operator. The patches are forwarded to <code>kustomize</code> internally, and take the same format.</p> <p>The unpatched resources are available in the subdirectories of the <code>pkg/resources/satellite</code> directory.</p>"},{"location":"reference/linstorsatelliteconfiguration/#warning","title":"Warning","text":"<p>No checks are run on the result of user-supplied patches: the resources are applied as-is. Patching some fundamental aspect, such as removing a specific volume from a container may lead to a degraded cluster.</p>"},{"location":"reference/linstorsatelliteconfiguration/#example_8","title":"Example","text":"<p>This example configures the LINSTOR Satellite to use the \"TRACE\" log level, creating very verbose output.</p> <pre><code>apiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: all-satellites\nspec:\n  patches:\n    - target:\n        kind: ConfigMap\n        name: satellite-config\n      patch: |-\n        apiVersion: v1\n        kind: ConfigMap\n        metadata:\n          name: satellite-config\n        data:\n          linstor_satellite.toml: |\n            [logging]\n              linstor_level = \"TRACE\"\n</code></pre>"},{"location":"reference/linstorsatelliteconfiguration/#status","title":"<code>.status</code>","text":"<p>Reports the actual state of the cluster.</p>"},{"location":"reference/linstorsatelliteconfiguration/#statusconditions","title":"<code>.status.conditions</code>","text":"<p>The Operator reports the current state of the Satellite Configuration through a set of conditions. Conditions are identified by their <code>type</code>.</p> <code>type</code> Explanation <code>Applied</code> The given configuration was applied to all <code>LinstorSatellite</code> resources."},{"location":"tutorial/","title":"Tutorials","text":"<p>These tutorials help you get started with Piraeus Datastore</p>"},{"location":"tutorial/#get-started","title":"Get Started","text":"<p>Get started with Piraeus Datastore by deploying Piraeus Operator and provisioning your first volume.</p>"},{"location":"tutorial/#creating-replicated-volumes","title":"Creating Replicated Volumes","text":"<p>Create replicated volumes, making your data accessible on any cluster node.</p>"},{"location":"tutorial/#creating-and-restoring-from-snapshots","title":"Creating and Restoring from Snapshots","text":"<p>Create snapshots and restore a volume, safeguarding data against accidental deletion.</p>"},{"location":"tutorial/get-started/","title":"Get Started","text":"<p>Learn about the ways to get started with Piraeus Datastore by deploying Piraeus Operator and provisioning your first volume.</p>"},{"location":"tutorial/get-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install the Linux kernel headers on the hosts.</li> <li>Install <code>kubectl</code> version <code>&gt;= 1.22</code></li> </ul>"},{"location":"tutorial/get-started/#install-piraeus-operator","title":"Install Piraeus Operator","text":"<p>In this tutorial we will be using <code>kubectl</code> with the built-in <code>kustomize</code> feature to deploy Piraeus Operator. All resources needed to run Piraeus Operator are included in a single Kustomization.</p> <p>Install Piraeus Operator by running:</p> <pre><code>$ kubectl apply --server-side -k \"https://github.com/piraeusdatastore/piraeus-operator//config/default?ref=v2.7.1\"\nnamespace/piraeus-datastore configured\n...\n</code></pre> <p>The Piraeus Operator will be installed in a new namespace <code>piraeus-datastore</code>. After a short wait the operator will be ready. The following command waits until the Operator is ready:</p> <pre><code>$ kubectl wait pod --for=condition=Ready -n piraeus-datastore -l app.kubernetes.io/component=piraeus-operator\npod/piraeus-operator-controller-manager-dd898f48c-bhbtv condition met\n</code></pre>"},{"location":"tutorial/get-started/#deploy-piraeus-datastore","title":"Deploy Piraeus Datastore","text":"<p>Now, we will deploy Piraeus Datastore using a new resource managed by Piraeus Operator. We create a <code>LinstorCluster</code>, which creates all the necessary resources (Deployments, Pods, and so on...) for our Datastore:</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: piraeus.io/v1\nkind: LinstorCluster\nmetadata:\n  name: linstorcluster\nspec: {}\nEOF\n</code></pre> <p>Again, all workloads will be deployed to the <code>piraeus-datastore</code> namespace. After a short wait the Datastore will ready:</p> <pre><code>$ kubectl wait pod --for=condition=Ready -n piraeus-datastore -l app.kubernetes.io/name=piraeus-datastore\npod/linstor-controller-65cbbc74db-9vm9n condition met\npod/linstor-csi-controller-5ccb7d84cd-tvd9h condition met\npod/linstor-csi-node-2lkpd condition met\npod/linstor-csi-node-hbcvv condition met\npod/linstor-csi-node-hmrd7 condition met\npod/n1.example.com condition met\npod/n2.example.com condition met\npod/n3.example.com condition met\npod/piraeus-operator-controller-manager-dd898f48c-bhbtv condition met\n</code></pre> <p>We can now inspect the state of the deployed LINSTOR\u00ae Cluster using the <code>linstor</code> client:</p> <pre><code>$ kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor node list\n+-------------------------------------------------------------------+\n| Node           | NodeType  | Addresses                   | State  |\n|===================================================================|\n| n1.example.com | SATELLITE | 10.116.72.166:3366 (PLAIN)  | Online |\n| n2.example.com | SATELLITE | 10.127.183.190:3366 (PLAIN) | Online |\n| n3.example.com | SATELLITE | 10.125.97.33:3366 (PLAIN)   | Online |\n+-------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/get-started/#configuring-storage","title":"Configuring Storage","text":"<p>We have not yet configured any storage location for our volumes. This can be accomplished by creating a new <code>LinstorSatelliteConfiguration</code> resource. We will create a storage pool of type <code>fileThinPool</code> on each node. We chose <code>fileThinPool</code> as it does not require further configuration on the host.</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: piraeus.io/v1\nkind: LinstorSatelliteConfiguration\nmetadata:\n  name: storage-pool\nspec:\n  storagePools:\n    - name: pool1\n      fileThinPool:\n        directory: /var/lib/piraeus-datastore/pool1\nEOF\n</code></pre> <p>This will cause some Pods to be recreated. While this occurs <code>linstor node list</code> will temporarily show offline nodes:</p> <pre><code>$ kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor node list\n+--------------------------------------------------------------------+\n| Node           | NodeType  | Addresses                   | State   |\n|====================================================================|\n| n1.example.com | SATELLITE | 10.116.72.166:3366 (PLAIN)  | OFFLINE |\n| n2.example.com | SATELLITE | 10.127.183.190:3366 (PLAIN) | OFFLINE |\n| n3.example.com | SATELLITE | 10.125.97.33:3366 (PLAIN)   | OFFLINE |\n+--------------------------------------------------------------------+\n</code></pre> <p>Waiting a bit longer, the nodes will be <code>Online</code> again. Once the nodes are connected again, we can verify that the storage pools were configured:</p> <pre><code>$ kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor storage-pool list\n+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n| StoragePool          | Node           | Driver    | PoolName                         | FreeCapacity | TotalCapacity | CanSnapshots | State | SharedName |\n|=========================================================================================================================================================|\n| DfltDisklessStorPool | n1.example.com | DISKLESS  |                                  |              |               | False        | Ok    |            |\n| DfltDisklessStorPool | n2.example.com | DISKLESS  |                                  |              |               | False        | Ok    |            |\n| DfltDisklessStorPool | n3.example.com | DISKLESS  |                                  |              |               | False        | Ok    |            |\n| pool1                | n1.example.com | FILE_THIN | /var/lib/piraeus-datastore/pool1 |    24.54 GiB |     49.30 GiB | True         | Ok    |            |\n| pool1                | n2.example.com | FILE_THIN | /var/lib/piraeus-datastore/pool1 |    23.03 GiB |     49.30 GiB | True         | Ok    |            |\n| pool1                | n3.example.com | FILE_THIN | /var/lib/piraeus-datastore/pool1 |    26.54 GiB |     49.30 GiB | True         | Ok    |            |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/get-started/#using-piraeus-datastore","title":"Using Piraeus Datastore","text":"<p>We now have successfully deployed and configured Piraeus Datastore, and are ready to create our first <code>PersistentVolume</code> in Kubernetes.</p> <p>First, we will set up a new <code>StorageClass</code> for our volumes. In the <code>StorageClass</code>, we specify the storage pool from above:</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: piraeus-storage\nprovisioner: linstor.csi.linbit.com\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  linstor.csi.linbit.com/storagePool: pool1\nEOF\n</code></pre> <p>Next, we will create a <code>PersistentVolumeClaim</code>, requesting 1G of storage from our newly created <code>StorageClass</code>.</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-volume\nspec:\n  storageClassName: piraeus-storage\n  resources:\n    requests:\n      storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\nEOF\n</code></pre> <p>When we check the created PersistentVolumeClaim, we can see that it remains in <code>Pending</code> state.</p> <pre><code>$ kubectl get persistentvolumeclaim\nNAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS      AGE\ndata-volume   Pending                                      piraeus-storage   14s\n</code></pre> <p>We first need to create a \"consumer\", which in this case is just a <code>Pod</code>. For our consumer, we will create a Deployment for a simple web server, serving files from our volume.</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: web-server\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: web-server\n    spec:\n      containers:\n        - name: web-server\n          image: nginx\n          volumeMounts:\n            - mountPath: /usr/share/nginx/html\n              name: data\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: data-volume\nEOF\n</code></pre> <p>After a short wait, the Pod is <code>Running</code>, and our <code>PersistentVolumeClaim</code> is now <code>Bound</code>:</p> <pre><code>$ kubectl wait pod --for=condition=Ready -l app.kubernetes.io/name=web-server\npod/web-server-84867b5449-hgdzx condition met\n$ kubectl get persistentvolumeclaim\nNAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\ndata-volume   Bound    pvc-9e1149e7-33db-47a7-8fc6-172514422143   1Gi        RWO            piraeus-storage   1m\n</code></pre> <p>Checking the running container, we see that the volume is mounted where we expected it:</p> <pre><code>$ kubectl exec deploy/web-server -- df -h /usr/share/nginx/html\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/drbd1000   973M   24K  906M   1% /usr/share/nginx/html\n</code></pre> <p>Taking a look with the <code>linstor</code> client, we can see that the volume is listed in LINSTOR and marked as <code>InUse</code> by the Pod.</p> <pre><code>$ kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor resource list-volumes\n+-------------------------------------------------------------------------------------------------------------------------------------------+\n| Node           | Resource                                 | StoragePool | VolNr | MinorNr | DeviceName    | Allocated | InUse  |    State |\n|===========================================================================================================================================|\n| n1.example.com | pvc-9e1149e7-33db-47a7-8fc6-172514422143 | pool1       |     0 |    1000 | /dev/drbd1000 | 16.91 MiB | InUse  | UpToDate |\n+-------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>We have now successfully set up Piraeus Datastore and used it to provision a Persistent Volume in a Kubernetes cluster.</p>"},{"location":"tutorial/replicated-volumes/","title":"Creating Replicated Volumes","text":"<p>Learn about creating replicated volumes, making your data accessible on any cluster node.</p> <p>In this tutorial you will learn how to create a replicated volume, and verify that the data stays accessible when moving Pods from one node to another.</p>"},{"location":"tutorial/replicated-volumes/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes Cluster with at least two nodes.</li> <li>An installed and configured Piraeus Datastore. Learn how to get started in our introduction tutorial</li> </ul>"},{"location":"tutorial/replicated-volumes/#creating-the-volume","title":"Creating the Volume","text":"<p>First, we will create a new <code>StorageClass</code> for our replicated volumes. We will be using the <code>pool1</code> storage pool from the Get Started tutorial, but this time also set the <code>placementCount</code> to 2, telling LINSTOR\u00ae to store the volume data on two nodes.</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: piraeus-storage-replicated\nprovisioner: linstor.csi.linbit.com\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  linstor.csi.linbit.com/storagePool: pool1\n  linstor.csi.linbit.com/placementCount: \"2\"\nEOF\n</code></pre> <p>Next, we will again create a <code>PersistentVolumeClaim</code>, requesting a 1G replicated volume from our newly created <code>StorageClass</code>.</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: replicated-volume\nspec:\n  storageClassName: piraeus-storage-replicated\n  resources:\n    requests:\n      storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\nEOF\n</code></pre> <p>For our workload, we will create a Pod which will use the replicated volume to log its name, the current date, and the node it is running on.</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: volume-logger\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: volume-logger\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: volume-logger\n    spec:\n      terminationGracePeriodSeconds: 0\n      containers:\n        - name: volume-logger\n          image: busybox\n          args:\n            - sh\n            - -c\n            - |\n              echo \"Hello from \\$HOSTNAME, running on \\$NODENAME, started at \\$(date)\" &gt;&gt; /volume/hello\n              # We use this to keep the Pod running\n              tail -f /dev/null\n          env:\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - mountPath: /volume\n              name: replicated-volume\n      volumes:\n        - name: replicated-volume\n          persistentVolumeClaim:\n            claimName: replicated-volume\nEOF\n</code></pre> <p>After a short wait, the Pod is <code>Running</code>, our <code>PersistentVolumeClaim</code> is now <code>Bound</code>, and we can see that LINSTOR placed the volume on two nodes:</p> <pre><code>$ kubectl wait pod --for=condition=Ready -l app.kubernetes.io/name=volume-logger\npod/volume-logger-84dd47f4cb-trh4l\n$ kubectl get persistentvolumeclaim\nNAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                 AGE\nreplicated-volume   Bound    pvc-dbe422ac-c5ae-4786-a624-74d2be8a262d   1Gi        RWO            piraeus-storage-replicated   1m\n$ kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor resource list-volumes\n+-------------------------------------------------------------------------------------------------------------------------------------------+\n| Node           | Resource                                 | StoragePool | VolNr | MinorNr | DeviceName    | Allocated | InUse  |    State |\n|===========================================================================================================================================|\n| n1.example.com | pvc-dbe422ac-c5ae-4786-a624-74d2be8a262d | pool1       |     0 |    1000 | /dev/drbd1000 | 16.91 MiB | InUse  | UpToDate |\n| n2.example.com | pvc-dbe422ac-c5ae-4786-a624-74d2be8a262d | pool1       |     0 |    1000 | /dev/drbd1000 |   876 KiB | Unused | UpToDate |\n+-------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>NOTE: If your cluster has three or more nodes, you will actually see a third volume, marked as <code>TieBreaker</code>. This is intentional and improves the behaviour should one of the cluster nodes become unavailable.</p> <p>Now, we can check that our Pod actually logged the expected information by reading <code>/volume/hello</code> in the Pod:</p> <pre><code>$ kubectl exec deploy/volume-logger -- cat /volume/hello\nHello from volume-logger-84dd47f4cb-trh4l, running on n1.example.com, started at Fri Feb  3 08:53:47 UTC 2023\n</code></pre>"},{"location":"tutorial/replicated-volumes/#testing-replication","title":"Testing Replication","text":"<p>Now, we will verify that when we move the Pod to another node, we still have access to the same data.</p> <p>To test this, we will disable scheduling on the node the Pod is currently running on. This forces Kubernetes to move the Pod to another node once we trigger a restart. In our examples, the Hello message tells us that the Pod was started on <code>n1.example.com</code>, so this is the node we disable. Replace the name with your own node name.</p> <pre><code>$ kubectl cordon n1.example.com\nnode/n1.example.com cordoned\n</code></pre> <p>Now, we can trigger a new rollout of the deployment. Since we disabled scheduling <code>n1.example.com</code>, another node will have to take over our Pod.</p> <pre><code>$ kubectl rollout restart deploy/volume-logger\ndeployment.apps/volume-logger restarted\n$ kubectl wait pod --for=condition=Ready -l app.kubernetes.io/name=volume-logger\npod/volume-logger-5db9dd7b87-lps2f condition met\n$ kubectl get pods -owide\nNAME                             READY   STATUS    RESTARTS   AGE   IP            NODE             NOMINATED NODE   READINESS GATES\nvolume-logger-5db9dd7b87-lps2f   1/1     Running   0          26s   10.125.97.9   n2.example.com   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>As expected, the Pod is now running on a different node, in this case on <code>n2.example.com</code>.</p> <p>Now, we can verify that the message from the original pod is still present:</p> <pre><code>$ kubectl exec deploy/volume-logger -- cat /volume/hello\nHello from volume-logger-84dd47f4cb-trh4l, running on n1.example.com, started at Fri Feb  3 08:53:47 UTC 2023\nHello from volume-logger-5db9dd7b87-lps2f, running on n2.example.com, started at Fri Feb  3 08:55:42 UTC 2023\n</code></pre> <p>As expected, we still see the message from <code>n1.example.com</code>, as well as the message from the new Pod on <code>n2.example.com</code>.</p> <p>We can also see that LINSTOR now shows the volume as <code>InUse</code> on the new node:</p> <pre><code>$ kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor resource list-volumes\n+-------------------------------------------------------------------------------------------------------------------------------------------+\n| Node           | Resource                                 | StoragePool | VolNr | MinorNr | DeviceName    | Allocated | InUse  |    State |\n|===========================================================================================================================================|\n| n1.example.com | pvc-dbe422ac-c5ae-4786-a624-74d2be8a262d | pool1       |     0 |    1000 | /dev/drbd1000 | 16.91 MiB | Unused | UpToDate |\n| n2.example.com | pvc-dbe422ac-c5ae-4786-a624-74d2be8a262d | pool1       |     0 |    1000 | /dev/drbd1000 |   952 KiB | InUse  | UpToDate |\n+-------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>You have now successfully created a replicated volume and verified that the data is accessible from multiple nodes.</p>"},{"location":"tutorial/replicated-volumes/#resetting-the-disabled-node","title":"Resetting the Disabled Node","text":"<p>Now that we have verified replication works, we can reset the disabled node:</p> <pre><code>$ kubectl uncordon n1.example.com\nnode/n1.example.com uncordoned\n</code></pre>"},{"location":"tutorial/snapshots/","title":"Creating and Restoring From Snapshots","text":"<p>Learn how to create snapshots of your data, and how you can restore the data in a snapshot.</p> <p>Snapshots create a copy of the volume content at a particular point in time. This copy remains untouched when you make modifications to the volume content. This, for example, enables you to create backups of your data before performing modifications or deletion on your data.</p> <p>Since a backup is useless, unless you have a way to restore it, this tutorial will teach you both how to create a snapshot, and how to restore in the case of accidental deletion of your data.</p>"},{"location":"tutorial/snapshots/#prerequisites","title":"Prerequisites","text":"<ul> <li>An installed and configured Piraeus Datastore. Learn how to get started in our introduction tutorial</li> <li>A storage pool supporting snapshots. LINSTOR supports snapshots for <code>LVM_THIN</code>, <code>FILE_THIN</code>, <code>ZFS</code> and <code>ZFS_THIN</code> pools.   If you followed the introduction tutorial, you are using the supported <code>FILE_THIN</code> pool.</li> <li>A cluster with <code>snapshot-controller</code> deployed. To check if   it is already deployed, try running:   <pre><code>$ kubectl api-resources --api-group=snapshot.storage.k8s.io -oname\nvolumesnapshotclasses.snapshot.storage.k8s.io\nvolumesnapshotcontents.snapshot.storage.k8s.io\nvolumesnapshots.snapshot.storage.k8s.io\n</code></pre>   If your output looks like above, you are good to go.   If your output is empty, you should deploy a snapshot controller. You can quickly deploy it by using:   <pre><code>kubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//client/config/crd\nkubectl apply -k https://github.com/kubernetes-csi/external-snapshotter//deploy/kubernetes/snapshot-controller\n</code></pre></li> </ul>"},{"location":"tutorial/snapshots/#creating-an-example-workload","title":"Creating an Example Workload","text":"<p>We will be using the same workload as in the replicated volumes tutorial. This workload will save the Pods name, the node it is running on, and a timestamp to our volume. By logging this information to our volume, we can easily keep track of our data.</p> <p>First, we create our <code>StorageClass</code>, <code>PersistentVolumeClaim</code> and <code>Deployment</code>:</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\n---\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: piraeus-storage\nprovisioner: linstor.csi.linbit.com\nallowVolumeExpansion: true\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  linstor.csi.linbit.com/storagePool: pool1\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-volume\nspec:\n  storageClassName: piraeus-storage\n  resources:\n    requests:\n      storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: volume-logger\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: volume-logger\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: volume-logger\n    spec:\n      terminationGracePeriodSeconds: 0\n      containers:\n        - name: volume-logger\n          image: busybox\n          args:\n            - sh\n            - -c\n            - |\n              echo \"Hello from \\$HOSTNAME, running on \\$NODENAME, started at \\$(date)\" &gt;&gt; /volume/hello\n              # We use this to keep the Pod running\n              tail -f /dev/null\n          env:\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n          volumeMounts:\n            - mountPath: /volume\n              name: data-volume\n      volumes:\n        - name: data-volume\n          persistentVolumeClaim:\n            claimName: data-volume\nEOF\n</code></pre> <p>Then, we wait for the Pod to start, and verify that the expected information was logged to our volume:</p> <pre><code>$ kubectl wait pod --for=condition=Ready -l app.kubernetes.io/name=volume-logger\npod/volume-logger-cbcd897b7-jrmks condition met\n$ kubectl exec deploy/volume-logger -- cat /volume/hello\nHello from volume-logger-cbcd897b7-jrmks, running on n3.example.com, started at Mon Feb 13 15:32:46 UTC 2023\n</code></pre>"},{"location":"tutorial/snapshots/#creating-a-snapshot","title":"Creating a Snapshot","text":"<p>Creating a snapshot requires the creation of a <code>SnapshotClass</code> first. The <code>SnapshotClass</code> specifies our <code>linstor.csi.linbit.com</code> provisioner, and sets the clean-up policy for our snapshots to <code>Delete</code>, meaning deleting the Kubernetes resources will also delete the snapshots in LINSTOR\u00ae.</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: piraeus-snapshots\ndriver: linstor.csi.linbit.com\ndeletionPolicy: Delete\nEOF\n</code></pre> <p>Next, we will request the creation of a snapshot using a <code>VolumeSnapshot</code> resource. The <code>VolumeSnapshot</code> resource references the <code>PersistentVolumeClaim</code> resource we created initially, as well as our newly created <code>VolumeSnapshotClass</code>.</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: data-volume-snapshot-1\nspec:\n  volumeSnapshotClassName: piraeus-snapshots\n  source:\n    persistentVolumeClaimName: data-volume\nEOF\n</code></pre> <p>Now, we need to wait for the snapshot to be created. We can then also verify its creation in LINSTOR:</p> <pre><code>$ kubectl wait volumesnapshot --for=jsonpath='{.status.readyToUse}'=true data-volume-snapshot-1\nvolumesnapshot.snapshot.storage.k8s.io/data-volume-snapshot-1 condition met\n$ kubectl get volumesnapshot data-volume-snapshot-1\nNAME                     READYTOUSE   SOURCEPVC     SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS       SNAPSHOTCONTENT                                    CREATIONTIME   AGE\ndata-volume-snapshot-1   true         data-volume                           1Gi           piraeus-snapshots   snapcontent-a8757c1d-cd37-42d2-9557-a24b1222d118   15s            15s\n$ kubectl -n piraeus-datastore exec deploy/linstor-controller -- linstor snapshot list\n+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n| ResourceName                             | SnapshotName                                  | NodeNames      | Volumes  | CreatedOn           | State      |\n|=========================================================================================================================================================|\n| pvc-9c04b307-d22d-454f-8f24-ed5837fe4426 | snapshot-a8757c1d-cd37-42d2-9557-a24b1222d118 | n3.example.com | 0: 1 GiB | 2023-02-13 15:36:18 | Successful |\n+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"tutorial/snapshots/#modifying-the-data","title":"Modifying the Data","text":"<p>Now we want to simulate a situation where we accidentally delete some important data. The important data in our example workload is the log of Pod name, Node name and timestamp. We will manually delete the file on the volume to simulate accidental removal of an important file on a persistent volume:</p> <pre><code>$ kubectl exec deploy/volume-logger -- rm /volume/hello\n$ kubectl exec deploy/volume-logger -- cat /volume/hello\ncat: can't open '/volume/hello': No such file or directory\ncommand terminated with exit code 1\n</code></pre>"},{"location":"tutorial/snapshots/#restoring-from-a-snapshot","title":"Restoring From a Snapshot","text":"<p>This is the exact situation where snapshots can come in handy. Since we created a snapshot before we made removed the file, we can create a new volume to recover our data. We will be replacing the existing <code>data-volume</code> with a new version based on the snapshot.</p> <p>First, we will stop the Deployment by scaling it down to zero Pods, so that it does not interfere with our next steps:</p> <pre><code>$ kubectl scale deploy/volume-logger --replicas=0\ndeployment.apps \"volume-logger\" deleted\n$ kubectl rollout status deploy/volume-logger\ndeployment \"volume-logger\" successfully rolled out\n</code></pre> <p>Next, we will also remove the <code>PersistentVolumeClaim</code>. We still have the snapshot which contains the data we want to restore, so we can safely remove the volume.</p> <pre><code>$ kubectl delete pvc/data-volume\npersistentvolumeclaim \"data-volume\" deleted\n</code></pre> <p>Now, we will create a new <code>PersistentVolumeClaim</code>, referencing our snapshot. This will create a volume, using the data from the snapshot.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-volume\nspec:\n  storageClassName: piraeus-storage\n  resources:\n    requests:\n      storage: 1Gi\n  dataSource:\n    apiGroup: snapshot.storage.k8s.io\n    kind: VolumeSnapshot\n    name: data-volume-snapshot-1\n  accessModes:\n    - ReadWriteOnce\nEOF\n</code></pre> <p>Since we named this new volume <code>data-volume</code>, we can just scale up our Deployment again, and the new Pod will start using the restored volume:</p> <pre><code>$ kubectl scale deploy/volume-logger --replicas=1\ndeployment.apps/volume-logger scaled\n</code></pre> <p>After the Pod started, we can once again verify the content of our volume:</p> <pre><code>$ kubectl wait pod --for=condition=Ready -l app.kubernetes.io/name=volume-logger\npod/volume-logger-cbcd897b7-5qjbz condition met\n$ kubectl exec deploy/volume-logger -- cat /volume/hello\nHello from volume-logger-cbcd897b7-jrmks, running on n3.example.com, started at Mon Feb 13 15:32:46 UTC 2023\nHello from volume-logger-cbcd897b7-gr6hh, running on n3.example.com, started at Mon Feb 13 15:42:17 UTC 2023\n</code></pre> <p>You have now successfully created a snapshot, and used it to back up and restore a volume after accidental deletion.</p>"}]}